---
title: "Linear Model Specification and Interpretation"
subtitle: Statistics 516, Homework 1 (Solutions)
output:
  html_document:
    theme: readable
  pdf_document: default
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{array}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", message = FALSE, out.width = "100%", fig.align = "center", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](hw1-solutions.pdf) copy of this homework assignment.", sep = ""), "")`

```{r options, echo = FALSE}
options(digits = 4, width = 100)
```

This homework assignment concerns specifying and making inferences from linear models using data from several studies. In particular, you will see how to make inferences concerning linear combinations of model parameters. Note that you will likely need to install several packages to access the data used in these problems. 

## Dopamine $\beta$-Hydroxylase Activity in Schizophrenics After Neuroleptic Treatment

The data frame `Dopamine` in the package **BSDA** is from an observational study of the response of schizophrenic patients to treatment.[^sternberg] Schizophrenic patients who had been treated with a neuroleptic drug were classified as either remaining psychotic or becoming non-psychotic after the treatment. Samples of cerebrospinal fluid from all patients in the study were assayed for dopamine $\beta$-hydroxylase (DBH) activity. DBH is an enzyme that catalyzes the conversion of the dopamine to norepinephrine, both of which are thought to be involved in the pathology of schizophrenia. There researchers thought that a difference in DBH activity between the two groups might delineate a subgroup of patients with a dopamine-sensative condition. You can see the raw data as follows with the variables `dbh` (DBH activity) and `group` (psychotic or non-psychotic).
```{r}
library(BSDA)
head(Dopamine)
tail(Dopamine)
```
The plot below shows the data.[^flip]
```{r, fig.height = 2}
library(ggplot2)
p <- ggplot(Dopamine, aes(x = group, y = dbh)) + 
  theme_minimal() + geom_point() + coord_flip() +
  labs(x = NULL, y = "DBH (nanomoles/ml-hour per mg of protein)")
plot(p)
```
We can also get some basic summary statistics (mean, standard deviation, and sample size) using the **dplyr** package.[^dplyr]
```{r}
library(dplyr)
Dopamine %>% group_by(group) %>% 
  summarize(meandbh = mean(dbh), sddbh = sd(dbh), n = n())
```
As can be seen from the descriptive statistics, DBH is a bit lower, on average, for the 15 non-psychotic patients. Here you will use a linear model to make inferences about DBH and how it differs between patients classified as psychotic and non-psychotic. 

1. Estimate a linear model using the `lm` function with DBH (`dbh`) as the response variable and patient group (`group`) as the explanatory variable. Report the parameter estimates and their standard errors using the `summary` function.

    **Solution**: Here is the estimated model.
    ```{r}
    m <- lm(dbh ~ group, data = Dopamine)
    summary(m)$coefficients
    ```

2. The model you estimated in the previous problem can be written as 
$$
  E(Y_i) = \beta_0 + \beta_1 x_i,
$$
where $Y_i$ is the $i$-th observation of DBH. Explain how the value of $x_i$ is defined for this model (i.e., how would you determine the value of $x_i$ for a given patient?). Write the model case-wise to express the expected DBH as a function of $\beta_0$ and/or $\beta_1$ for psychotic and non-psychotic patients. Let $\mu_p$ and $\mu_n$ be the expected DBH of psychotic and non-psychotic patients, respectively. Using the case-wise representation of the model, write each of these parameters as a function of $\beta_0$ and/or $\beta_1$ (i.e., how would you compute $\mu_p$ and $\mu_n$ using $\beta_0$ and $\beta_1$?).

    **Solution**: The `grouppsychotic` in the output from `summary` shows that $x_i$ is an indicator variable defined as
    $$
    x_i = 
    \begin{cases}
    1, & \text{if the $i$-th observation is of a psychotic patient}, \\
    0, & \text{otherwise}.
    \end{cases}
    $$
    Thus the model can be written case-wise as
    $$
    E(Y_i) = 
    \begin{cases}
    \beta_0, & \text{if the $i$-th observation is of a non-psychotic patient}, \\
    \beta_0 + \beta_1, & \text{if the $i$-th observation is of a psychotic patient}.
    \end{cases}
    $$
    Thus we have that $\mu_p = \beta_0 + \beta_1$ and $\mu_n = \beta_0$.
    

3. Using the `lincon` and `contrast` functions from the **trtools** package, produce estimates, standard errors, and confidence intervals for $\mu_p$ and $\mu_n$. For the `lincon` function, use the fact that each of these parameters can be written as a function of $\beta_0$ and/or $\beta_1$. The results from `lincon` and `contrast` should be the same.

    **Solution**: Here is how to make those inferences using `lincon` and `contrast`.
    ```{r}
    library(trtools)
    lincon(m, a = c(1,1)) # b0 + b1 (psychotic)
    lincon(m, a = c(1,0)) # b0 (non-psychotic)
    trtools::contrast(m, a = list(group = c("psychotic","nonpsychotic")),
      cnames = c("psychotic","non-psychotic"))
    ```
    Note that we do not necessarily need to use `lincon` or `contrast` to make inferences about $\mu_n = \beta_0$ since that is given by `summary`.
    ```{r}
    cbind(summary(m)$coefficients, confint(m))
    ```
    The **emmeans** package offers some of the same functionality as `lincon` and `contrast`, although the interface is quite different. It is a very powerful package. In these solutions I will show how to use it to make some of the same inferences that are made using `lincon` and/or `contrast`.
    ```{r}
    library(emmeans)
    emmeans(m, ~group) # estimate expected response for each group
    ```

4. Using the `lincon` and `contrast` functions, produce an estimate, standard error, and confidence interval for $\mu_p - \mu_n$, as well as the test statistic and p-value for a test of the null hypothesis that $\mu_p - \mu_n$ = 0. The results from `lincon` and `contrast` should be the same.[^t.test]

    **Solution**: Here is how to make inferences regarding $\mu_p - \mu_n$ using `lincon` and `contrast`.
    ```{r}
    lincon(m, a = c(0,1))
    trtools::contrast(m, 
      a = list(group = "psychotic"), 
      b = list(group = "nonpsychotic"))
    ```
    Here is how to estimate this differences using the **emmeans** package.
    ```{r}
    pairs(emmeans(m, ~group)) # estimate difference in expected response
    ```
Note that when using `pairs` the estimated difference is that for $\mu_n - \mu_p$. To get $\mu_p - \mu_n$ we can use the options `reverse = TRUE`. Also the functions in **emmeans** may not necessarily give both a test and a confidence interval for what is being estimated. To force it to provide both we can use the option `infer = TRUE`.
    ```{r}
    pairs(emmeans(m, ~group), reverse = TRUE, infer = TRUE)
    ```

5. There are alternative prameterizations of this model. Estimate a linear model using the `lm` function with the model formula `dbh ~ -1 + group`, and repeat what you did in the previous four problems but for this model.[^noint] **Note**: This problem is **extra credit** for students enrolled in Stat 436, but is **required** for students enrolled in Stat 516.

    **Solution**: Here is the estimated model.
    ```{r}
    m <- lm(dbh ~ -1 + group, data = Dopamine)
    summary(m)$coefficients
    ```
    This model can be written as $E(Y_i) = \beta_1x_{i1} + \beta_2x_{i2}$. From the output of `summary` we can see that $x_{i1}$ and $x_{i2}$ are indicator variables defined as 
    $$
    x_{i1} = 
    \begin{cases}
    1, & \text{if the $i$-th observation is of a non-psychotic patient}, \\
    0, & \text{otherwise},
    \end{cases}
    $$
    and 
    $$
    x_{i2} = 
    \begin{cases}
    1, & \text{if the $i$-th observation is of a psychotic patient}, \\
    0, & \text{otherwise}.
    \end{cases}
    $$
    So we have that $\mu_p = \beta_2$ and $\mu_n = \beta_1$. Here is how we can make inferences for $\mu_p$ and $\mu_n$ using `lincon`, `contrast`, and the **emmeans** package.
    ```{r}
    lincon(m, a = c(0,1)) 
    lincon(m, a = c(1,0)) 
    trtools::contrast(m, a = list(group = c("psychotic","nonpsychotic")), 
      cnames = c("psychotic","nonpsychotic"))
    # emmeans(m, ~group)
    ```
    Here is how we can make inferences for $\mu_p - \mu_n$.
    ```{r}
    lincon(m, a = c(-1,1))
    trtools::contrast(m, a = list(group = "psychotic"), b = list(group = "nonpsychotic"))
    pairs(emmeans(m, ~group), reverse = TRUE, infer = TRUE)
    ```
    Note that for this parameterization we can get inferences for $\mu_p = \beta_2$ and $\mu_n = \beta_1$ from `summary` as well, but not $\mu_p - \mu_n = \beta_2 - \beta_1$.
    ```{r}
    cbind(summary(m)$coefficients, confint(m))
    ```
    Perhaps one of the advantages of functions like `contrast` and those from the **emmeans** package is that we do not usually need to be concerned with how the model is parameterized. 

[^noint]: Note how including `-1` in the model formula causes the model to *not* include a term of $\beta_0$.

[^t.test]: Introductory statistics classes typically discuss inferences for the difference between the means based on two independent samples. That is what you are doing here, although you are framing the inferences in terms of a linear model. 

[^sternberg]: Sternberg, D. E., Van Kammen, D. P., & Bunney, W. E. (1982). Schizophrenia: Dopamine $\beta$-hydroxylase activity and treatment response. *Science*, *216*, 1423--1425.

[^dplyr]: The **dplyr** package (sometimes used in conjunction with the **tidyr**) package is very useful for data manipulation and descriptive analysis. There is a bit of a learning curve to using it, but it is well worth learning in my opinion. 

[^flip]: Note now `coord_flip()` can be used to "flip" the ordinate and abscissa of the plot which works nicely here to orient the plot horizontally.

## Weight Gain in Rats Exposed to Thiouracil and Thyroxin

The data frame `rat` in the package **ALA** is from an experiment investigating the effects of thiouracil and thyroxin on growth of rats.[^box] The **ALA** package is located in the R-Forge repository and not the CRAN repository, which is the default repository used by `install.packages`, so use the command `install.packages("ALA", repos = "http://R-Forge.R-project.org")` to specify the correct repository for installing **ALA** package. In this experiment thirty rats were each randomly assigned to one of three treatment groups where for two of the three groups an additive (thiouracil or thyroxin) was put in the drinking water.[^drugs] Measurements of rat weight (in grams) were observed at a baseline of zero weeks before putting the additives in the drinking water, and again at one, two, three, and four weeks after the additives were introduced. Three of the rats from the thyroxin group were lost so the total number of rats from which we have data is 27.[^missingdata] The following shows first few rows of the `rat` data frame.
```{r}
library(ALA)
head(rat)
```
Note that the row names are not consecutive integers. This is likely because the data were originally sorted by treatment and week before being sorted by rat and then stored in the data frame. The row names usually do not have any effect on what we are doing here, but if you wanted to "reset" them you could use `rownames(rat) <- NULL`. The data can be plotted as follows. 
```{r}
library(ggplot2)
p <- ggplot(rat, aes(x = week, y = weight)) + theme_minimal() + 
  geom_point() + facet_wrap(~ treatment) + labs(y = "Weight (g)", x = "Week")
plot(p)
```
These data are *longitudinal* in that multiple observations are made on the same rat over time. Special methods are necessary to provide proper inferences for such designs, and we will discuss these later in the semester. But for now we will ignore this issue. You might pretend that each rat was only observed *once* (i.e., the rats in a given treatment group observed at one week are *different* from those observed at another week).

1. Estimate a linear model using the `lm` function with `weight` as your response variable, and `week` and `treatment` as your explanatory variables, respectively. The model should be specified with an "interaction" between `treatment` and `week` so that the rate of change in expected weight per week can be different across the treatment groups. Report the parameter estimates using the `summary` function. You should get something like the following.

    ```{r, echo = FALSE}
    m <- lm(weight ~ treatment + week + treatment:week, data = rat)
    summary(m)$coefficients
    ```
    
    **Solution**: The model can be estimated as follows.
    ```{r}
    m <- lm(weight ~ treatment + week + treatment:week, data = rat)
    summary(m)$coefficients
    ```
    
0. As can be seen from the output of `summary` from the previous problem, the model has six terms, including $\beta_0$, so it can be written as
$$
  E(Y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5},
$$
where $Y_i$ is the $i$-th observation of weight (in grams), and $x_{i1}, x_{i2}, \dots, x_{i5}$ depend on treatment and/or week. Explain how $x_{i1}$, $x_{i2}$, $x_{i3}$, $x_{i4}$, and $x_{i5}$ are defined for this model (i.e., how would you determine their values for a given observation?). Then write the model case-wise to show how the expected weight depends on week for each of the three treatment conditions.

    **Solution**: From `summary` we can see that $x_{i1}$ and $x_{i2}$ are both indicator variables defined as 
    $$
    x_{i1} = 
      \begin{cases}
      1, & \text{if the treatment used for the $i$-th observation is thiouracil}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    and 
    $$
    x_{i2} = 
    \begin{cases}
    1, & \text{if the treatment used for the $i$-th observation is thyroxin}, \\
    0, & \text{otherwise}.
    \end{cases}
    $$
    Then $x_{i3}$ is the week (0, 1, 2, 3, or 4), and $x_{i4} = x_{i1}x_{i3}$ and $x_{i5} = x_{i2}x_{i3}$. This implies that we could also write $x_{i4}$ and $x_{i5}$ as 
    $$
    x_{i4} = 
      \begin{cases}
      w_i, & \text{if the treatment used for the $i$-th observation is thiouracil}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    and 
    $$
    x_{i5} = 
    \begin{cases}
    w_i, & \text{if the treatment used for the $i$-th observation is thyroxin}, \\
    0, & \text{otherwise},
    \end{cases}
    $$
    where $w_i = x_{i3}$ (i.e., week for the $i$-th observation). We can therefore write the model case-wise as
    $$
    E(Y_i) = 
    \begin{cases}
      \beta_0 + \beta_3 w_i, & \text{if the treatment for the $i$-th observation is control}, \\
      \beta_0 + \beta_1 + (\beta_3 + \beta_4) w_i, & \text{if the treatment for the $i$-th observation is thiouracil}, \\ 
      \beta_0 + \beta_2 + (\beta_3 + \beta_5) w_i, & \text{if the treatment for the $i$-th observation is thyroxin}, \\ 
    \end{cases}
    $$
    where $w_i = x_{i3}$ is the week for the $i$-th observation. Note that if you entered the terms in the model formula in a different order (e.g., `week` before `treatment` this would change the parameterization of the model).

0. Plot the model by creating an artificial data set of combinations of values of week and treatment using the `expand.grid` function, computing the predicted values from the model using the `predict` function, and adding lines to the plot using the code above and `geom_line`. 

    **Solution**: Here is our artifical data set.
    ```{r}
    d <- expand.grid(week = seq(0, 4, length = 100), 
      treatment = c("control","thiouracil","thyroxin"))
    head(d)
    ```
    There are a couple of things to note here. One is that we really only need two points for a line. So you could have `week = c(0,4)`. But we will want to have more points in the future when we are plotting curves and not lines. A shortcut you could use to specify the levels of treatment is `treatment = unique(rat$treatment)` which will give you the unique values of `treatment`. Now we can add the predicted valus using the `predict` function, which effectively computes the estimated expected response for every combination of `treatment` and `week`.
    ```{r}
    d$yhat <- predict(m, newdata = d)
    head(d)
    ```
    Now we can create the plot. Here is the complete code for the plot with the data and the estimated model.
    ```{r}
    p <- ggplot(rat, aes(x = week, y = weight)) + theme_minimal() + 
      geom_point() + facet_wrap(~ treatment) + labs(y = "Weight (g)", x = "Week") + 
      geom_line(aes(y = yhat), data = d)
    plot(p)
    ```

0. Using the `contrast` function, estimate the expected weight at zero weeks and again at four weeks for rats in each of the three treatment conditions. 

    **Solution**: Here are the estimated weights at zero weeks.
    ```{r}
    trtools::contrast(m, 
      a = list(treatment = c("control","thiouracil","thyroxin"), week = 0),
      cnames = c("control","thiouracil","thyroxin"))
    trtools::contrast(m, 
      a = list(treatment = c("control","thiouracil","thyroxin"), week = 4),
      cnames = c("control","thiouracil","thyroxin"))
    ```
    Here is how you would do that using the **emmeans** package.
    ```{r}
    emmeans(m, ~treatment, at = list(week = 0))
    emmeans(m, ~treatment, at = list(week = 4))
    ```
    You can also do this in one statement as follows.
    ```{r}
    emmeans(m, ~treatment|week, at = list(week = c(0,4)))
    ```

0. Using the `contrast` function, estimate the *difference* in expected weight at four weeks between the control group and the other two treatment groups, and also between the thiouracil and thyroxin groups. Also estimate these differences at zero weeks. 

    **Solution**: Here is the comparison between the the groups at four weeks.
    ```{r}
    trtools::contrast(m, 
      a = list(treatment = c("thiouracil","thyroxin"), week = 4),
      b = list(treatment = "control", week = 4),
      cnames = c("thiouracil vs control","thyroxin vs control"))
    trtools::contrast(m, 
      a = list(treatment = "thyroxin", week = 4),
      b = list(treatment = "thiouracil", week = 4))
    ```
    And here are the comparisons at zero weeks.
    ```{r}
    trtools::contrast(m, 
      a = list(treatment = c("thiouracil","thyroxin"), week = 0),
      b = list(treatment = "control", week = 0),
      cnames = c("thiouracil vs control","thyroxin vs control"))
    trtools::contrast(m, 
      a = list(treatment = "thyroxin", week = 0),
      b = list(treatment = "thiouracil", week = 0))
    ```

0. Using the `contrast` function, estimate the rate of change in expected weight per unit increase in week (i.e., the change in expected weight corresponding to a one week increase in time) for rats in *each* of the three treatment groups. 

    **Solution**: Here is how we can estimate the rates of change in expected weight per week.
    ```{r}
    trtools::contrast(m, 
      a = list(week = 1, treatment = c("control","thiouracil","thyroxin")),
      b = list(week = 0, treatment = c("control","thiouracil","thyroxin")),
      cnames = c("control","thiouracil","thyroxin"))
    ```
    We can estimate these quantities with the **emmeans** package a couple of ways. One is to use the `emtrends` function.
    ```{r}
    emtrends(m, ~treatment, var = "week", infer = TRUE)
    ```
    Another is to use the `pairs` function but use the `by` argument so that the pairs are within each level of `treatment`.
    ```{r}
    pairs(emmeans(m, ~treatment*week, at = list(week = c(0,1))),
      by = "treatment", reverse = TRUE, infer = TRUE)
    ```
    Note that here the `emmeans` part creates estimates of the expected response at weeks 0 and 1 for each treatment.
    ```{r}
    emmeans(m, ~treatment*week, at = list(week = c(0,4)))
    ```
    Putting that "inside" the `pairs` function then generates inferences for the difference between pairs of conditions, and the `by` argument forces those to be within each level of `treatment`. This is perhaps more complicated than is necessary, but an advantage of this approach is that we could do the same thing for something other than an increase of one week. For example, we can estimate the increase in expected weight after four weeks for each treatment condition.
    ```{r}
    pairs(emmeans(m, ~treatment*week, at = list(week = c(0,4))),
      by = "treatment", reverse = TRUE, infer = TRUE)
    ```
    We could even go one step further and compare the rates of change among the three treatment conditions.
    ```{r}
    pairs(pairs(emmeans(m, ~treatment*week, at = list(week = c(0,4))),
      by = "treatment", reverse = TRUE), by = NULL)
    ```
    Here the `by = NULL` argument is necessary because they "inner" use of `pairs` grouped the comparison by `treatment`, but now we want to make comparisons across treatment conditions. Note that `pairs` automatically applied an adjustment for the family-wise error rate here. This can be removed by adding the option `adjust = "none"`.
    ```{r}
    pairs(pairs(emmeans(m, ~treatment*week, at = list(week = c(0,4))),
      by = "treatment", reverse = TRUE), by = NULL, adjust = "none")
    ```
    The **emmeans** package is quite powerful, but it there is a bit of a learning curve. You can estimate differences of differences like this with with the `contrast` function from **trtools** as well by using two additional arguments. Here is how to estimate the difference in the increase in the expected weight from week 0 to week 4 between the control and thiouracil conditions.
    ```{r}
    trtools::contrast(m,
      a = list(treatment = "control", week = 4),
      b = list(treatment = "control", week = 0),
      u = list(treatment = "thiouracil", week = 4),
      v = list(treatment = "thiouracil", week = 0))
    ```
    Here the `contrast` function considers the difference in the expected response between the conditions specified `a` and `b`, and also between `u` and `v`, and then makes inferences for the difference between those differences!
    
0. Now consider a model for these data but using the model formula `weight ~ treatment:week` with the `lm` function.[^intercept] Repeat what you did in the previous problems but using now this model. **Note**: This problem is **extra credit** for students enrolled in Stat 436, but is **required** for students enrolled in Stat 516.

    **Solution**: Here is the estimated model.
    ```{r}
    m <- lm(weight ~ treatment:week, data = rat)
    summary(m)$coefficients
    ```
    The model can be written as $E(Y_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3}$. We can explain $x_{i1}$, $x_{i2}$, and $x_{i3}$ in a couple of ways. If we were to define three new variables called $d_{i1}$, $d_{i2}$, and $d_{i3}$ such that 
    $$
      d_{i1} = 
      \begin{cases}
      1, & \text{if the treatment for the $i$-th observation is control}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    $$
    d_{i2} = 
      \begin{cases}
      1, & \text{if the treatment for the $i$-th observation is thiouracil}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    $$
    d_{i3} = 
      \begin{cases}
      1, & \text{if the treatment for the $i$-th observation is thyroxin}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    then we can say that $x_{i1} = d_{i1}w_i$, $x_{i2} = d_{i2}w_i$, and $x_{i3} = d_{i3}w_i$, where $w_i$ is the week for the $i$-th observation. We can also define these variables without explicitly referencing indicator variables as
    $$
      x_{i1} = 
      \begin{cases}
      w_i, & \text{if the treatment for the $i$-th observation is control}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    $$
    x_{i2} = 
      \begin{cases}
      w_i, & \text{if the treatment for the $i$-th observation is thiouracil}, \\
      0, & \text{otherwise},
      \end{cases}
    $$
    $$
    x_{i3} = 
      \begin{cases}
      w_i, & \text{if the treatment for the $i$-th observation is thyroxin}, \\
      0, & \text{otherwise}.
      \end{cases}
    $$
    We can write the model case-wise as
    $$
    E(Y_i) = 
    \begin{cases}
      \beta_0 + \beta_1w_i, & \text{if the treatment for the $i$-th observation is control}, \\
      \beta_0 + \beta_2w_i, & \text{if the treatment for the $i$-th observation is thiouracil}, \\
      \beta_0 + \beta_3w_i, & \text{if the treatment for the $i$-th observation is thyroxin}. \\
    \end{cases}
    $$
    For everything else we can just cut-and-paste the code from before, but apply it to this alternative model. Here is the plot.
    ```{r}
    d <- expand.grid(week = seq(0, 4, length = 100), 
      treatment = c("control","thiouracil","thyroxin"))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(rat, aes(x = week, y = weight)) + theme_minimal() + 
      geom_point() + facet_wrap(~ treatment) + labs(y = "Weight (g)", x = "Week") + 
      geom_line(aes(y = yhat), data = d)
    plot(p)
    ```
    Notice how the three lines have the same intercept (i.e., the same estimated expected weight at zero weeks). I can make this a bit more obvious by putting the three treatment conditions in the same plot.
    ```{r}
    p <- ggplot(rat, aes(x = week, y = weight, color = treatment)) + theme_minimal() + 
      geom_point(alpha = 0.5) + labs(y = "Weight (g)", x = "Week", color = "Treatment") +
      geom_line(aes(y = yhat), data = d) + 
      theme(legend.position = c(0.15, 0.8))
    plot(p)
    ```
    Here are the estimated weights at zero weeks.
    ```{r}
    trtools::contrast(m, 
      a = list(treatment = c("control","thiouracil","thyroxin"), week = 0),
      cnames = c("control","thiouracil","thyroxin"))
    trtools::contrast(m, 
      a = list(treatment = c("control","thiouracil","thyroxin"), week = 4),
      cnames = c("control","thiouracil","thyroxin"))
    ```
    Here is the comparison between the the groups at four weeks.
    ```{r}
    trtools::contrast(m, 
      a = list(treatment = c("thiouracil","thyroxin"), week = 4),
      b = list(treatment = "control", week = 4),
      cnames = c("thiouracil vs control","thyroxin vs control"))
    trtools::contrast(m, 
      a = list(treatment = "thyroxin", week = 4),
      b = list(treatment = "thiouracil", week = 4))
    ```
    And here are the comparisons at zero weeks.
    ```{r}
    trtools::contrast(m, 
      a = list(treatment = c("thiouracil","thyroxin"), week = 0),
      b = list(treatment = "control", week = 0),
      cnames = c("thiouracil vs control","thyroxin vs control"))
    trtools::contrast(m, 
      a = list(treatment = "thyroxin", week = 0),
      b = list(treatment = "thiouracil", week = 0))
    ```
    Here are the estimated rates of change in expected weight per week.
    ```{r}
    trtools::contrast(m, 
      a = list(week = 1, treatment = c("control","thiouracil","thyroxin")),
      b = list(week = 0, treatment = c("control","thiouracil","thyroxin")),
      cnames = c("control","thiouracil","thyroxin"))
    ```
    Note that with this parameterization these rates of change are $\beta_1$, $\beta_2$, and $\beta_3$, and so inferences are given by `summary` as well. 

[^intercept]: This model is specified such that the expected weight at zero weeks is the same for rats in all three treatment conditions, which makes sense because at the beginning of the study before the additives were put in the drinking water there would not be a difference in expected weight between the treatment conditions (assuming the treatments were randomized, which they were). 

[^box]: Source: Box, G. E. P. (1950). Problems in the analysis of growth and wear curves. *Biometrics*, *6(4)*, 362--389. 

[^drugs]: Thioracil is an anti-thyroid medication that is sometimes used to treat hyperthyroidism, and thyroxin is a hormone made by the thyroid that controls growth and development. 

[^missingdata]: Missing data can be a serious issue in longitudinal studies and in regression in general. We generally assume that missing data are "missing at random" meaning that while whether or not an observation is missing may depend on the explanatory variable(s) it is not related to the response variable once we account for the explanatory variable. Here this would mean that within a given treatment group the probability that an observation is missing does not depend on weight. 

## Otter Survey

The data frame `otters` in the package **SDaA** is from a survey of dens (holts) of Eurasian otters (*Lutra lutra*) along the coast of Shetland in the United Kingdom.[^kruuk] The observational units here are 110 m deep by 5 km long sections along the coast. These sections were selected using a stratified random sampling design where simple random sampling was applied to the sections in each of four strata defined by the habitat: cliff, agricultural, peat, and non-peat. The number of holts was counted for the sections that were sampled. Here are the first few observations of that data.
```{r}
library(SDaA)
head(otters)
```
The four strata (`habitat`) are just coded with integers. This is problematic for two reasons. One is that if we were to use this variable as an explanatory variable it would be specified as being quantitative and not categorical, which would not make sense. We can fix this by converting it to a factor, but it we are going to do that we might as well give the levels more descriptive labels. The code below creates a new variable `stratum` which does this.  
```{r}
library(dplyr)
otters <- otters %>%
  mutate(stratum = factor(habitat, levels = 1:4, 
    labels = c("cliff","agricultural","peat","non-peat")))
head(otters)
```
In what follows we will use the variable `stratum` instead of `habitat`. Here is a dot plot of the data.[^dotplot]
```{r}
library(ggplot2) 
p <- ggplot(otters, aes(x = stratum, y = holts)) + theme_classic() +
  geom_dotplot(binaxis = "y", binwidth = 1, stackdir = "center") + 
  labs(x = "Stratum", y = "Number of Holts")
plot(p)
```
Here are some basic descriptive statistics for the number of holts per section by stratum (mean, standard deviation, and sample size).
```{r}
library(dplyr)
otters %>% group_by(stratum) %>%
  summarize(meanholts = mean(holts), sdholts = sd(holts), n = n())
```
Notice how the strata with more holts per section, on average, have higher variance. This is common for counts, and it is a problem for proper inferences since our inferences will assume that the variability of the response variable is (relatively) constant. We will discuss how to deal with this problem later, but ignore it here for the purpose of this assignment. Here you will use a linear model to make inferences about the abundance of otter dens.[^samplingdesign]

1. Estimate a linear model using the `lm` function with `holts` as the response variable and `stratum` as the explanatory variable. Report the parameter estimates using `summary` function. Note that the model can be written as $E(Y_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3}$. Explain briefly what $x_{i1}$, $x_{i2}$, and $x_{i3}$ each represent (i.e., how would you determine the their values for a given section?). Write the model case-wise to show how the expected number of holts of a sampled section can be written as a function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$. Finally, let $\mu_c$, $\mu_a$, $\mu_p$, and $\mu_n$ denote the expected number of holts in a section from the cliff, agricultural, peat, and non-peat habitats, respectively. Write each of these as a function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$.

    **Solution**: We can estimate the model as follows.
    ```{r}
    m <- lm(holts ~ stratum, data = otters)
    summary(m)$coefficients
    ```
    This shows that $x_{i1}$, $x_{i2}$, and $x_{i3}$ are defined as
    $$
    x_{i1} = 
    \begin{cases}
    1, & \text{if the $i$-th observation is from the agricultural stratum}, \\
    0, & \text{otherwise},
    \end{cases}
    $$
    $$
    x_{i2} = 
    \begin{cases}
    1, & \text{if the $i$-th observation is from the peat stratum}, \\
    0, & \text{otherwise},
    \end{cases}
    $$
    $$
    x_{i3} = 
    \begin{cases}
    1, & \text{if the $i$-th observation is from the non-peat stratum}, \\
    0, & \text{otherwise}.
    \end{cases}
    $$
    So the model can be written case-wise as 
    $$
    E(Y_i) = 
    \begin{cases}
    \beta_0, & \text{if the $i$-th observation is from the cliff stratum}, \\
    \beta_0 + \beta_1, & \text{if the $i$-th observation is from the agricultural stratum}, \\
    \beta_0 + \beta_2, & \text{if the $i$-th observation is from the peat stratum}, \\
    \beta_0 + \beta_3, & \text{if the $i$-th observation is from the non-peat stratum}.
    \end{cases}
    $$
    Thus we have that $\mu_c = \beta_0$, $\beta_a = \beta_0 + \beta_1$, $\mu_p = \beta_0 + \beta_2$, and $\mu_n = \beta_0 + \beta_3$.

0. Use the `contrast` function to estimate the expected number of holts in a section sampled from each stratum, and also the *difference* in the expected number of holts between a sampled *peat* section and each of the other three types of habitat.

    **Solution**: Here is how to estimate the expected number of holts in a section using `contrast`.
    ```{r}
    trtools::contrast(m, a = list(stratum = unique(otters$stratum)), 
      cnames = unique(otters$stratum))
    ```
    And here are the inferences for the difference between the peat stratum and the other three.
    ```{r}
    trtools::contrast(m, 
      a = list(stratum = "peat"), 
      b = list(stratum = c("agricultural","cliff","non-peat")),
      cnames = c("peat versus agricultural","peat versus cliff","peat versus non-peat"))
    ```
    Here is how to do this using the **emmeans** package. The estimated expected number of holts in each section is relatively simple.
    ```{r}
    emmeans(m, ~stratum)
    ```
    For the pairwise comparisons there are a couple of approaches. We can generate *all possible* pairwise comparisons as follows.
    ```{r}
    pairs(emmeans(m, ~stratum), infer = TRUE, adjust = "none")
    ```
    Note that the differences are not necessarily in the same direction here. If you want to get just pairwise comparisons between one level and the other levels you can use the `contrast` function from the **emmeans** package which works a little differently from the function of the same name from the **trtools** package.
    ```{r}
    contrast(emmeans(m, ~stratum), method = "trt.vs.ctrl", ref = "peat",
      reverse = TRUE, infer = TRUE, adjust = "none")
    ```
    Note that specifying `adjust = "none"` means that the tests and confidence intervals are not adjusted for multiple testing. An adjustment means that the family-wise error rate which is the probability of making *at least one Type I error* (assuming all null hypotheses are true) can be maintained at a given significance level, and the joint confidence level (i.e., the probability that *all* confidence intervals contain what is being estimated) is kept at 95%. Whether or not this is necessary depends on the user's goals. The `contrast` function from the **trtools** package does not make these adjustments by default, but it can by specifying `adjust = TRUE`. It uses a method that is equivalent to the "mvt" method used by functions in the **emmeans** package, which is the most reliable way to make these kinds of adjustments (although it is a little bit more computationally intensive, and it relies a numerical approximation that can sometimes give very slightly different results).
    ```{r}
    trtools::contrast(m,
      a = list(stratum = "peat"),
      b = list(stratum = c("agricultural","cliff","non-peat")),
      cnames = c("peat versus agricultural","peat versus cliff","peat versus non-peat"),
      adjust = TRUE)
    contrast(emmeans(m, ~stratum), method = "trt.vs.ctrl", ref = "peat",
      reverse = TRUE, infer = TRUE, adjust = "mvt")
    ```

0. Suppose we want to estimate the *number* of holts in *all* of the sections in a given stratum. For the cliffs stratum, for example, the total number of holts (denoted as $\tau_c$) would be computed as $\tau_c = N_c\mu_c$, where $N_c$ is the *total* number of sections in the cliffs stratum (i.e., not the sample size, but the population size for that stratum). The stratum sizes for the four strata are known to be $N_c$ = 89, $N_a$ = 61, $N_p$ = 40, and $N_n$ = 47. Use the `lincon` function to estimate the total number of holts in each stratum. Note that to do this you will need to write the quantity of interest (e.g., $N_c\mu_c$) as a linear combination of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ to determine the appropriate coefficients to use with `lincon`.

    **Solution**: First note that we can write these totals as $\tau_c = N_c\beta_0$, $\tau_a = N_a(\beta_0 + \beta_1) = N_a\beta_0 + N_a\beta_1$, $\tau_p = N_p(\beta_0 + \beta_2) = N_p\beta_0 + N_p\beta_2$, and $\tau_n = N_n(\beta_0 + \beta_3) = N_n\beta_0 + N_n\beta_3$. So we can estimate these quantities as follows with `lincon`.
    ```{r}
    lincon(m, a = c(89,0,0,0))  # cliff
    lincon(m, a = c(61,61,0,0)) # agricultural
    lincon(m, a = c(40,0,40,0)) # peat
    lincon(m, a = c(47,0,0,47)) # non-peat
    ```
    The `contrast` function in the **emmeans** package can do this but with a somewhat different approach. If we use it to estimate $\mu_c$, $\mu_a$, $\mu_p$, and $\mu_n$ it will allow us to also estimate a linear combination of these quantities which we might write as $a_c\mu_c + a_a\mu_a + a_p\mu_p + a_n\mu_n$ where we specify the coefficients $a_c$, $a_a$, $a_p$, ad $a_n$. Here is how that would work.
    ```{r}
    contrast(emmeans(m, ~stratum), method = list(stratum = c(89,0,0,0)), infer = TRUE) # cliff
    contrast(emmeans(m, ~stratum), method = list(stratum = c(0,61,0,0)), infer = TRUE) # agricultural
    contrast(emmeans(m, ~stratum), method = list(stratum = c(0,0,40,0)), infer = TRUE) # peat
    contrast(emmeans(m, ~stratum), method = list(stratum = c(0,0,0,47)), infer = TRUE) # non-peat
    ```
    
0. Now suppose you wanted to estimate the mean number of holts per section and the total number of holts in all sections for the *population* of sections rather than for a particular stratum. Denote this mean and total as $\mu$ and $\tau$, respectively. These can be written as 
$$
  \mu = \frac{N_c}{N}\mu_c + \frac{N_a}{N}\mu_a + 
  \frac{N_p}{N}\mu_p + \frac{N_n}{N}\mu_n,
$$
where $N = N_c + N_a + N_p + N_n$, and
$$
  \tau = N_c\mu_c + N_a\mu_a + N_p\mu_p + N_n\mu_n.
$$
In a previous problem you expressed $\mu_c$, $\mu_a$, $\mu_p$, and $\mu_n$ as functions of the parameters $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$. In the expressions for $\mu$ and $\tau$ above, substitute $\mu_c$, $\mu_a$, $\mu_p$, and $\mu_n$ with the corresponding function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$, and then simplify the expressions so that $\mu$ and $\tau$ are then written as linear combinations of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$. Then use the `lincon` function to compute estimates of $\mu$ and $\tau$ as well as confidence intervals for these parameters and the standard errors of the estimators.

    **Solution**:
    Note that
    $$
      \mu = \frac{N_c}{N}\beta_0 + 
      \frac{N_a}{N}(\beta_0 + \beta_1) + 
      \frac{N_p}{N}(\beta_0 + \beta_2) + 
      \frac{N_n}{N}(\beta_0 + \beta_3) = 
      \beta_0 + \frac{N_a}{N}\beta_1 + 
      \frac{N_p}{N}\beta_2 + 
      \frac{N_n}{N}\beta_3,
    $$
    and 
    $$
      \tau = N_c\beta_0 + 
      N_a(\beta_0 + \beta_1) + 
      N_p(\beta_0 + \beta_2) + 
      N_n(\beta_0 + \beta_3) = 
      N\beta_0 + N_a\beta_1 + N_p\beta_2 + N_n\beta_3.
    $$
    We can estimate $\mu$ and $\tau$ using `lincon` as follows.
    ```{r}
    lincon(m, a = c(1,61/237,40/237,47/237))
    lincon(m, a = c(237,61,40,47))
    ```
    We can also estimate $\mu$ and $\tau$ using the **emmeans** package, but there we specify a linear combination of $\mu_c$, $\mu_a$, $\mu_p$, and $\mu_n$ using the `contrast` function from that package.
    ```{r}
    contrast(emmeans(m, ~stratum),
      method = list(stratum = c(89/237,61/237,40/237,47/237)), infer = TRUE)
    contrast(emmeans(m, ~stratum),
      method = list(stratum = c(89,61,40,47)), infer = TRUE)
    ```
    Note that we could also write `c(89/237,61/237,40/237,47/237)` as `c(89,61,40,47)/237`.

[^samplingdesign]: While this linear model will provide appropriate estimates, the standard errors and thus confidence intervals and tests will not be accurate because of the sampling design. This survey sampled the sections *without replacement* which causes the observations to be *dependent* because if one section is in the sample then it cannot be observed again. Taking this into account requires modifying the standard errors. This is standard practice in survey research, but it requires specialized software (e.g., see the **survey** package in R). If the number of sampled units is much smaller than the number of units that could be sampled (i.e., the population size), then little or no adjustment is necessary, but that is not the case here. Also the way the standard error is usually computed in survey research does not assume that the variance is the same across the strata, but that is being implicitly assumed here. So consider that we are using these data as an exercise but other than the point estimates the inferences are questionable.

[^kruuk]: Kruuk, H., Moorhouse, A., Conroy, J. W. H., Durbin, L., & Frears, S. (1989). An estimate of numbers and habitat preferences of otters *Lutra lutra* in Shetland, UK. *Biological Conservation*, *49(4)*, 241--254.

[^dotplot]: Dot plots are like scatter plots except they stack dots. I find them useful for smaller studies with categorical explanatory variables. They can be a bit tricky to make using `ggplot`. 

## Anger Management Study

The data frame `AngerManagement` from the package **restriktor** is from a study of the effectiveness of different types of anger management exercises on aggression.[^hoijtink] Subjects were randomly assigned to one of four treatment groups: no exercises, physical exercises, behavioral exercises, or both physical and behavioral exercises. The response variable was the reduction in aggression between the beginning and end of the study, so that a positive value means a reduction, a negative number means an increase, and zero means no change. Here is what the data look like.
```{r}
library(restriktor)
head(AngerManagement)
```
The data also includes the age of each subject, but that will not be used here. I am going to make a couple of changes to the data. One is to rename the control condition from `No` to `None`, and the other is to order the factor levels. There are various ways to manipulate factors and their levels. I like to use functions from the **forcats** package. The following creates a new variable called `treatment` that we will use in place of `Group` that has the properties I want. 
```{r}
library(forcats)
library(dplyr)
AngerManagement <- AngerManagement %>%
  mutate(treatment = fct_recode(Group, "None" = "No")) %>%
  mutate(treatment = fct_relevel(treatment, c("None", "Physical", "Behavioral", "Both")))
```
This is not necessary. We could use the original variable `Group`, although by changing the order of the levels we do change the parameterization of the model.[^levelorder] Here is a dot plot showing the data.
```{r, fig.height = 3}
p <- ggplot(AngerManagement, aes(x = treatment, y = Anger)) + theme_minimal() +
  geom_dotplot(binaxis = "y", binwidth = 1, stackdir = "center") +
  labs(x = "Treatment", y = "Aggression Reduction")
plot(p)
```
Here are some descriptive statistics for the aggression level reduction by treatment (mean, standard deviation, and sample size).
```{r}
AngerManagement %>% group_by(treatment) %>%
  summarize(meanagg = mean(Anger), sdagg = sd(Anger), n = n())
```
In what follows you will use a linear model to evaluate the effectiveness of physical and behavioral exercises for anger management.

1. Estimate a linear model using the `lm` function with `Anger` as your response variable and `treatment` as your explanatory variable. Show the output from `summary` and use this to explain what $x_{i1}$, $x_{i2}$, and $x_{i3}$ represent in the linear model
$$
  E(Y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3},
$$
where $Y_i$ is the $i$-th observation of aggression reduction. Finally, write the model case-wise to show how $E(Y_i)$ can be expressed as a function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$ for each of the four treatment conditions. 

    **Solution**: We can estimate the model as follows.
    ```{r}
    m <- lm(Anger ~ treatment, data = AngerManagement)
    summary(m)$coefficients
    ```
    The output of `summary` shows that indicator variables were created for all but the `None` levels of `treatment` so that 
$$
x_{i1} = 
\begin{cases}
1, & \text{if the treatment for the $i$-th observation is physical}, \\
0, & \text{otherwise},
\end{cases}
$$
$$
x_{i2} = 
\begin{cases}
1, & \text{if the treatment for the $i$-th observation is behavioral}, \\
0, & \text{otherwise},
\end{cases}
$$
$$
x_{i3} = 
\begin{cases}
1, & \text{if the treatment for the $i$-th observation is both}, \\
0, & \text{otherwise}.
\end{cases}
$$
Thus we have that the model can be written as
$$
E(Y_i) = 
\begin{cases}
\beta_0,           & \text{if the treatment for the $i$-th observation is none}, \\
\beta_0 + \beta_1, & \text{if the treatment for the $i$-th observation is physical}, \\
\beta_0 + \beta_2, & \text{if the treatment for the $i$-th observation is behavioral}, \\
\beta_0 + \beta_3, & \text{if the treatment for the $i$-th observation is both}.
\end{cases}
$$

0. Let $\mu_n$, $\mu_p$, $\mu_b$, and $\mu_{pb}$ denote the expected anger reduction when the treatment is no exercises, physical exercises, behavioral exercises, and both physical and behavioral exercises, respectively. Write each of these as functions of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$. Then write each of the following as functions of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$: $\mu_p - \mu_n$, $\mu_b - \mu_n$, $\mu_{pb} - \mu_n$, $\mu_b - \mu_p$, $\mu_{pb} - \mu_p$ and $\mu_{pb} - \mu_b$.

    **Solution**: First note that $\mu_n = \beta_0$, $\mu_p = \beta_0 + \beta_1$, $\mu_b = \beta_0 + \beta_2$, and $\mu_{pb} = \beta_0 + \beta_3$. From this we have that
    \begin{align*}
    \mu_p - \mu_n &= \beta_0 + \beta_1 - \beta_0 = \beta_1, \\
    \mu_b - \mu_n &= \beta_0 + \beta_2 - \beta_0 = \beta_2, \\
    \mu_{pb} - \mu_n & = \beta_0 + \beta_3 - \beta_0 = \beta_3, \\
    \mu_b - \mu_p &= \beta_0 + \beta_2 - (\beta_0 + \beta_1) = \beta_2 - \beta_1, \\
    \mu_{pb} - \mu_p &= \beta_0 + \beta_3 - (\beta_0 + \beta_1) = \beta_3 - \beta_1, \\
    \mu_{pb} - \mu_b &= \beta_0 + \beta_3 - (\beta_0 + \beta_2) = \beta_3 - \beta_2.
    \end{align*}

0. Use the `lincon` and `contrast` functions to estimate each of the quantities that you estimated in the previous problem. You should obtain the same results for each function.

    **Solution**: We can estimate these quantities as follows.
    ```{r}
    lincon(m, a = c(1,0,0,0))  # b0
    lincon(m, a = c(1,1,0,0))  # b0 + b1
    lincon(m, a = c(1,0,1,0))  # b0 + b2
    lincon(m, a = c(1,0,0,1))  # b0 + b3
    lincon(m, a = c(0,1,0,0))  # b1
    lincon(m, a = c(0,0,1,0))  # b2
    lincon(m, a = c(0,0,0,1))  # b3
    lincon(m, a = c(0,-1,1,0)) # b2 - b1
    lincon(m, a = c(0,-1,0,1)) # b3 - b1
    lincon(m, a = c(0,0,-1,1)) # b3 - b2
    trtools::contrast(m, a = list(treatment = c("None","Physical","Behavioral","Both")))
    trtools::contrast(m, a = list(treatment = "Physical"), b = list(treatment = "None"))
    trtools::contrast(m, a = list(treatment = "Behavioral"), b = list(treatment = "None"))
    trtools::contrast(m, a = list(treatment = "Both"), b = list(treatment = "None"))
    trtools::contrast(m, a = list(treatment = "Behavioral"), b = list(treatment = "Physical"))    
    trtools::contrast(m, a = list(treatment = "Both"), b = list(treatment = "Physical"))    
    trtools::contrast(m, a = list(treatment = "Both"), b = list(treatment = "Behavioral"))    
    ```
    Note that we have estimated all possible paired comparisons between the treatment conditions. This is fairly easy to do using the **emmeans** package since it requires only one statement.
    ```{r}
    emmeans(m, ~treatment)    
    pairs(emmeans(m, ~treatment), infer = TRUE, adjust = "none", reverse = TRUE)
    ```

0. The model used above uses a single factor with four levels, corresponding to what is sometimes called a *one-way design*. But it could also be viewed as a *factorial design* with two factors: use of physical exercises (yes or no), and use of behavioral exercises (yes or no).[^factorial] Students that are familiar with the analysis of factorial designs using an analysis of variance approach might remember the concepts of *main effects* and *interactions*. Inferences for main effects and interactions can be made even if we do not explicitly specify the model as having the two factors (with an interaction) as explanatory variables. Here the main effect for physical exercise can be written in terms of the difference between the average expected response for treatment conditions with physical exercise versus that for the treatment conditions without physical exercise. This can be written as
$$
  (\mu_p + \mu_{pb})/2 - (\mu_b + \mu_n)/2.
$$
Similarly the main effect for behavioral exercise can be written as 
$$
  (\mu_b + \mu_{pb})/2 - (\mu_p + \mu_n)/2.
$$
Finally the interaction can be written in terms of the difference in the effect of adding one type of exercise when the other type of exercise is being used versus when it is not, which can be written as
$$
  \mu_{pb} - \mu_p - (\mu_b - \mu_n)
$$
or, alternatively, as $\mu_{pb} - \mu_b - (\mu_p - \mu_n)$ which is algebraically equivalent. Write each of the three quantities above as functions of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$ by substituting each $\mu$ by the corresponding function of those parameters, and simplifying. You should find that each of these quantities can be written as a linear combination of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$. Use the `lincon` function to estimate each of these quantities. If you do this correctly the $p$-values you get should be the same as those given by the ANOVA table shown below, and the $F$ test statistics shown below should equal (approximately due to rounding) the squares of the $t$ test statistics reported by `lincon`.[^anova]
    ```{r}
    library(car)
    AngerManagement <- AngerManagement %>% 
      mutate(behavioral = ifelse(treatment %in% c("Behavioral","Both"), "yes", "no")) %>%
      mutate(physical = ifelse(treatment %in% c("Physical","Both"), "yes", "no"))
    m <- lm(Anger ~ behavioral + physical + behavioral:physical, data = AngerManagement)
    Anova(m)
    ```
    
    **Solution**: I am going to estimate the model again here because in the code above I specified a different parameterization.
    ```{r}
    m <- lm(Anger ~ treatment, data = AngerManagement)
    ```
The main effect of physical exercises can be written as 
$$
(\mu_p + \mu_{pb})/2 - (\mu_b + \mu_n)/2 = 
(2\beta_0 + \beta_1 + \beta_3)/2 - 
(2\beta_0 + \beta_2)/2 = \beta_1/2 + \beta_3/2 - \beta_2/2. 
$$
We can estimate this as follows.
    ```{r}
    lincon(m, a = c(0,0.5,-0.5,0.5))
    ```
    The main effect of behavioral exercises can be written as
    $$
    (\mu_b + \mu_{pb})/2 - (\mu_p + \mu_n)/2 = 
    (2\beta_0 + \beta_2 + \beta_3)/2 - 
    (2\beta_0 + \beta_1)/2 = \beta_2/2 + \beta_3/2 - \beta_1/2.
    $$
    We can estimate this as follows.
    ```{r}
    lincon(m, a = c(0,-0.5,0.5,0.5))
    ```
    Finally the interaction can be written as
    $$
    \mu_{pb} - \mu_p - (\mu_b - \mu_n) = \beta_0 + \beta_3 - (\beta_0 - \beta_1) - (\beta_0 + \beta_2) + \beta_0 = \beta_3 - \beta_1 - \beta_2.
    $$
    We can estimate this as follows.
    ```{r}
    lincon(m, a = c(0,-1,-1,1))
    ```
    
    Note that if the $t$ test statistics are squared we get (approximately) the $F$ test statistics, and the p-values are the same.
    
[^factorial]: Interestingly a model for a factorial design can always be written by "collapsing" the two or more factors of the design into one factor where each level of that factor is a combination of levels of the factors of the factorial design.     
    
[^anova]: Although I do not like to use ANOVA tables, and I do not recommend using them outside of some very specialized applications, if you must produce them then I would recommend using the `Anova` function from the **car** package. But only use this function if you fully understand how it works. 

[^hoijtink]: Hoijtink, H. (2012). *Informative hypotheses: Theory and practice for behavioral and social scientists*. Taylor & Francis. I am fairly certain that these data are fictional, but maybe not completely unrealistic. 

[^levelorder]: Usually when `lm` creates indicator variables it will create them for all but the first level. If the levels are not ordered then that will be the first level when they are ordered alphabetically. But here it will be that for the `None` level. A side effect of ordering the levels is that it allows us to control how they appear in a plot when using `ggplot` which is sometimes desirable. 