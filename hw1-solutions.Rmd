---
title: "Linear Model Specification and Interpretation"
subtitle: Statistics 516, Homework 1 (Solutions)
output:
  html_document:
    theme: readable
  pdf_document: default
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{array}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", message = FALSE, out.width = "100%", fig.align = "center", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](hw1.pdf) copy of this homework assignment.", sep = ""), "")`

This homework assignment concerns specifying and the interpreting (via inference) linear models using data from several studies. In particular, you will see how to make inferences concerning linear combinations of model parameters. You will likely need to install several packages to access the data. These include the **bootstrap** and **agridat** packages, as well as the **trtools** and **ggplot2** packages which you should have already installed. 

```{r, echo = FALSE}
options(digits = 4, width = 90)
```

## Bumpus' Sparrows

```{r, echo = FALSE}
library(dplyr)
d <- trtools::bumpus %>% group_by(survival) %>% 
  summarize(ybar = mean(humerus), sd = sd(humerus), n = n())
m <- d$ybar
s <- d$sd
n <- d$n

ys <- trtools::bumpus$humerus[trtools::bumpus$survival == TRUE]
yn <- trtools::bumpus$humerus[trtools::bumpus$survival == FALSE]

tmp <- t.test(ys, yn, var.equal = TRUE)
lower <- round(tmp$conf.int[1], 3)
upper <- round(tmp$conf.int[2], 3)
tstat <- round(tmp$statistic, 3)
```

A famous [lecture](https://amornithhistory.org/2018/03/05/professor-bumpus-and-his-sparrows) by biologist [Hermon Bumpus](https://en.wikipedia.org/wiki/Hermon_Carey_Bumpus) demonstrated natural selection using data concerning the survival of house sparrows (*Passer domesticus*) after a severe winter storm.[^bumpus] After the storm, moribund sparrows were brought to the Anatomical Laboratory at Brown University. Some of these sparrows were revived, but many died. All of the sparrows that were brought in were examined with respect to a variety of anatomical characteristics. It is interesting to compare the anatomical characteristics of sparrows that survived versus those that did not. Dot plots showing the distributions of humerus (upper wing bone) length in the samples of sparrows that survived and the sparrows that did not are shown below.[^dotplots]
```{r, fig.height = 2.25}
library(trtools)
library(ggplot2)

bumpus$survived <- factor(bumpus$survival, 
  levels = c(TRUE,FALSE), labels = c("yes","no"))

p <- ggplot(bumpus, aes(x = survived, y = humerus)) + theme_minimal() + 
  geom_dotplot(binaxis = "y", binwidth = 0.001) + coord_flip() + 
  labs(x = "Survived", y = "Humerus Length (in)")
plot(p)
```
Note that I created a new variable `survived` from the `survival` variable. This is not necessary, and you could use the original `survival` variable. I did it to change the labels to "yes" and "no".[^factor] The following code shows how you can use the **dplyr** package to compute the sample means, standard deviations, and sizes for the two samples of sparrows.[^dplyr]
```{r}
library(dplyr)
bumpus %>% group_by(survived) %>% 
  summarize(ybar = mean(humerus), sd = sd(humerus), n = n())
```
So the sample means for the observations of non-surviving and surviving sparrows are $\bar{y}_n$ = `r round(m[1], 3)` and $\bar{y}_y$ = `r round(m[2], 3)`, respectively, the sample standard deviations are $s_n$ = `r round(s[1], 4)` and $s_y$ = `r round(s[2], 4)`, respectively, and the sample sizes are $n_n$ = `r n[1]` and $n_y$ = `r n[2]`, respectively. Note that the sample means and standard deviations are rounded.[^signifdigits] Let $\mu_n$ and $\mu_y$ be the "population means" for humerus length for non-surviving and surviving sparrows, respectively, or what we would call the expected humerus lengths.[^popmean] In an introductory statistics course you learned a variety of ways to make inferences using data like these. You learned how to compute a confidence interval for $\mu_y - \mu_n$ such as
$$
  \bar{y}_y - \bar{y}_n \pm ts_p\sqrt{1/n_y + 1/n_n},
$$
where
$$
  s_p = \sqrt{\frac{(n_y-1)s_y^2 + (n_n-1)s_n^2}{n_y + n_n - 2}},
$$
is the "pooled" estimate of $\sigma$, the standard deviation of humerus length, and $t$ is a value from the *t*-distribution with $n_y + n_n - 2$ degrees of freedom that is used to specify the confidence level.[^pooled] The test statistic for a "t-test" of the null hypothesis that $\mu_y - \mu_n$ = 0 (i.e, $\mu_y = \mu_n$) is
$$
  t = \frac{\bar{y}_y - \bar{y}_n}{s_p\sqrt{1/n_y + 1/n_n}}.
$$
If you were to compute the confidence interval and test statistic using the formulas above you would get a confidence interval (with a 95% confidence level) for $\mu_y - \mu_n$ of approximately (`r lower` in, `r upper` in) and a test statistic for the null hypothesis $\mu_y - \mu_n$ = 0 of approximately *t* = `r tstat`. Here you will see how to make these inferences and others using a *linear model*.

1. Estimate a linear model using the `lm` function with humerus length as the response variable and survival as the explanatory variable. Report the parameter estimates and their standard errors using the `summary` function.

    **Solution**: We can estimate the model and produce the parameter estimates and standard errors as follows.
    ```{r}
    m <- lm(humerus ~ survived, data = bumpus)
    summary(m)$coefficients
    ```

2. The model you estimated in the previous problem can be written as 
$$
  E(Y_i) = \beta_0 + \beta_1 x_i,
$$
where $Y_i$ is the $i$-th observation of humerus length. Explain how the value of $x_i$ is defined for this model (i.e., how would you determine the value of $x_i$ for a given sparrow?). Write the model case-wise to express the expected humerus length as a function of $\beta_0$ and $\beta_1$ for sparrows that survived and those that did not. Let $\mu_y$ and $\mu_n$ be the expected humerus length of a sparrow that did and did not survive, respectively. Using the case-wise representation of the model, write each of these parameters as a function of $\beta_0$ and/or $\beta_1$ (i.e., how would you compute $\mu_y$ and $\mu_n$ using $\beta_0$ and $\beta_1$?).

    **Solution**: We can see from `summary` that $x_i$ is an indicator variable defined as $x_i$ = 1 if the $i$-th observation is of a non-surviving sparrow, and $x_i$ = 0 otherwise. Thus the model can be written case-wise as
    $$
    E(Y_i) = 
    \begin{cases}
      \beta_0, & \text{if the $i$-th observation is of a surviving sparrow}, \\
      \beta_0 + \beta_1, & \text{if the $i$-th observation is of a non-surviving sparrow}.
    \end{cases}
    $$
    Thus we have that $\mu_y = \beta_0$ and $\mu_n = \beta_0 + \beta_1$. It is important to note that if the variable `survival` is used instead of `survived` as the explanatory variable that the indicator variable will then be one when survival is `TRUE`, which reverses the definitions of $\mu_y$ and $\mu_n$ in terms of the parameters. But the inferences (if specified correctly) will be the same. 

3. Using the `lincon` *and* `contrast` functions, produce estimates, standard errors, and confidence intervals for $\mu_y$ and $\mu_n$. For the `lincon` function, use the fact that each of these parameters can be written as a function of $\beta_0$ and/or $\beta_1$. Note that the results from `lincon` and `contrast` should be the same.[^onesample]

    **Solution**: We can obtain inferences for $\mu_y$ and $\mu_n$ using `lincon` as follows.
    ```{r}
    library(trtools)
    lincon(m, a = c(1,0)) # surviving
    lincon(m, a = c(1,1)) # non-surviving
    ```
    The same inferences can be obtained using `contrast` as follows.
    ```{r}
    trtools::contrast(m, a = list(survived = c("yes","no")), cnames = c("yes","no"))
    ```
    Note that I am going to use `trtools::contrast` in these solutions because I will also be using the **emmeans** package. Here are how we can obtain these inferences using the **emmeans** package.
    ```{r}
    library(emmeans)
    emmeans(m, ~ survived)
    ```

4. Using the `lincon` *and* `contrast` functions, produce an estimate, standard error, and confidence interval for $\mu_y - \mu_n$, as well as the test statistic and p-value for a test of the null hypothesis that $\mu_y - \mu_n$ = 0. Note that the results from `lincon` and `contrast` should be the same. Also note that your confidence interval and test statistic should be the same as those shown in the problem description above.

    **Solution**: Here are the inferences using `lincon` and `contrast`. Note that $\mu_y - \mu_n = \beta_0 - (\beta_0 + \beta_1) = -\beta_1$.
    ```{r}
    lincon(m, a = c(0,-1))
    trtools::contrast(m, 
      a = list(survived = "yes"), 
      b = list(survived = "no"))
    ```
    And here is how it can be done using the **emmeans** package.
    ```{r}
    pairs(emmeans(m, ~ survived), infer = TRUE)
    ```
    Note that the confidence interval and test statistic are the same as those given using the formulas given in the problem description.

[^bumpus]: Bumpus, H. C. (1898). Eleventh lecture. The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. (A fourth contribution to the study of variation.) Biological Lectures: Woods Hole Marine Biological Laboratory, 209--225.

[^onesample]: In an introductory statistics class you would have learned how to compute a confidence interval for a single population mean as $\bar{y} \pm ts/\sqrt{n}$. Here we are essentially doing the same thing, except for each group, and replacing $s$ by $s_p$ and using a degrees of freedom of $n_y + n_n - 2$. Here the model uses *both* samples to estimate one standard deviation for both populations. 

[^dotplots]: I find dot plots to be useful sometimes for showing the distribution of a quantitative variable rather than a histogram or box plot, particularly when there are relatively few observations. They can be a bit tricky sometimes to specify when using the **ggplot2** package, but you'll see some examples that you can copy from my lectures and homework assignments.

[^factor]: This is one easy way to change the labels of the categories/levels of a categorical/factor variable. You can also just change the categories/levels of an existing variable, and I may show you an example of that later. Note that the original variable `survived` is what is sometimes called a "logical" that takes on values of either `TRUE` or `FALSE`. Unlike the values of a categorical variable or factor, the values of a logical variable are not put in quotes.

[^dplyr]: The **dplyr** package is extraordinarily useful for manipulating data, sometimes in combination with the **tidyr** package. 

[^signifdigits]: Here the function responsible for printing the output is automatically computing what it determines to be the number of significant digits to display. But you can override this with, say, `options(pillar.sigfig = 5)` to display five significant digits.

[^popmean]: The concept of "population mean" is used more often in introductory classes where the population might be viewed as a real or conceptual set of observations. In a survey it might refer to a large but finite collection of things on which we make observations. But in an observational study like this the populations are perhaps best thought of as the hypothetical and infinite set of observations from which we are "sampling" when we make our observations. 

[^pooled]: Using this "pooled" estimate assumes that the "population variance" of humerus length is the same for both populations (i.e., $\sigma_y^2 = \sigma_n^2$). An alternative approach (sometimes called [Welch's *t*-test](https://en.wikipedia.org/wiki/Welch%27s_t-test)) that does not make this assumption replaces 
$$
s_p\sqrt{1/n_y + 1/n_n}
$$
with 
$$
\sqrt{s_y^2/n_y + s_n^2/n_n},
$$
and modifies the degrees of freedom. This approach is also often covered in introductory statistics courses. The linear models we are using now assume that the variance stays constant and so is consistent with the assumption that $\sigma_y^2 = \sigma_n^2$, but we will later discuss how to deal with situations where this assumption is not reasonable. 

## Anti-Inflammatory Hormone Devices

For this problem you will be using the data frame `hormone` from the **bootstrap** package. The data are from a fictional study of devices for delivering anti-inflammatory hormones. The primary goal of the study is compare devices from three different manufacturing lots with respect to the amount of hormone remaining in the devices after use. The plot below shows the distribution of hormone remaining for the devices from the three lots.
```{r, fig.height = 2}
library(bootstrap)
library(ggplot2)

p <- ggplot(hormone, aes(x = Lot, y = amount)) + theme_minimal() + 
  geom_point(alpha = 0.5) + coord_flip() + labs(y = "Amount Remaining")
plot(p)
```
The following shows the mean for amount remaining by lot.
```{r}
library(dplyr)

hormone %>% group_by(Lot) %>% summarize(ybar = mean(amount))
```
The goal here is to make inferences about the expected amount of hormone remaining for devices from the three manufacturing lots.

1. Estimate a linear model using the `lm` function where the response variable is the amount of hormone remaining and the explanatory variable is lot. Report the parameter estimates and standard errors using the `summary` function.

    **Solution**: This model can be estimated as follows. 
    ```{r}
    m <- lm(amount ~ Lot, data = hormone)
    summary(m)$coefficients
    ```

0. The model you estimated in the previous problem can be written as
$$
  E(Y_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2},
$$
where $Y_i$ is the $i$-th observation of the amount of hormone remaining. Explain how $x_{i1}$ and $x_{i2}$ are defined for this model (i.e., how would you determine their values for a given device?). Then write the model case-wise to show how the expected amount of hormone remaining for each lot can be written as a function of $\beta_0$, $\beta_1$, and/or $\beta_2$.

    **Solution**: Inspection of the output from `summary` shows that $x_{i1}$ = 1 if the $i$-th observation is from Lot B, and $x_{i1}$ = 0 otherwise, and $x_{i2}$ = 1 if the $i$-th observation is from Lot C, and $x_{i2}$ = 0 otherwise. So the model can be written case-wise as 
$$
  E(Y_i) = 
  \begin{cases}
    \beta_0, & \text{if the $i$-th observation is from lot A}, \\
    \beta_0 + \beta_1, & \text{if the $i$-th observation is from lot B}, \\
    \beta_0 + \beta_2, & \text{if the $i$-th observation is from lot C}.
  \end{cases}
$$

0. Using the `contrast` *and* `lincon` functions, produce estimates, standard errors, and confidence intervals for the expected amount of hormone remaining for each lot. Note that the results from `lincon` and `contrast` should be the same.

    **Solution**: The inferences can be obtained using `lincon` and `contrast` as follows.
    ```{r}
    lincon(m, a = c(1,0,0)) # lot A
    lincon(m, a = c(1,1,0)) # lot B
    lincon(m, a = c(1,0,1)) # lot C
    trtools::contrast(m, a = list(Lot = c("A","B","C")), 
      cnames = c("Lot A","Lot B","Lot C"))
    ```
    Here is how this can be done using the **emmeans** package.
    ```{r}
    emmeans(m, ~ Lot)
    ```

0. Using the `contrast` *and* `lincon` functions, produce estimates, standard errors, and confidence intervals for the *difference* in the expected amount of hormone remaining between lot *C* and *B*, lots *C* and *A*, and between lots *A* and *B*. Note that the results from `lincon` and `contrast` should be the same.

    **Solution**: Note that the difference in the expected response between lots *C* and *B* is $\beta_0 + \beta_2 - (\beta_0 - \beta_1) = \beta_2 - \beta_1$, the difference in the expected response between lots *C* and *A* is $\beta_0 + \beta_2 - \beta_0 = \beta_2$, and the difference in the expected response between lots *A* and *B* is $\beta_0 - (\beta_0 + \beta_1) = -\beta_1$. We can estimate these differences using `lincon` as follows.
    ```{r}
    lincon(m, a = c(0,-1,1)) # beta2 - beta1
    lincon(m, a = c(0,0,1))  # beta2
    lincon(m, a = c(0,-1,0)) # -beta1
    ```
    Here is how we can make these inferences using `contrast`. Note that we can do it in one statement.
    ```{r}
    trtools::contrast(m,
      a = list(Lot = c("C","C","A")),
      b = list(Lot = c("B","A","B")),
      cnames = c("C vs B","C vs A","A vs B"))
    ```
    We can also do this using the `pairs` function in the **emmeans** package as follows.
    ```{r}
    pairs(emmeans(m, ~Lot), infer = TRUE, adjust = "none")
    ```
    Note that the direction of subtraction is different when using `pairs`. By default it appears to use the order of the categories/levels (alphabetical if not specified otherwise). But you can reverse the direction of subtraction using the `reverse = TRUE` argument.
    ```{r}
    pairs(emmeans(m, ~Lot), infer = TRUE, adjust = "none", reverse = TRUE)
    ```

0. The model and analyses in the previous problems fails to take into account that devices in the three lots tended to have different amounts of use as can be seen in the following.
    ```{r, fig.height = 2}
    p <- ggplot(hormone, aes(x = Lot, y = hrs)) + theme_minimal() + 
      geom_point(alpha = 0.5) + coord_flip() + labs(y = "Hours of Use")
    plot(p)
    hormone %>% group_by(Lot) %>% summarize(wear = mean(hrs))
    ```
As can be seen in the plot and in the descriptive statistics, the devices from lot *C* had, on average, the least hours of use, while devices from lot *B* tended to have the most hours of use. We can view the relationship among all three variables in the following plot.
    ```{r, fig.height = 4}
    p <- ggplot(hormone, aes(x = hrs, y = amount, color = Lot)) + theme_minimal() + 
      geom_point() + labs(x = "Hours of Use", y = "Amount of Hormone Remaining")
    plot(p)
    ```
There is nothing statistically incorrect about the model used in the previous problem, but it may not be useful since it does not allow for a "fair" comparison between the lots since the devices in the lots differ across lots with respect to use. A more useful comparison would be to compare the expected amount of hormone between the lots while "controlling for" hours of use --- i.e., what is the difference in the expected amount of hormone for devices from different lots but with the same amount of use? Estimate a linear model with amount of hormone remaining as a response variable and *both* lot and hours of use as explanatory variables. Do not include an "interaction" term in your model so that the rate of change in expected amount of hormone with respect to amount of use is the same for each lot. Report the parameter estimates and their standard errors using the `summary` function, parameter confidence intervals using the `confint` function, and plot the estimated expected amount of hormone remaining as a function of hours of use and lot by extending the code given above. Note that this will require you to create an artificial data set using the `expand.grid` function for different combinations of hours of use and lots. For the hours of use variable, have your values go from 29 to 402 hours, which are the smallest and largest values observed in the data.

    **Solution**: Here is how to specify the model and produce parameter estimates with confidence intervals.
    ```{r}
    m <- lm(amount ~ Lot + hrs, data = hormone)
    cbind(summary(m)$coefficients, confint(m))
    ```
    Next we can create a data frame for plotting purposes.
    ```{r}
    d <- expand.grid(Lot = c("A","B","C"), hrs = c(29,402))
    d$yhat <- predict(m, newdata = d)
    ```
    Finally we can "add" the estimated model to the plot.
    ```{r}
    p <- p + geom_line(aes(y = yhat), data = d)
    plot(p)
    ```
    
0. The model you estimated in the previous problem can be written as
$$
  E(Y_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3}.
$$
Explain how $x_{i1}$, $x_{i2}$, and $x_{i3}$ are defined (i.e., how would you determine their values for a given device). Then write the model case-wise to show how the expected amount of hormone remaining can be written as a function of $\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$, and hours of use for each lot. 

    **Solution**: The variables $x_{i1}$ and $x_{i2}$ are defined as they were in the previous model. They are indicator variables for lots *B* and *C*, respectively. Then $x_{i3}$ is simply hours of use for the $i$-th observation. The model can be written case-wise as 
    $$
    E(Y_i) = 
      \begin{cases}
        \beta_0, & \text{if the $i$-th observation is from lot A}, \\
        \beta_0 + \beta_1, & \text{if the $i$-th observation is from lot B}, \\
        \beta_0 + \beta_2, & \text{if the $i$-th observation is from lot C}.
      \end{cases}
    $$
    
0. Use the `contrast` function to estimate (a) the expected amount of hormone remaining in a device from each of the three lots that has had 200 hours of use, and (b) the *difference* in the expected amount of hormone remaining between lot *C* and *B*, lots *C* and *A*, and between lots *A* and *B*, when the hours of use is 200 hours. Comment briefly on how your comparisons between the lots in (b) compare to what you found earlier when you did not control for hours of use. 

    **Solution**: Here are the estimated expected amount of hormone remaining for a device from each lot after 200 hours of use.
    ```{r}
    trtools::contrast(m, a = list(Lot = c("A","B","C"), hrs = 200),
      cnames = c("A","B","C"))
    ```
    And here are the pairwise differences.
    ```{r}
    trtools::contrast(m,
      a = list(Lot = c("C","C","A"), hrs = 200),
      b = list(Lot = c("B","A","B"), hrs = 200),
      cnames = c("C vs B","C vs A","A vs B"))
    ```
    We can also do this with the **emmeans** package by using the `at` argument to specify the value of hours of use.
    ```{r}
    emmeans(m, ~Lot, at = list(hrs = 200))
    pairs(emmeans(m, ~Lot, at = list(hrs = 200)), infer = TRUE, adjust = "none")
    ```
    By controlling for hours of use (i.e., making comparisons between lots for the same number of hours of use) the inferences for the comparisons between the lots has changed. When not controlling for hours of use devices from lot *C* showed higher expected responses in comparison to devices from the other two lots. But when controlling for hours of use we no longer see a clear difference when comparing devices from lots *C* and *B*. Also, whereas before there was no clear difference in devices from lots *A* and *B*, when controlling for hours of use we see that devices from lot *B* tend to have higher expected responses in comparison to devices from lot *A*. 
    
0. In the model used in the previous questions with hours of use as an explanatory variable with lot, one of the $\beta_j$ parameters in the model is the rate of change in the expected amount of hormone remaining with respect to hours of use (i.e., the change in the expected amount of hormone remaining for a one hour increase in use). Because of how the model was specified, this is the same for the three lots. Now use the `contrast` function to estimate this same quantity for each lot. You should obtain the same estimate, standard error, confidence interval, and test statistic as for the corresponding $\beta_j$ parameter as shown by `summary` and `confint`, and these should be the same for each lot. Also use the `contrast` function to estimate the change in the expected amount of hormone remaining for a 100 hour increase in use. Note that your estimate, standard error, and confidence interval endpoints should be 100 times what you found for a one hour increase, but the test statistic should be the same.

    **Solution**: In this model $\beta_3$ is the rate of change in the expected amount of hormone remaining per hour of use. We can also estimate this using `contrast` as follows.
    ```{r}
    trtools::contrast(m, 
      a = list(Lot = c("A","B","C"), hrs = 2),
      b = list(Lot = c("A","B","C"), hrs = 1),
      cnames = c("A","B","C"))
    ```
    The rate of change can also be estimated using the `emtrends` function from the **emmeans** package.
    ```{r}
    emtrends(m, ~Lot, var = "hrs")
    ```
    For a 100 hour increase we can estimate the rate of change as follows.
    ```{r}
    trtools::contrast(m, 
      a = list(Lot = c("A","B","C"), hrs = 200),
      b = list(Lot = c("A","B","C"), hrs = 100),
      cnames = c("A","B","C"))
    ```      

0. Consider the following model "formula" argument for the `lm` function: `amount ~ Lot:hrs`.[^intercept] It may not be clear exactly what kind of model this specifies, but you can deduce the model from the output from `summary`. Estimate this model and give the parameter estimates and their standard errors using the `summary` function. Also plot the model with the raw data like you did with the previous model. This model can be written as
$$
  E(Y_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3},
$$
but now $x_{i1}$, $x_{i2}$, and $x_{i3}$ are different from what they were in the previous model, and so $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ have different interpretations as well. Explain how $x_{i1}$, $x_{i2}$, and $x_{i3}$ are defined for this model (i.e., how would you determine their values for a given device). Write the model case-wise to show how the expected amount of hormone remaining can be written as a function of $\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$, and hours of use for each lot. Use the `contrast` function to estimate (a) the expected amount of hormone remaining in devices from each lot after zero hours of use, and after 200 hours of use, and (b) the change in the expected amount of hormone for a one hour increase in the amount of use for each lot. Compare these estimates to the parameter estimates from `summary`, and briefly discuss how one would interpret the parameters $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ in terms of the relationship between expected hormone remaining as a function of lot and hours of use. **Note**: This problem is *extra credit* for students enrolled in Stat 436, but is *required* for students enrolled in Stat 516.

    **Solution**: Here are the parameter estimates.
    ```{r}
    m <- lm(amount ~ Lot:hrs, data = hormone)
    summary(m)$coefficients
    ```
    According to the output, $x_{i1}$ is the *product* of an indicator variable for an observation from lot *A* and the hours of use. Similarly $x_{i2}$ and $x_{i3}$ are the products of indicator variables for lots *B* and *C*, respectively, and hours of use. So the model can be written case-wise as
    $$
    E(Y_i) = 
    \begin{cases}
    \beta_0 + \beta_1 h_i, & \text{if the $i$-th observation is from lot A}, \\
    \beta_0 + \beta_2 h_i, & \text{if the $i$-th observation is from lot B}, \\
    \beta_0 + \beta_3 h_i, & \text{if the $i$-th observation is from lot C}. \\
    \end{cases}
    $$
    Using contrast we can estimate the expected amount of hormone remaining for each lot after 0 and 100 hours of use.
    ```{r}
    trtools::contrast(m, a = list(Lot = c("A","B","C"), hrs = 0), 
      cnames = c("A","B","C"))
    trtools::contrast(m, a = list(Lot = c("A","B","C"), hrs = 100), 
      cnames = c("A","B","C"))
    ```
    Here is how we could do that with the **emmeans** package.
    ```{r}
    emmeans(m, ~Lot, at = list(hrs = 0))
    emmeans(m, ~Lot, at = list(hrs = 100))
    ```
    Note that the estimated expected response at zero hours is the same for all three lots, and these are also equal to the estimate for $\beta_0$. In this model $\beta_0$ represents the expected amount of hormone remaining at zero hours for all three lots. Assuming that the devices from the three lots start with the same amount of hormone, on average, and that any differences are due to measurement error or random differences in how the devices are filled that do not systematically vary between lots, such a model may be reasonable. Now consider the estimates of the change in the expected amount of hormone remaining after one additional hour of use for each lot.
    ```{r}
    trtools::contrast(m,
      a = list(Lot = c("A","B","C"), hrs = 2),
      b = list(Lot = c("A","B","C"), hrs = 1),
      cnames = c("A","B","C"))
    ```
    We can also do this using the `emtrends` function from the **emmeans** package.
    ```{r}
    emtrends(m, ~Lot, var = "hrs")
    ```
    Note that these estimates are the same as those for $\beta_1$, $\beta_2$, and $\beta_3$. In this model, those parameters equal the rate of change in the expected amount of hormone remaining for devices from each lot. You can also use the `pairs` function from the **emmeans** package to compare these parameters.
    ```{r}
    pairs(emtrends(m, ~Lot, var = "hrs"), infer = TRUE, adjust = "none")
    ```
    So we can see, for example, that the differences in the rates of change are statistically significant when comparing lots *A* and *B* and when comparing lots *A* and *C*, but not when comparing lots *B* and *C*. The `contrast` function can also do this, but it is a bit more tedious. 
    
[^intercept]: In the original problem I had asked you to use the model formula `amount ~ 1 + Lot:hrs` rather than `amount ~ Lot:hrs` which is what I have here. They both result in the same model. The `1` is not necessary.
     
## Daphnia Survey

The data in the data frame `daphniastrat` from the **trtools** package are from a survey of daphnia (water fleas) in a fresh water lake.[^daphnia] Researchers obtained one-liter samples of water from three depth layers: the *epilimnion* (the warmer surface layer), the *thermocline* (the middle layer between the warmer and colder layers), and the *hypolimnion* (the colder bottom layer). The number of daphnia within each water sample was then recorded. A plot of the raw data is shown below.
```{r, fig.height = 3}
library(ggplot2)
library(trtools)
p <- ggplot(daphniastrat, aes(x = layer, y = count)) + theme_minimal() + 
  geom_dotplot(binaxis = "y", stackdir = "center") + 
  labs(x = "Layer", y = "Number of Daphnia")
plot(p)
```
Some descriptive statistics of the number of daphnia for each layer can be obtained as follows using the **dplyr** package.
```{r}
library(dplyr)
daphniastrat %>% group_by(layer) %>% 
  summarize(mean = mean(count), sd = sd(count), samples = n())
```
The following concern inferences about the daphnia within each layer and in the entire lake.

1. Specify a linear model using the `lm` function with count as the response variable and layer as the explanatory variable. Report the parameter estimates using the `summary` function. Let $\mu_e$, $\mu_t$, and $\mu_h$ represent the expected number of daphnia in one liter of water sampled from the epilimnion, thermocline, and hypolimnion layers, respectively. If we assume simple random sampling of the one liter samples from each layer, then $\mu_e$, $\mu_t$, and $\mu_h$ then also represent the mean number of daphnia for epilimnion, thermocline, and hypolimnion layers, respectively (i.e., the daphnia density in each layer). Write each of these parameters as a function of $\beta_0$, $\beta_1$, and/or $\beta_2$.

    **Solution**: We can specify the model and obtain the parameter estimates as follows.
    ```{r}
    m <- lm(count ~ layer, data = daphniastrat)
    summary(m)$coefficients
    ```
    Note that we can write the model case-wise as
    $$
      E(Y_i) = 
      \begin{cases}
      \beta_0, & \text{if the $i$-th water sample is from the epilimnion layer}, \\
      \beta_0 + \beta_1, & \text{if the $i$-th water sample is from the thermocline layer}, \\
      \beta_0 + \beta_1, & \text{if the $i$-th water sample is from the hypolimnion layer}.
      \end{cases}
    $$
    This implies that $\mu_e = \beta_0$, $\mu_t = \beta_0 + \beta_1$, and $\mu_h = \beta_0 + \beta_2$. 
    
0. The volumes of the epilimnion, thermocline, and hypolimnion layers are 100kL, 200kL, and 400kL, respectively, so the volume of the lake as a whole is 700kL or 700000 liters. The mean number of daphnia per liter for the whole lake, denoted here as $\mu$, is therefore the weighted average of the mean number of daphnia per liter from each layer computed as
$$
  \mu = \tfrac{1}{7}\mu_e + \tfrac{2}{7}\mu_t + \tfrac{4}{7}\mu_h.
$$
The *total* number of daphnia in the lake, which we might represent as the parameter $\tau$, is equal to 700000$\mu$, so that
$$
  \tau = 100000\mu_e + 200000\mu_t + 400000\mu_h.
$$
In the previous problem you expressed $\mu_e$, $\mu_t$, and $\mu_h$ as functions of the parameters $\beta_0$, $\beta_1$, and $\beta_2$. In the expressions for $\mu$ and $\tau$ above, substitute $\mu_e$, $\mu_t$, and $\mu_h$ with the corresponding function of $\beta_0$, $\beta_1$, and $\beta_2$, and then simplify the expressions so that $\mu$ and $\tau$ are then written as linear combinations of $\beta_0$, $\beta_1$, and $\beta_2$. Then use the `lincon` function to compute estimates of $\mu$ and $\tau$ as well as confidence intervals for these parameters and the standard errors of the estimators.[^stratified]

    **Solution**: Note that 
    $$
      \mu = \beta_0 + \tfrac{2}{7}\beta_1 + \tfrac{4}{7}\beta_2,
    $$
    and
    $$
      \tau = 700000\beta_0 + \tfrac{1400000}{7}\beta_1 + \tfrac{2800000}{7}\beta_2 = 700000\beta_0 + 200000\beta_1 + 400000\beta_2.
    $$
    So we can estimate $\mu$ and $\tau$ using `lincon` as follows.
    ```{r}
    lincon(m, a = c(1, 2/7, 4/7))
    lincon(m, a = c(700000, 200000, 400000)) 
    ```
    Note that you can also have R do the multiplication for you for the coefficients for $\tau$.
    ```{r}
    lincon(m, a = 700000 * c(1, 2/7, 4/7))
    ```
    Inferences for $\mu$ can also be obtained using the **emmeans** package.
    ```{r}
    levels(daphniastrat$layer) # checking the order of the levels
    emmeans(m, ~1, weights = c(1/7, 2/7, 4/7))
    ```
    We cannot use this approach to estimate $\tau$ because the `weights` argument for the `emmeans` function will normalize the weights so that they sum to one. But since $\tau = 700000\mu$ you could obtain these inferences "by hand" by multiplying the estimate, standard error, and confidence interval limits by 700000. The `contrast` function from the **trtools** package will also allow you to make inferences concerning $\mu$ and $\tau$ by specifying a transformation function to take the estimates of $\mu_e$, $\mu_t$, and $\mu_h$. This requires a little bit of R programming.
    ```{r}
    f <- function(mu) {
      1/7 * mu[1] + 2/7 * mu[2] + 4/7 * mu[3]    
    }
    trtools::contrast(m, 
      a = list(layer = c("epilimnion","thermocline","hypolimnion")), tf = f)
    ```
    Here the argument `tf` allows me to specify a "transformation function" to make inferences about a function of whatever `contrast` would produce (here the estimated expected number of daphnia for each layer). This is maybe a little easier than using `lincon`, but more work than using `emmeans`. A benefit of using `contrast` like this is that it is fairly powerful since the user can specify other kinds of transformation functions. 
    
0. In the previous problem you estimated the total number of daphnia in the lake. Now consider the problem of estimating the total number of daphnia *in each layer*. The total number of daphnia in the epilimnion layer is $\tau_e$ = 100000$\mu_e$. Similarly, the total number of daphnia in the thermocline and hypolimnion layers are $\tau_t$ = 200000$\mu_t$ and $\tau_h$ = 400000$\mu_h$, respectively. Write $\tau_e$, $\tau_t$, and $\tau_h$ as linear combinations of $\beta_0$, $\beta_1$, and $\beta_2$, and use the `lincon` function produce an estimate, standard error, and confidence interval for each parameter. 

    **Solution**: From above we have that $\tau_e$ = 100000$\beta_0$, $\tau_t$ = 200000$\beta_0$ + 200000$\beta_1$, and $\tau_h$ = 400000$\beta_0$ + 400000$\beta_2$. These can be estimated as follows.
    ```{r}
    lincon(m, a = c(100000, 0, 0))
    lincon(m, a = c(200000, 200000, 0))
    lincon(m, a = c(400000, 0, 400000))
    ```
    This can also be done using `contrast` by programming a transformation function.
    ```{r}
    f <- function(mu) {
      c(100000*mu[1], 200000*mu[2], 400000*mu[3])
    }
    trtools::contrast(m, 
      a = list(layer = c("epilimnion","thermocline","hypolimnion")), 
      tf = f, delta = TRUE)
    ```
    The `delta = TRUE` argument here is required here for a technical reason to overcome a limitation of the `contrast` function to understand certain kinds of transformation functions. 
    

[^stratified]: For students familiar with survey sampling theory, the estimators of $\mu$ and $\tau$ being used here are equivalent to estimators used for stratified random sampling designs. The standard errors, however, are not quite the same. The main reason is that here we are implicitly assuming that the population variances in each layer (i.e., $\sigma^2_e$, $\sigma^2_t$, and $\sigma^2_h$) are equal, which is usually not assumed in stratified random sampling. We will learn how to relax this assumption later. Another issue is that we are not taking into account sampling without replacement from a finite population, but given the large volume of each layer relative to the number of liters sampled any such correction would be negligible. 

[^daphnia]: The original cited source for these data is a textbook on survey sampling. The data may not be real.

## Germination of Orobanche Seeds

Crowder (1978) featured data from an experiment concerning the parasitic plant *Orobanche aegyptiaca* (Egyptian broomrape).[^crowder] Plates of seeds of two genotypes (*O.\ aegyptiaca 73* and *O.\ aegyptiaca 75*) were randomly assigned to be exposed to extract from either bean or cucumber plants (as parasitic plants, the seeds remain dormant until stimulated by the presence of a host plant). The number of germinating seeds out of the number of seeds on the plate was then recorded.[^proportions] The data are in a data frame called `crowder.seeds` in the package **agridat**. 
```{r, fig.height = 4}
library(ggplot2)
library(agridat)

crowder.seeds$y <- crowder.seeds$germ / crowder.seeds$n # creating response variable

p <- ggplot(crowder.seeds, aes(y = y, x = extract)) + theme_minimal() + 
  geom_point(alpha = 0.5) + facet_wrap(~ gen) +
  labs(x = "Extract Type", y = "Proportion Germinated")
plot(p)
```
This is a *randomized block design* where the blocking variable is the genotype and the randomized treatment is extract type. In a classic analysis of variance (ANOVA) of these data, one might investigate the "main effect" of the treatment and perhaps that of the blocking variable, and also the "interaction" between the treatment and blocking variables. Tests of the main effects and interaction are sometimes reported in an ANOVA table like the following.
```{r, echo = FALSE}
m.type3 <- lm(y ~ gen + extract + gen:extract, data = crowder.seeds,
  contrast = list(gen = contr.sum, extract = contr.sum))
library(car)
Anova(m.type3, type = 3)
```
But unfortunately for students (and some researchers) the understanding of what is a "main effect" or "interaction" are not always well understood. They understand the computational details but not actually what they are testing. But what is really meant by a "main effect" or "interaction" can be made more clear by carefully examining the quantities and hypotheses involved. 

[^proportions]: Later this semester we will learn about some alternative approaches to modeling proportions as response variables.

1. Consider the following linear model.
    ```{r}
    m <- lm(y ~ gen + extract + gen:extract, data = crowder.seeds)
    summary(m)$coefficients
    ```
The model has the form 
$$
  E(Y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3},
$$
where $Y_i$ is the proportion of germinating seeds for the $i$-th observation. How are $x_{i1}$, $x_{i2}$, and $x_{i3}$ defined in this model? That is, how would you determine their values for a given observation? Finally, write the model case-wise to show how the expected proportion of germinating seeds can be written as a function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$. Note that there are *four* cases: the O73 genotype exposed to bean extract, the O75 genotype exposed to cucumber extract, the O75 genotype exposed to bean extract, and the O75 genotype exposed to cucumber extract. 

    **Solution**: From the output we can see that $x_{i1}$ is an indicator variable for genotype O75 so that
    $$
    x_{i1} = 
    \begin{cases}
      1, & \text{if the $i$-th observation is for genotype O75}, \\
      0, & \text{otherwise},
    \end{cases}
    $$
    and $x_{i2}$ is an indicator variable for cucumber extract so that 
    $$
    x_{i2} = 
    \begin{cases}
      1, & \text{if the $i$-th observation is for cucumber extract}, \\
      0, & \text{otherwise}.
    \end{cases}
    $$
    Finally $x_{i3} = x_{i1}x_{i2}$ so that
    $$
    x_{i3} = 
    \begin{cases}
      1, & \text{if the $i$-th observation is for genotype O75 and cucumber extract}, \\
      0, & \text{otherwise}.
    \end{cases}
    $$
    We can write the model case-wise as
    $$
    E(Y_i) = 
    \begin{cases}
      \beta_0, & \text{if the $i$-th observation is for genotype O73 and bean extract}, \\
      \beta_0 + \beta_1, & \text{if the $i$-th observation is for genotype O75 and bean extract}, \\
      \beta_0 + \beta_2, & \text{if the $i$-th observation is for genotype O73 and cucumber extract}, \\
      \beta_0 + \beta_1 + \beta_2 + \beta_3, & \text{if the $i$-th observation is for genotype O75 and cucumber extract}.
    \end{cases}
    $$
      
2. Let $\mu_{73,b}$ denote the expected proportion of seeds of the O73 genotype when exposed to the bean extract. Similarly let $\mu_{73,c}$, $\mu_{75,b}$, and $\mu_{75,c}$ denote expected proportion of seeds germinating corresponding to the other three combinations of genotype and extract type. Provide estimates of each of these four expected values with standard errors and confidence intervals using both `lincon` and `contrast`.[^genotype]

    **Solution**: Note that from the previous problem we can see that
    \begin{align*}
      \mu_{73,b} & = \beta_0, \\
      \mu_{75,b} & = \beta_0 + \beta_1, \\
      \mu_{73,c} & = \beta_0 + \beta_2, \\
      \mu_{75,c} & = \beta_0 + \beta_1 + \beta_2 + \beta_3.
    \end{align*}
    We can estimate these quantities using `lincon` as follows.
    ```{r}
    lincon(m, a = c(1,0,0,0)) # O73 & bean
    lincon(m, a = c(1,1,0,0)) # O75 & bean
    lincon(m, a = c(1,0,1,0)) # O73 & cucumber
    lincon(m, a = c(1,1,1,1)) # O75 & cucumber
    ```
    And here is how to do it using `contrast`.
    ```{r}
    trtools::contrast(m,
      a = list(gen = c("O73","O75","O73","O75"), 
        extract = c("bean","bean","cucumber","cucumber")), 
      cnames = c("O73 & bean", "O75 & bean", "O73 & cucumber", "O75 & cucumber"))
    ```
    Here are several ways this can be done with the **emmeans** package.
    ```{r}
    emmeans(m, ~ gen*extract)
    emmeans(m, ~ gen | extract)
    emmeans(m, ~ extract | gen)
    ```
    Note that these all provide the same information, but the latter two can be used when estimating marginal means (as shown below) or pairwise comparisons of the levels of one factor within the levels of a second factor (sometimes called "simple effects"). Here are the estimates simple effects.
    ```{r}
    pairs(emmeans(m, ~ gen | extract), infer = TRUE)
    pairs(emmeans(m, ~ extract | gen), infer = TRUE)
    ```
    Also if you wanted to do pairwise comparisons among all *four* treatment conditions you could do it this way.
    ```{r}
    pairs(emmeans(m, ~ gen*extract), infer = TRUE, adjust = "none")
    ```

3. So-called "main effects" are based on what are sometimes called *marginal means* --- i.e., the mean expected value obtained by averaging over the levels of the other factor(s). The marginal means for the two extract types are
$$
\mu_b = \frac{\mu_{O73,b} + \mu_{O75,b}}{2} \ \ \ \text{and} \ \ \
\mu_c = \frac{\mu_{O73,c} + \mu_{O75,c}}{2},
$$
and the marginal means for the two genotypes are
$$
\mu_{O73} = \frac{\mu_{O73,b} + \mu_{O73,c}}{2} \ \ \ \text{and} \ \ \
\mu_{O75} = \frac{\mu_{O75,b} + \mu_{O75,c}}{2}.
$$
Based on your results from the previous problem, write $\mu_b$, $\mu_c$, $\mu_{O73}$, and $\mu_{O75}$ as linear combinations of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ by replacing each $\mu$ with the corresponding function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$ and simplifying. Use the `lincon` function to estimate the four marginal means.[^marginal] 

    **Solution**: The marginal means for the extract types can be written as
    $$
      \mu_b = \frac{\beta_0 + \beta_0 + \beta_1}{2} = \beta_0 + \tfrac{1}{2}\beta_1,
    $$
    and 
    $$
      \mu_c = \frac{\beta_0 + \beta_2 + \beta_0 + \beta_1 + \beta_2 + \beta_3}{2} = \beta_0 + \tfrac{1}{2}\beta_1 + \beta_2 + \tfrac{1}{2}\beta_3. 
    $$
    These can be estimated using `lincon` as follows.
    ```{r}
    lincon(m, a = c(1, 0.5, 0, 0))
    lincon(m, a = c(1, 0.5, 1, 0.5))
    ```
    Here is how these can be estimated using the **emmeans** package.
    ```{r}
    emmeans(m, ~ extract)
    ```
    The marginal means for genotype can be written as
    $$
      \mu_{\text{O73}} = \frac{\beta_0 + \beta_0 + \beta_2}{2} = \beta_0 + \tfrac{1}{2}\beta_2,
    $$
    and 
    $$
      \mu_{O75} = \frac{\beta_0 + \beta_1 + \beta_0 + \beta_1 + \beta_2 + \beta_3}{2} = \beta_0 + \beta_1 + \tfrac{1}{2}\beta_2 + \tfrac{1}{2}\beta_3.
    $$
    We can estimate these using `lincon` as follows.
    ```{r}
    lincon(m, a = c(1,0,0.5,0))
    lincon(m, a = c(1,1,0.5,0.5))
    ```
    Here is how these can be estimated using the **emmeans** package.
    ```{r}
    emmeans(m, ~ gen)
    ```

4. The main effect of extract type is defined in terms of the marginal means for extract type. It is defined as $\mu_c - \mu_b$ (or $\mu_b - \mu_c)$, and the null hypothesis for a test of the main effect could be stated as $H_0\!: \mu_c - \mu_b = 0$. Similarly the main effect of genotype is defined as $\mu_{O75} - \mu_{O73}$ (or $\mu_{O73} - \mu_{O75}$), and the null hypothesis for a test of the main effect could be stated as $H_0\!: \mu_{O75} - \mu_{O73} = 0$. Using your results from the previous problem, write $\mu_c - \mu_b$ and $\mu_{O75} - \mu_{O73}$ as linear combinations of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$. Also report the result of a test of the null hypothesis for each main effect using the `lincon` function. If you do this correctly the squared *t* test statistics and the p-values reported by `lincon` should match those shown in the ANOVA table above shown in the `gen` and `extract` rows.

    **Solution**: The main effect of extract type can be written as
    $$
      \mu_c - \mu_b = \beta_0 + \tfrac{1}{2}\beta_1 + \beta_2 + \tfrac{1}{2}\beta_3 - \left(\beta_0 + \tfrac{1}{2}\beta_1\right) = 
      \beta_2 + \tfrac{1}{2}\beta_3.
    $$
    We can estimate this with `lincon` as follows.
    ```{r}
    lincon(m, a = c(0,0,1,0.5))
    ```
    The main effect for genotype can be written as
    $$
      \mu_{\text{O75}} - \mu_{\text{O73}} =
      \beta_0 + \beta_1 + \tfrac{1}{2}\beta_2 + \tfrac{1}{2}\beta_3 - \left(\beta_0 + \tfrac{1}{2}\beta_2 \right) = \beta_1 + \tfrac{1}{2}\beta_3.
    $$
    This can be estimated using `lincon` as follows.
    ```{r}
    lincon(m, a = c(0,1,0,0.5))
    ```
    Inferences for these main effects can also be obtained using the **emmeans** package, although the direction of subtraction is not necessarily the same.
    ```{r}
    pairs(emmeans(m, ~ extract), infer = TRUE)
    pairs(emmeans(m, ~ gen), infer = TRUE)
    ```
    Since each main effect involves only a single linear combination, the *t* test statistic can be used. But for the main effect of a factor with more than two levels the null hypothesis involves two or more linear combinations and the *F* test statistic must be used. This can be done using the `test` function from **emmeans** (see the example from lecture with the `ToothGrowth` data). 

5. The definition of an "interaction" in a linear model is that the differences among the expected values over one factor do not depend on the level of the other factor. The null hypothesis for the interaction could be written as
$$
  H_0\!: \mu_{075,c} - \mu_{075,b} = \mu_{073,c} - \mu_{073,b}
$$
or, equivalently,
$$
  H_0\!: \mu_{075,c} - \mu_{075,b} - \mu_{073,c} + \mu_{073,b} = 0.
$$
Write $\mu_{075,c} - \mu_{075,b} - \mu_{073,c} + \mu_{073,b}$ as a linear combination of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ by replacing each $\mu$ or each difference between $\mu$'s with a function of $\beta_0$, $\beta_1$, $\beta_2$, and/or $\beta_3$ you found earlier and simplifying. Finally report the results of a test of this null hypothesis using `lincon`. If you do this correctly the squared *t* test statistic and the p-value reported by `lincon` should match those shown in the ANOVA table above shown in the `gen:extract` row.

    **Solution**: The null hypothesis can be written as $\beta_3$ = 0 after simplifying the linear combination. This can be tested using `lincon` as follows.
    ```{r}
    lincon(m, a = c(0,0,0,1))
    ```
    But note that this test is also given in the output from `summary`.
    ```{r}
    summary(m)$coefficients
    ```

6. Suppose the model was specified *without* an interaction as follows.
    ```{r}
    m <- lm(y ~ gen + extract, data = crowder.seeds)
    summary(m)$coefficients
    ```
The model is now
$$
  E(Y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}.
$$
Repeat problems 2, 3, and 4 with this model, but noting in each case that the parameters are now just $\beta_0$, $\beta_1$, and $\beta_2$ (i.e., there is no $\beta_3$ parameter for this model). **Note**: This problem is *extra credit* for students enrolled in Stat 436, but is *required* for students in Stat 516.

    **Solution**: Because of the way the model is parameterized, removing the interaction from the model is effectively equivalent to setting $\beta_3$ = 0. So we can write
    \begin{align*}
      \mu_{73,b} & = \beta_0, \\
      \mu_{75,b} & = \beta_0 + \beta_1, \\
      \mu_{73,c} & = \beta_0 + \beta_2, \\
      \mu_{75,c} & = \beta_0 + \beta_1 + \beta_2.
    \end{align*}
    These can be estimated using `lincon` as follows.
    ```{r}
    lincon(m, a = c(1,0,0)) # O73 & bean
    lincon(m, a = c(1,1,0)) # O75 & bean
    lincon(m, a = c(1,0,1)) # O73 & cucumber
    lincon(m, a = c(1,1,1)) # O75 & cucumber
    trtools::contrast(m,
      a = list(gen = c("O73","O75","O73","O75"), 
        extract = c("bean","bean","cucumber","cucumber")), 
      cnames = c("O73 & bean", "O75 & bean", "O73 & cucumber", "O75 & cucumber"))
    ```
    The marginal means for extract type can be estimated as follows.
    ```{r}
    lincon(m, a = c(1,0.5,0)) # O73
    lincon(m, a = c(1,0.5,1)) # O75
    emmeans(m, ~ extract)
    ```
    And the marginal means for genotype can be estimated as follows.
    ```{r}
    lincon(m, a = c(1,0,0.5)) # bean
    lincon(m, a = c(1,1,0.5)) # cucumber
    emmeans(m, ~ gen)
    ```
    The main effects for extract type and genotype reduce to $\beta_2$ and $\beta_1$, respectively. Inferences for these parameters are given by `summary`, but we can also get them from `lincon` as follows.
    ```{r}
    lincon(m, a = c(0,0,1)) # extract
    lincon(m, a = c(0,1,0)) # genotype
    ```
    Of course, the **emmeans** package can be used to make inferences about marginal means and main effects using the same syntax as for the model with the interaction. One thing that is worth noting is that in a model like this without an interaction, the simple effects (i.e., the pairwise differences for the levels of one factor within the levels of the other) *equal* the main effects. Consider, for example, the simple and main effects for extract type.
    ```{r}
    pairs(emmeans(m, ~ extract | gen), infer = TRUE)
    pairs(emmeans(m, ~ extract), infer = TRUE)
    ```

[^crowder]: Crowder, M. J. (1978). Beta-binomial ANOVA for proportions.\ *Applied Statistics*, *27*, 34-37. 

[^genotype]: Note that in the level names `O75` and `O73` of the `gen` factor the `O` is a capital letter "O" and not a zero. 
 
[^marginal]: People sometimes confuse these estimates with the means that would be obtained by simply averaging the observations within each level of each each factor. These are the same only if the number of observations in each combination of levels that are averaged are *equal*. Otherwise they will depend on the sample sizes, which is usually undesirable. However in some cases people will estimate marginal means as weighted averages in observational studies to reflect the relative number of units in each combination of levels within a population.


 

