---
output:
  html_document: 
    theme: readable
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "02-14-2024"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment = "", echo = TRUE, message = FALSE, out.width = "100%", fig.align = "center", cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

```{r packages, echo = FALSE}
library(tidyverse)
suppressWarnings(library(kableExtra))
```

```{r utilities, echo = FALSE}
source("../../utilities.R")
```

```{r options, echo = FALSE}
options(digits = 4)
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`



## Assumption 3: Independence of Errors/Observations

In the regression model
$$
	Y_i = f(x_{i1}, x_{i2}, \dots, x_{ik}) + \epsilon_i,
$$
it is assumed that $\epsilon_1, \epsilon_2, \dots, \epsilon_n$ are *mutually independent*. Equivalently, it is assumed that $Y_1, Y_2, \dots, Y_n$ are *conditionally independent* given $x_{i1}, x_{i2}, \dots, x_{ik}$. That is, the distribution of any one $\epsilon_i$ (or $Y_i$) should not depend on the values of the other $\epsilon_{i'}$ (or $Y_{i'}$) ($i' \neq i$). 

A weaker condition that $\text{Cov}(\epsilon_i, \epsilon_{i'}) = 0$ for all $i \neq i'$ is sufficient. The *covariance* between $\epsilon_i$ and $\epsilon_{i'}$ is defined as
$$
  \text{Cov}(\epsilon_i, \epsilon_{i'}) = \text{Corr}(\epsilon_i, \epsilon_{i'})\sqrt{\text{Var}(\epsilon_i)\text{Var}(\epsilon_{i'})}.,
$$
Note that this implies the same condition for $Y_i$ and $Y_{i'}$.

**Consequences**: Severe violations of independence can result in two problems.

1. Biased standard errors, incorrect p-values, and incorrect confidence/prediction intervals. 

2. Inefficient estimation of model parameters (and functions thereof).

**Detection**: A common reason for a lack of independence is that multiple observations are influenced by one or more common random but unobserved effects, or in some cases one observation influences another observation directly. Typically observations that are "closer" in some sense --- time, space, or observational units --- are not independent. Plotting with this in mind can sometimes reveal a lack of independence.

**Example**: Serial dependence --- observations close in time. 
```{r, echo = FALSE}
set.seed(101)
x <- 1:100
s <- outer(x, x, function(a, b) 0.9^(abs(a-b)))
y <- MASS::mvrnorm(1, rep(0, length(x)), s)
tmp <- data.frame(x = x, y = y)
p <- ggplot(tmp, aes(x = x, y = y))
p <- p + geom_point() + geom_segment(aes(x = x, xend = x, y = y, yend = 0))
p <- p + xlab("Time or Serial Order") + ylab("Residual") + theme_minimal()
plot(p)
```

**Example**: Nested observations --- observations within a common experimental unit.

```{r}
source("http://webpages.uidaho.edu/~trjohns/grooming.txt")
head(grooming)
tail(grooming)
p <- ggplot(grooming, aes(x = day, y = frequency, color = id)) +
  geom_point() + facet_grid(~ collar, scales = "free_x") +
  theme_minimal() + 
  labs(x = "Relative Day", y = "Grooming Frequency", 
    color = "Rabbit")
plot(p)
```
```{r, fig.height = 6}
m <- lm(log(frequency) ~ day + collar + day:collar, data = grooming)
grooming$yhat <- predict(m)
grooming$rest <- rstudent(m)
p <- ggplot(grooming, aes(x = yhat, y = rest)) + 
  geom_point() + 
  geom_segment(aes(x = yhat, xend = yhat, y = rest, yend = 0)) +
  facet_wrap(~ id) + theme_minimal() + 
  labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```

**Solutions**: There are several possible solutions.

1. Generalized least squares.

2. Robust standard error estimators. 

3. Models that allow for dependence of observations.

We will discuss each of these in later lectures.

## Assumption 4: Normality

**Description**: It is assumed that the distribution of each $\epsilon_i$ is normal, which implies that the distribution of each $Y_i$ is also normal (conditional on $x_{i1}, x_{i2}, \dots, x_{ik}$). 

**Consequences**: Confidence/prediction intervals and p-values may not be accurate if the distribution of $\epsilon_i$ (or $Y_i$) is very non-normal, but only if $n$ is very small relative to the number of parameters ($p$). This is because under fairly general circumstances as $n \rightarrow \infty$ the *sampling distribution* of the model parameters (and functions thereof) "approach" a normal distribution (i.e., "asymptotically normal") *regardless* of the distribution of the error/response. With the exception of prediction intervals, we only require that *sampling distributions* are (approximately) normal.

Consider the model $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ where the distribution of each $\epsilon_i$ is
```{r, echo = FALSE}
e <- seq(0, 1, length = 1000)
y <- dbeta(e, 0.2, 0.2)
if (knitr::is_html_output()) {
    xlab <- expression(epsilon[i])
} else {
    xlab <- "$\\epsilon_i$"
}
p <- ggplot(data.frame(e = (e - 0.5)*2, y = y), aes(x = e, y = y)) + theme_classic()
p <- p + geom_line() + labs(x = xlab, y = "Probability Density")
plot(p)
```
and $x_i$ is 1, 2, or 3 with $n/3$ observations at each distinct value of $x_i$. What about the *sampling distributions* of $\hat\beta_0$ and $\hat\beta_1$? 

Sampling distributions when $n = 3$:
```{r, echo = FALSE, warning = FALSE}
set.seed(123)
n <- 3
x <- rep(1:3, each = n/3)
r <- 10000
b0 <- rep(NA, r)
b1 <- rep(NA, r)
for (i in 1:r) {
    y <- 10 + x + (rbeta(length(x), 0.2, 0.2) - 0.5)*2
    m <- lm(y ~ x)
    b0[i] <- coef(m)[1]
    b1[i] <- coef(m)[2]
}

d <- data.frame(b0 = b0, b1 = b1)

if (knitr::is_html_output()) {
    b0 <- expression(hat(beta)[0])
    b1 <- expression(hat(beta)[1])
} else {
    b0 <- "$\\beta_0$"
    b1 <- "$\\beta_1$"
}

p0 <- ggplot(d, aes(x = b0, y = ..density..)) + theme_classic()
p0 <- p0 + geom_histogram(fill = "white", color = "black")
p0 <- p0 + labs(x = b0, y = "Estimated Probability Density")

p1 <- ggplot(d, aes(x = b1, y = ..density..)) + theme_classic()
p1 <- p1 + geom_histogram(fill = "white", color = "black")
p1 <- p1 + labs(x = b1, y = "Estimated Probability Density")

cowplot::plot_grid(p0, p1, ncol = 2)
```

Sampling distributions when $n = 6$:
```{r, echo = FALSE}
n <- 6
x <- rep(1:3, each = n/3)
r <- 10000
b0 <- rep(NA, r)
b1 <- rep(NA, r)
for (i in 1:r) {
    y <- 10 + x + (rbeta(length(x), 0.2, 0.2) - 0.5)*2
    m <- lm(y ~ x)
    b0[i] <- coef(m)[1]
    b1[i] <- coef(m)[2]
}

d <- data.frame(b0 = b0, b1 = b1)

if (knitr::is_html_output()) {
    b0 <- expression(hat(beta)[0])
    b1 <- expression(hat(beta)[1])
} else {
    b0 <- "$\\beta_0$"
    b1 <- "$\\beta_1$"
}
p0 <- ggplot(d, aes(x = b0, y = ..density..)) + theme_classic()
p0 <- p0 + geom_histogram(fill = "white", color = "black")
p0 <- p0 + labs(x = b0, y = "Estimated Probability Density")

p1 <- ggplot(d, aes(x = b1, y = ..density..)) + theme_classic()
p1 <- p1 + geom_histogram(fill = "white", color = "black")
p1 <- p1 + labs(x = b1, y = "Estimated Probability Density")

cowplot::plot_grid(p0, p1, ncol = 2)
```

Sampling distributions when $n = 12$:
```{r, echo = FALSE}
n <- 12
x <- rep(1:3, each = n/3)
r <- 10000
b0 <- rep(NA, r)
b1 <- rep(NA, r)
for (i in 1:r) {
    y <- 10 + x + (rbeta(length(x), 0.2, 0.2) - 0.5)*2
    m <- lm(y ~ x)
    b0[i] <- coef(m)[1]
    b1[i] <- coef(m)[2]
}

d <- data.frame(b0 = b0, b1 = b1)

if (knitr::is_html_output()) {
    b0 <- expression(hat(beta)[0])
    b1 <- expression(hat(beta)[1])
} else {
    b0 <- "$\\beta_0$"
    b1 <- "$\\beta_1$"
}

p0 <- ggplot(d, aes(x = b0, y = ..density..)) + theme_classic()
p0 <- p0 + geom_histogram(fill = "white", color = "black")
p0 <- p0 + labs(x = b0, y = "Estimated Probability Density")

p1 <- ggplot(d, aes(x = b1, y = ..density..)) + theme_classic()
p1 <- p1 + geom_histogram(fill = "white", color = "black")
p1 <- p1 + labs(x = b1, y = "Estimated Probability Density")

cowplot::plot_grid(p0, p1, ncol = 2)
```

**Detection**: Plots of standardized or studentized residuals can reveal severe non-normality. Histograms can be used, but quantile-quantile (qq) plots are preferred. A qq-plot is a plot of the sorted residuals against their expected "order statistics" under the assumption that they are normally-distributed.

```{r, fig.height = 5, echo = c(1,3,4)}
m <- lm(Gas ~ Insul + Temp + Insul:Temp, data = MASS::whiteside)
par(mfcol = c(1,2))
hist(rstandard(m))
qqnorm(rstandard(m))
```
Note that it *not* usually appropriate to use plots of the response variable since they are not *identically* distributed. 

**Solutions**: For severe non-normality there are generally three solutions.

1. Increase $n$ (unless $n$ is already "sufficiently large" and then non-normality is a non-issue). 

2. Transform the response variable.

3. Use a model that assumes the response variable has some other distribution.

## Tests of Assumptions

A variety of statistical tests have been proposed for various assumptions, where the null hypothesis is that the assumption is true. In my opinion these are rarely very useful for several reasons.

1. Such tests have their own assumptions, which may not be well met.

0. Such tests do not clearly reveal the *severity* of the violation. While it is true that tests tend to have more power when the violation is severe, they can also have high power when the violation is negligible (as may also be the consequences) if $n$ is large.

0. Perhaps the question to ask is not if the assumption is wrong, but rather if it is "reasonable" --- i.e., is it "close enough" or are inferences likely to be very adversely affected?

## Effects of Explanatory Variable Distributions

Regression models do not make any assumptions about the distribution of the explanatory variables. Their distribution, however, does affect inferences just as sample size does. 

**Example**: Consider a linear model 
$$
   E(Y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2},
$$
where the explanatory variables are distributed as shown below.
```{r, echo = FALSE}
n <- scan(text = "
40	40	40	40	40
40	40	40	40	40
40	40	40	40	40
40	40	40	40	40
40	40	40	40	40")
d <- expand.grid(x1 = -2:2 + 2, x2 = -2:2 + 2) %>% mutate(n = n) %>%
   uncount(weights = n) %>% mutate(y = x1 + x2 + rnorm(n(), 0, 5))

p <- ggplot(d, aes(x = x1, y = x2)) + theme_minimal()
p <- p + geom_count() + coord_equal()
p <- p + labs(size = "Observations")
p <- p + labs(x = tex("$x_1$"), y = tex("$x_2$"))
p <- p + scale_size_continuous(breaks = unique(n))
plot(p)
```
Note the standard errors for $\hat\beta_1$ and $\hat\beta_2$ shown below.
```{r, echo = FALSE}
m <- lm(y ~ x1 + x2, data = d)
cbind(summary(m)$coefficients, confint(m))
```
Now suppose we reduce the variance of the first explanatory variable.
```{r, echo = FALSE}
n <- scan(text = "
10	40	100	40	10
10	40	100	40	10
10	40	100	40	10
10	40	100	40	10
10	40	100	40	10
")
d <- expand.grid(x1 = -2:2 + 2, x2 = -2:2 + 2) %>% mutate(n = n) %>%
   uncount(weights = n) %>% mutate(y = x1 + x2 + rnorm(n(), 0, 5))

p <- ggplot(d, aes(x = x1, y = x2)) + theme_minimal()
p <- p + geom_count() + coord_equal()
p <- p + labs(size = "Observations")
p <- p + labs(x = tex("$x_1$"), y = tex("$x_2$"))
p <- p + scale_size_continuous(breaks = unique(n))
plot(p)
```
Note the standard errors for $\hat\beta_1$ and $\hat\beta_2$ shown below.
```{r, echo = FALSE}
m <- lm(y ~ x1 + x2, data = d)
cbind(summary(m)$coefficients, confint(m))
```
Now suppose that the two explanatory variables have the same variance as in the first example, but have a much higher *covariance*.
```{r, echo = FALSE}
n <- scan(text = "
1	2	5	10	182
2	5	10	173	10
5	10	170	10	5
10	173	10	5	2
182	10	5	2	1

")
d <- expand.grid(x1 = 2:-2 + 2, x2 = -2:2 + 2) %>% mutate(n = n) %>%
   uncount(weights = n) %>% mutate(y = x1 + x2 + rnorm(n(), 0, 5))

p <- ggplot(d, aes(x = x1, y = x2)) + theme_minimal()
p <- p + geom_count() + coord_equal()
p <- p + labs(size = "Observations")
p <- p + labs(x = tex("$x_1$"), y = tex("$x_2$"))
p <- p + scale_size_continuous(breaks = unique(n))
plot(p)
```
Note the standard errors for $\hat\beta_1$ and $\hat\beta_2$ shown below.
```{r, echo = FALSE}
m <- lm(y ~ x1 + x2, data = d)
cbind(summary(m)$coefficients, confint(m))
```
This last example illustrates what is called (multi)collinearity where there exist $\lambda_0, \lambda_1, \lambda_2, \dots, \lambda_k$ (other than that all $\lambda_0, \lambda_1, \lambda_2, \dots, \lambda_k$ equal zero) such that
$$
  \lambda_0 + \lambda_1x_{i1} + \lambda_2x_{i2} + \cdots + \lambda_kx_{ik} \approx 0 
$$
for all observations. Another way to say this is that we could almost determine the value of one explanatory variable from the others using a linear model where that explanatory variable is the response variable. When there are only two explanatory variables involved this will be evident from a high correlation (in absolute value) between those two explanatory variables. 

