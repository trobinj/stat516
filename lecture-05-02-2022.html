<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Monday, May 2</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics 436/516</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="lectures.html">Lectures</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Monday, May 2</h1>

</div>


<p>You can also download a <a href="lecture-05-02-2022.pdf">PDF</a> copy
of this lecture.</p>
<p>Assume that there is a <em>true</em> model <span
class="math display">\[
    Y = f(x) + \epsilon
\]</span> where here <span class="math inline">\(x\)</span> represents
one or more explanatory variables, and <span
class="math inline">\(E(\epsilon) = 0\)</span> so that <span
class="math inline">\(E(Y) = f(x)\)</span>. But the model we use/assume
is <span class="math display">\[
    Y = g(x) + \tilde\epsilon.
\]</span> We use the data to produce <span
class="math inline">\(\hat{g}(x)\)</span>, which can be viewed as an
estimate of <span class="math inline">\(f(x)\)</span>. Ideally <span
class="math inline">\(\hat{g}(x)\)</span> would tend to be close to
<span class="math inline">\(f(x)\)</span>. We can define the expected
squared distance between <span class="math inline">\(\hat{g}(x)\)</span>
and <span class="math inline">\(f(x)\)</span> as the <em>mean squared
error</em> <span class="math display">\[
    E(\hat{g}(x) - f(x))^2,
\]</span> which can be decomposed into two terms: <span
class="math display">\[
    E(\hat{g}(x) - f(x))^2 = \underbrace{[E(\hat{g}(x)) -
f(x)]^2}_{\text{bias}} + \text{Var}[\hat{g}(x)].
\]</span> A variety of factors will affect the bias and variance.</p>
<ol style="list-style-type: decimal">
<li><p>Different models may be able to produce a <span
class="math inline">\(\hat{g}\)</span> that has higher/lower
variance.</p></li>
<li><p>As a model becomes more <em>complex</em> the bias tends to
decrease, but the variance will tend to increase.</p></li>
<li><p>As the sample size increases, the variance will decrease but bias
will tend to stay the same, so with large sample sizes we can have more
complex models with the same mean squared error.</p></li>
<li><p>The design will affect the mean squared error. For example, the
“distribution” of the values of the explanatory variable(s) will affect
the mean squared error.</p></li>
</ol>
<p>For a given design, we would like to select a model such that will
<em>minimize</em> <span class="math inline">\(E(\hat{g}(x) -
f(x))^2\)</span> (a bias-variance trade-off). The model should be
complex enough to capture the statistical relationship between the
response variable and the explanatory variables, but not so complex that
it results in “over-fitting” the data.</p>
<div id="prediction-error" class="section level2">
<h2>Prediction Error</h2>
<p>The expected prediction error is <span class="math inline">\(E(Y -
\hat{g}(x))^2\)</span> where <span class="math inline">\(Y\)</span> is a
<em>new</em> observation (i.e., one that is not used to obtain <span
class="math inline">\(\hat{g}(x)\)</span>). It can be shown that <span
class="math display">\[
    E(Y - \hat{g}(x))^2 = E(\hat{g}(x) - f(x))^2 + \sigma^2,
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the
variance of <span class="math inline">\(\epsilon\)</span>. The choice of
model will affect <span class="math inline">\(E(\hat{g}(x) -
f(x))^2\)</span> but not <span class="math inline">\(\sigma^2\)</span>.
We can generalize this to multiple values of the explanatory variables
and use the expected average prediction error and the expected average
mean squared error: <span class="math display">\[
    E\left(\frac{1}{n}\sum_{i=1}^n (Y_i - \hat{g}(x_i))^2\right) =
    E\left(\frac{1}{n}\sum_{i=1}^n (\hat{g}(x_i) - f(x_i))^2\right) +
\sigma^2.
\]</span> Note that minimizing the expected prediction error will
therefore minimize the model’s mean squared error. The expected
prediction error can be estimated using cross-validation.</p>
<p><strong>Note</strong>: Some researchers will use a statistic like the
<em>coefficient of determination</em> (i.e., the squared correlation
between the predicted and actual values of <span
class="math inline">\(y_i\)</span>, sometimes written <span
class="math inline">\(R^2\)</span>) or something similar to estimate
(the lack of) prediction error, and to evaluate models. This is
<strong>not</strong> recommended because such estimates can be (very)
biased in that they can (severely) underestimate prediction error.</p>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross-Validation</h2>
<p>An estimate of the expected prediction error is to use <span
class="math display">\[
    \frac{1}{n}\sum_{i=1}^n(y_i - \hat{g}_i(x_i))^2,
\]</span> where <span class="math inline">\(\hat{g}_i\)</span> is the
estimate of <span class="math inline">\(g\)</span> obtained after
<em>omitting</em> the <span class="math inline">\(i\)</span>-th
observation. This is sometimes called <em>leave-one-out
cross-validation</em>. Another approach is to use <span
class="math inline">\(K\)</span>-fold cross-validation.</p>
<ol style="list-style-type: decimal">
<li><p>Divide the observations randomly into <em>K</em> (nearly) equal
sub-samples. Denote these as <span class="math inline">\(\mathcal{S}_1,
\mathcal{S}_2, \dots, \mathcal{S}_K\)</span>.</p></li>
<li><p>For <span class="math inline">\(k = 1, 2, \dots, K\)</span>,
estimate the model <em>without</em> the <span
class="math inline">\(k\)</span>-th sub-sample of observations to obtain
<span class="math inline">\(\hat{g}_k\)</span>.</p></li>
<li><p>Compute an estimate of the expected prediction error as <span
class="math display">\[
\frac{1}{n}\sum_{k=1}^K\sum_{i \in \mathcal{S}_k} (y_i -
\hat{g}_k(x_i))^2.
\]</span></p></li>
</ol>
<p>Comments about <em>K</em>-fold cross-validation:</p>
<ol style="list-style-type: decimal">
<li><p>Leave-one-out cross-validation is a special case where <span
class="math inline">\(K = n\)</span>.</p></li>
<li><p>The random assignment of observations to sub-samples may need to
be constrained somewhat depending on the design (e.g., to avoid empty
factor levels).</p></li>
<li><p>The process can (and should) be repeated multiple times with
different random assignments of observations to sub-samples, and then
the estimates are averaged over replications. This reduces the
variability of the estimator of the expected prediction error.</p></li>
<li><p>The value of <em>K</em> needs to be specified. Values of <span
class="math inline">\(K\)</span> = 5 or <span
class="math inline">\(K\)</span> = 10 are common in practice. Higher
values of <em>K</em> result in less bias in the estimation of the
expected prediction error, but more variance, and lower values of
<em>K</em> result in lower variance but higher bias!</p></li>
</ol>
<p><strong>Example</strong>: Consider the following models for the
<code>CrabShip</code> data.</p>
<pre class="r"><code>library(Stat2Data)
data(CrabShip)
library(dplyr)

m1 &lt;- nls(Oxygen ~ case_when(
  Noise == &quot;ambient&quot; ~ beta_a * Mass^gamma,
  Noise == &quot;ship&quot;    ~ beta_s * Mass^gamma
), start = list(beta_a = 2.9, beta_s = 4.5, gamma = 0.5), 
data = CrabShip)
summary(m1)$coefficients</code></pre>
<pre><code>       Estimate Std. Error t value  Pr(&gt;|t|)
beta_a  17.8658     7.5707   2.360 2.476e-02
beta_s  26.3527    10.9220   2.413 2.193e-02
gamma    0.5607     0.1036   5.411 6.615e-06</code></pre>
<pre class="r"><code>m2 &lt;- lm(Oxygen ~ -1 + Noise:sqrt(Mass), data = CrabShip)
summary(m2)$coefficients</code></pre>
<pre><code>                        Estimate Std. Error t value  Pr(&gt;|t|)
Noiseambient:sqrt(Mass)    22.87      1.087   21.04 2.620e-20
Noiseship:sqrt(Mass)       33.54      1.126   29.79 6.602e-25</code></pre>
<pre class="r"><code>m3 &lt;- lm(Oxygen ~ -1 + Noise:Mass, data = CrabShip)
summary(m3)$coefficients</code></pre>
<pre><code>                  Estimate Std. Error t value  Pr(&gt;|t|)
Noiseambient:Mass    2.914     0.1739   16.76 2.093e-17
Noiseship:Mass       4.507     0.1875   24.04 4.746e-22</code></pre>
<pre class="r"><code>m4 &lt;- lm(Oxygen ~ Noise*Mass, data = CrabShip)
summary(m4)$coefficients</code></pre>
<pre><code>               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)     103.270    29.3894  3.5139 0.001423
Noiseship       -34.390    43.0782 -0.7983 0.430957
Mass              1.187     0.5121  2.3178 0.027462
Noiseship:Mass    2.071     0.7826  2.6456 0.012857</code></pre>
<p>The <strong>cvTools</strong> package facilitates the
cross-validation.</p>
<pre class="r"><code>library(cvTools)
set.seed(123)
cv1 &lt;- cvFit(m1, data = CrabShip, y = CrabShip$Oxygen, K = 5, R = 25, cost = mspe)
cv2 &lt;- cvFit(m2, data = CrabShip, y = CrabShip$Oxygen, K = 5, R = 25, cost = mspe)
cv3 &lt;- cvFit(m3, data = CrabShip, y = CrabShip$Oxygen, K = 5, R = 25, cost = mspe)
cv4 &lt;- cvFit(m4, data = CrabShip, y = CrabShip$Oxygen, K = 5, R = 25, cost = mspe)</code></pre>
<p>Here <code>K</code> is the number of folds and <code>R</code> is the
number of repetitions.</p>
<pre class="r"><code>cv1</code></pre>
<pre><code>5-fold CV results:
  CV 
1257 </code></pre>
<pre class="r"><code>summary(cv1)</code></pre>
<pre><code>5-fold CV results:
          CV
Min.    1147
1st Qu. 1174
Median  1226
Mean    1257
3rd Qu. 1335
Max.    1482</code></pre>
<pre class="r"><code>cvSelect(&quot;m1&quot; = cv1, &quot;m2&quot; = cv2, &quot;m3&quot; = cv3, &quot;m4&quot; = cv4)</code></pre>
<pre><code>
5-fold CV results:
  Fit   CV
1  m1 1257
2  m2 1171
3  m3 1841
4  m4 1360

Best model:
  CV 
&quot;m2&quot; </code></pre>
</div>
<div id="akaikes-information-criterion-aic" class="section level2">
<h2>Akaike’s Information Criterion (AIC)</h2>
<p>If maximum likelihood is used to estimate the parameters of a model,
we could use the log-likelihood in cross-validation. Imagine we did the
following:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the model parameters with one sample of observations.
This produces a log-likelihood function <span class="math inline">\(\log
L\)</span>. This is a function of the observations and the estimated
parameters.</p></li>
<li><p>Compute the value of the log-likelihood function when applying to
to a second sample of observations — i.e., compute <span
class="math inline">\(\log L\)</span> with different observations but
the same parameter estimates.</p></li>
</ol>
<p>A model with a larger expected value of this log-likelihood has a
smaller expected “distance” to the true model (this distance is known as
the Kullback-Leibler distance). An estimate of this expected
log-likelihood is <span class="math display">\[
    \log L - p,
\]</span> where <span class="math inline">\(\log L\)</span> is the value
of the maximized log-likelihood function from our data and model, and
<span class="math inline">\(p\)</span> is the number of estimated
parameters in the model. So we would like to have large values of <span
class="math inline">\(\log L - p\)</span>, but for historical reasons we
multiply this by <span class="math inline">\(-2\)</span> to obtain <span
class="math display">\[
    \text{AIC} = -2\log L + 2p,
\]</span> which we want to be small. If <span
class="math inline">\(n\)</span> is small relative to <span
class="math inline">\(p\)</span> then a better estimator is <span
class="math display">\[
    \text{AIC}_c = \text{AIC} + 2pn/(n-p-1).
\]</span> <strong>Example</strong>: Consider again the
<code>CrabShip</code> data. We cannot use a mixture of model classes so
we will fit the linear models with <code>nls</code>.</p>
<pre class="r"><code>m2 &lt;- nls(Oxygen ~ case_when(
  Noise == &quot;ambient&quot; ~ beta_a * sqrt(Mass),
  Noise == &quot;ship&quot;    ~ beta_s * sqrt(Mass)
), start = list(beta_a = 0, beta_s = 0), 
data = CrabShip)

m3 &lt;- nls(Oxygen ~ case_when(
  Noise == &quot;ambient&quot; ~ beta_a * Mass,
  Noise == &quot;ship&quot;    ~ beta_s * Mass
), start = list(beta_a = 0, beta_s = 0), 
data = CrabShip)

m4 &lt;- nls(Oxygen ~ case_when(
  Noise == &quot;ambient&quot; ~ alpha_a + beta_a * Mass,
  Noise == &quot;ship&quot;    ~ alpha_s + beta_s * Mass
), start = list(beta_a = 0, beta_s = 0, alpha_a = 0, alpha_s = 0), 
data = CrabShip)

library(AICcmodavg)
mynames = c(&quot;nonlinear&quot;, &quot;-1 + Noise:sqrt(Mass)&quot;, &quot;-1 + Noise:Mass&quot;, &quot;Noise*Mass&quot;)
aictab(list(m1, m2, m3, m4), modnames = mynames)</code></pre>
<pre><code>
Model selection based on AICc:

                      K  AICc Delta_AICc AICcWt Cum.Wt     LL
-1 + Noise:sqrt(Mass) 3 339.5       0.00   0.69   0.69 -166.4
nonlinear             4 341.7       2.17   0.23   0.92 -166.2
Noise*Mass            5 344.0       4.42   0.08   1.00 -165.9
-1 + Noise:Mass       3 354.0      14.44   0.00   1.00 -173.6</code></pre>
<p><strong>Example</strong>: Consider again our models for the
<code>bliss</code> data.</p>
<pre class="r"><code>m1 &lt;- glm(cbind(dead, exposed - dead) ~ concentration, family = binomial, data = bliss)
m2 &lt;- glm(cbind(dead, exposed - dead) ~ poly(concentration, 2), family = binomial, data = bliss)
m3 &lt;- glm(cbind(dead, exposed - dead) ~ poly(concentration, 3), family = binomial, data = bliss)
m4 &lt;- glm(cbind(dead, exposed - dead) ~ poly(concentration, 4), family = binomial, data = bliss)</code></pre>
<p>AIC can be obtained from <code>summary</code> or using the
<code>AIC</code> function.</p>
<pre class="r"><code>summary(m1)</code></pre>
<pre><code>
Call:
glm(formula = cbind(dead, exposed - dead) ~ concentration, family = binomial, 
    data = bliss)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.129  -0.416   0.276   0.927   1.535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -14.8084     1.2898   -11.5   &lt;2e-16 ***
concentration   0.2492     0.0214    11.6   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 289.141  on 15  degrees of freedom
Residual deviance:  12.505  on 14  degrees of freedom
AIC: 58.47

Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>AIC(m1)</code></pre>
<pre><code>[1] 58.47</code></pre>
<p>Utility functions for working with AIC are available in the
<strong>AICcmodavg</strong> package.</p>
<pre class="r"><code>library(AICcmodavg)
mynames = c(&quot;degree = 1&quot;, &quot;degree = 2&quot;, &quot;degree = 3&quot;, &quot;degree = 4&quot;)
aictab(list(m1, m2, m3, m4), modnames = mynames)</code></pre>
<pre><code>
Model selection based on AICc:

           K  AICc Delta_AICc AICcWt Cum.Wt     LL
degree = 2 3 57.90       0.00   0.60   0.60 -24.95
degree = 1 2 59.40       1.50   0.28   0.89 -27.24
degree = 3 4 61.49       3.59   0.10   0.99 -24.93
degree = 4 5 65.85       7.95   0.01   1.00 -24.92</code></pre>
<p>AIC (or <span class="math inline">\(\text{AIC}_c\)</span>) is often
expressed relative to a candidate model with the lowest AIC by computing
the AIC difference defined for the <span
class="math inline">\(k\)</span>-th model as <span
class="math display">\[
    \Delta_k = \text{AIC}_k -
\min(\text{AIC}_1,\text{AIC}_2,\dots,\text{AIC}_K) \ge 0.
\]</span> These differences are then sometimes normalized into “weights”
defined as <span class="math display">\[
    w_k = \frac{\exp(-\Delta_k/2)}{\exp(-\Delta_1/2) + \exp(-\Delta_2/2)
+ \cdots + \exp(-\Delta_K/2)}.
\]</span> The weights then have the property that <span
class="math inline">\(0 &lt; w_k &lt; 1\)</span> and <span
class="math inline">\(\sum_{k=1}^K w_k = 1\)</span>.</p>
<p><strong>Example</strong>: Consider an accelerated failure time model
for how long it takes for insulating fluids to break down under
different constant voltages. What distribution(s) might we specify?</p>
<pre class="r"><code>library(Sleuth3)
library(flexsurv)
head(case0802)</code></pre>
<pre><code>     Time Voltage  Group
1    5.79      26 Group1
2 1579.52      26 Group1
3 2323.70      26 Group1
4   68.85      28 Group2
5  108.29      28 Group2
6  110.29      28 Group2</code></pre>
<pre class="r"><code>m1 &lt;- flexsurvreg(Surv(Time) ~ Voltage, dist = &quot;gengamma&quot;, data = case0802)
m2 &lt;- flexsurvreg(Surv(Time) ~ Voltage, dist = &quot;gamma&quot;, data = case0802)
m3 &lt;- flexsurvreg(Surv(Time) ~ Voltage, dist = &quot;weibull&quot;, data = case0802)
m4 &lt;- flexsurvreg(Surv(Time) ~ Voltage, dist = &quot;exponential&quot;, data = case0802)</code></pre>
<p>Most functions in the <strong>AICcmodavg</strong> have not been
extended to deal with <code>flexsurvreg</code> objects. But we can
compute most quantities of interest “manually” easily enough.</p>
<pre class="r"><code>aicc1 &lt;- AIC(m1) + 2*4*76/(76 - 4 - 1) # generalized gamma (p=4)
aicc2 &lt;- AIC(m2) + 2*3*76/(76 - 3 - 1) # gamma (p=3)
aicc3 &lt;- AIC(m3) + 2*3*76/(76 - 3 - 1) # weibull (p=3)
aicc4 &lt;- AIC(m4) + 2*2*76/(76 - 2 - 1) # exponential (p=2)
delta &lt;- c(aicc1,aicc2,aicc3,aicc4) - min(aicc1,aicc2,aicc3,aicc4)
wghts &lt;- exp(-delta/2)/sum(exp(-delta/2))
data.frame(model = c(&quot;gengamma&quot;,&quot;gamma&quot;,&quot;weibull&quot;,&quot;exponential&quot;),
  aicc = c(aicc1,aicc2,aicc3,aicc4), delta = delta, weight = wghts) </code></pre>
<pre><code>        model  aicc delta  weight
1    gengamma 617.2 3.796 0.08341
2       gamma 614.6 1.220 0.30250
3     weibull 613.4 0.000 0.55663
4 exponential 617.9 4.542 0.05746</code></pre>
</div>
<div id="issues-to-consider-when-using-prediction-error-or-aic"
class="section level2">
<h2>Issues to Consider When Using Prediction Error or AIC</h2>
<ol style="list-style-type: decimal">
<li><p>Prediction error and AIC depend on the <em>design</em>. Two
design issues to consider are the sample size and the distribution of
values of the explanatory variables. Changing the design can change what
is the “best” model, even though the underlying “true” model has not
changed. Both approaches attempt to identify the best model <em>we can
estimate for a given design</em>.</p></li>
<li><p>AIC is <em>relative</em>. Unlike simple cross-validation measures
it does not give any indication of how well a given model fits the data.
The AIC of a model is only interpretable <em>relative</em> to that of
another model <em>for the same data</em>.</p></li>
<li><p>There is little to no basis for what is a “significant” or
“meaningful” difference in an estimate of prediction error or AIC. One
reason is that we only have an <em>estimate</em> of the prediction error
or the quantity estimated by AIC. Another reason is that to put any
weight on a difference in prediction error or AIC we would need to
quantify the <em>cost</em> of using a better or worse model.</p></li>
<li><p>A complex issue is the effect of model selection on inferences.
That is, what is the sampling distribution of a quantity of interest if
we first use the data to select a model and then use that model to make
inferences. One approach is to use <em>model averaging</em>, but this is
not without its own problems.</p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
