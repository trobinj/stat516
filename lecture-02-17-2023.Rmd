---
output:
  html_document: 
    theme: readable
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "02-17-2023"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
output:
  html_document: 
    theme: readable
  pdf_document: default
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, out.width = "100%", fig.align = "center", cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"), comment = "")
```

```{r packages, echo = FALSE}
library(tidyverse)
suppressWarnings(library(kableExtra))
```

```{r utilities, echo = FALSE}
source("../../utilities.R")
```

```{r options, echo = FALSE}
options(digits = 4)
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`


## Solutions for Heteroscedasticity

We will discuss four solutions to heteroscedasticity in linear and nonlinear regression: variance-stabilizing transformations, weighted least squares, robust standard errors, and models that do not assume homoscedasticity.

### Variance-Stabilizing Transformations

The idea is to use $Y_i^* = g(Y_i)$ instead of $Y_i$ as the response variable, where $g$ is a *variance-stabilizing transformation*. 

**Example**: Consider again the cancer survival time data.
```{r}
library(Stat2Data)
data(CancerSurvival) 
CancerSurvival$Organ <- with(CancerSurvival, reorder(Organ, Survival, mean))
p <- ggplot(CancerSurvival, aes(x = Organ, y = Survival)) +
   geom_jitter(height = 0, width = 0.25) + 
   labs(y = "Survival Time (Days)") + theme_minimal()
plot(p)
m <- lm(Survival ~ Organ, data = CancerSurvival)

CancerSurvival$yhat <- predict(m)
CancerSurvival$rest <- rstudent(m)

p <- ggplot(CancerSurvival, aes(x = yhat, y = rest, color = Organ)) +
  geom_point(alpha = 0.5) + theme_minimal() + 
  labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```
A model for *log* time might exhibit something closer to homoscedasticity.
```{r}
p <- ggplot(CancerSurvival, aes(x = Organ, y = log(Survival))) +
   geom_jitter(height = 0, width = 0.25) + 
   labs(y = "log(Days)") + theme_minimal()
plot(p)
m <- lm(log(Survival) ~ Organ, data = CancerSurvival)

CancerSurvival$yhat <- predict(m)
CancerSurvival$rest <- rstudent(m)

p <- ggplot(CancerSurvival, aes(x = yhat, y = rest, color = Organ)) + 
   geom_point(alpha = 0.5) + theme_minimal() + 
   labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```
Comments on variance-stabilizing transformations.

1. Depending on the situation, other transformations may exhibit variance-stabilizing properties. Some common transformations are $\sqrt{Y_i}$, $\log(Y_i)$, $1/\sqrt{Y_i}$ and $1/Y_i$ for right-skewed response variables, and $n_i \sin^{-1}\sqrt{Y_i}$ when $Y_i$ is a proportion with a denominator of $n_i$. 

0. A limitation of variance stabilizing transformations is that it is often difficult (and undesirable) to to *interpret* the model in terms of the transformed response variable (although there are exceptions as we will later see with the log transformation in the context of accelerated failure time models for survival data). 

0. It is important to note that for any *nonlinear* transformation that $E[g(Y)] \neq g[E(Y)]$ (i.e., the expected transformed response does not necessarily equal the transformed expected response). For example, the expected log of survival time does not equal the log of the expected survival time. So we cannot obtain inferences for the expected response by applying the inverse function. For example, while we have that $\exp[\log(Y)] = Y$, this **does not** imply that $\exp\left\{E[\log(Y)]\right\} = E(Y)$. 

### Weighted Least Squares

A *weighted* least squares (WLS) estimator of the regression model parameters minimizes
$$
	\sum_{i=1}^n w_i (y_i - \hat{y}_i)^2,
$$
were $w_i > 0$ is the *weight* for the $i$-th observation. So-called *ordinary least squares* (OLS) or *unweighted least squares* is a special case where all $w_i$ = 1. 

To account for heteroscedasticity, the weights should be *inversely proportional to the variance of the response* so that 
$$
	w_i \propto \frac{1}{\text{Var}(Y_i)}.
$$
Estimation is *efficient* meaning that the *true* standard errors (which are not necessarily the *reported* standard errors shown by software since these are estimates and may be biased without using weights as defined above) are as small as they can be when using weighted least squares.

**Example**: One situation where we can anticipate heteroscedasticity and the need for weights is when the response variable is the mean of a varying number of observations. Consider the following data.
```{r, message = FALSE}
library(alr4)
head(allshoots)
allshoots$Type <- factor(allshoots$Type, labels = c("short","long"))
head(allshoots)
p <- ggplot(allshoots, aes(x = Day, y = ybar, size = n, color = Type)) +
   geom_point(alpha = 0.5) + theme_minimal() + 
   labs(x = "Days Since Dormancy", 
      y = "Mean Number of Stem Units per Shoot",
      size = "Number of Shoots")
plot(p)
```
The response variable is an *mean* of several observations so that
$$
  Y_i = \frac{Z_{i1} + Z_{i2} + \cdots + Z_{in_i}}{n_i}
$$
where $Z_{ij}$ is the length of the $j$-th shoot that goes into the $i$-th average, and a total of $n_i$ shoots go into the $i$-th average. If $\text{Var}(Z_{ij}) = \sigma^2$ then $\text{Var}(Y_i) = \sigma^2/n_i$. Thus the weights should be
$$
  w_i \propto \frac{1}{\sigma^2/n_i} = \frac{n_i}{\sigma^2}.
$$
Since $1/\sigma^2$ is a constant for all observations, we can define the weights as $w_i = n_i$. The weights can be specified in `lm` and `nls` (and other functions for regression) using the `weights` argument. 
```{r}
# weighted least squares
m <- lm(ybar ~ Type + Day + Type:Day, weights = n, data = allshoots)
cbind(summary(m)$coefficients, confint(m))
trtools::contrast(m, 
  a = list(Type = c("short","long"), Day = 1),
  b = list(Type = c("short","long"), Day = 0),
  cnames = c("short shoot slope","long shoot slope"))
```

**Example**: Consider again the cancer survival time data.
```{r}
p <- ggplot(CancerSurvival, aes(x = Organ, y = Survival)) +
   geom_jitter(height = 0, width = 0.25) + 
   labs(y = "Survival Time (Days)") + theme_minimal()
plot(p)
m.ols <- lm(Survival ~ Organ, data = CancerSurvival)

CancerSurvival$yhat <- predict(m.ols)
CancerSurvival$rest <- rstudent(m.ols)

p <- ggplot(CancerSurvival, aes(x = yhat, y = rest, color = Organ)) +
   geom_point(alpha = 0.5) + theme_minimal() +
   labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```
There are a couple of ways we could go with these data. One is that since we have a categorical explanatory variable with multiple observations per category, we could *estimate* the variance of $Y_i$ of each organ, and then set the weights to the reciprocals of these estimated variances.
```{r, echo = FALSE}
CancerSurvival <- CancerSurvival %>% select(-yhat, -rest)
```
```{r}
library(dplyr)
CancerSurvival %>% group_by(Organ) %>% 
  summarize(variance = var(Survival), weight = 1/var(Survival))
```
We can use the following to compute weights and add them to the data frame.
```{r}
CancerSurvival <- CancerSurvival %>% 
   group_by(Organ) %>% mutate(w = 1/var(Survival))
head(CancerSurvival)
```
Now let's estimate the model using weighted least squares with these weights and inspect the residuals. 
```{r}
m.wls <- lm(Survival ~ Organ, weights = w, data = CancerSurvival)

CancerSurvival$yhat <- predict(m.wls)
CancerSurvival$rest <- rstudent(m.wls)

p <- ggplot(CancerSurvival, aes(x = yhat, y = rest, color = Organ)) +
   geom_point(alpha = 0.5) + theme_minimal() + 
   labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```
Note how this affects our inferences.
```{r}
cbind(summary(m.ols)$coefficients, confint(m.ols))
cbind(summary(m.wls)$coefficients, confint(m.wls))
organs <- unique(CancerSurvival$Organ)
trtools::contrast(m.ols, a = list(Organ = organs), cnames = organs)
trtools::contrast(m.wls, a = list(Organ = organs), cnames = organs)
trtools::contrast(m.ols,
   a = list(Organ = "Breast"),
   b = list(Organ = c("Bronchus","Stomach","Colon","Ovary")),
   cnames = c("Breast vs Bronchus", "Breast vs Stomach", 
      "Breast vs Colon","Breast vs Ovary"))
trtools::contrast(m.wls,
   a = list(Organ = "Breast"),
   b = list(Organ = c("Bronchus","Stomach","Colon","Ovary")),
   cnames = c("Breast vs Bronchus", "Breast vs Stomach", 
      "Breast vs Colon","Breast vs Ovary"))
```
Here's how you can do the comparison of one level with all others using the `contrast` function from the **emmeans** package.
```{r}
library(emmeans)
contrast(emmeans(m.wls, ~ Organ), "trt.vs.ctrl", ref = "Breast", 
   reverse = TRUE, adjust = "none", infer = TRUE)
```
Another approach is to assume that the variance of the response variable is some function of its expected response, and thus the weights are a function of the expected response. With right-skewed response variables one common functional relationship is that
$$
   \text{Var}(Y_i) \propto E(Y_i),
$$
or, more generally,
$$
   \text{Var}(Y_i) \propto E(Y_i)^p,
$$
where $p$ is some power (usually $p \ge 1$). So the weights would then be
$$
   w_i \propto \frac{1}{E(Y_i)^p}.
$$
We do not know $E(Y_i)$, but $\hat{y}_i$ is an estimate of $E(Y_i)$. But we need the weights to compute $\hat{y}_i$!

Two situations:

1. Estimates of the model parameters and thus $\hat{y}_i$ *do not* depend on the weights.[^wghts] Here we can compute the weights with an initial regression model without weights.

2. Estimates of the model parameters and thus $\hat{y}_i$ *do* depend on the weights. An approach we can use here is *iteratively weighted least squares*.

[^wghts]: In practice the most common situation where this will happen is with models with only a single categorical explanatory variable (or models with multiple categorical explanatory variables that include interactions). 

It can be shown that $\hat{y}_i$ does not depend on the weights for the model for the `CancerSurvival` model. We can use the estimates from ordinary least squares to obtain weights of $w_i = 1/\hat{y}_i^p$.
```{r}
m.ols <- lm(Survival ~ Organ, data = CancerSurvival)

CancerSurvival$w <- 1/predict(m.ols) 
m.wls <- lm(Survival ~ Organ, data = CancerSurvival, weights = w)

CancerSurvival$yhat <- predict(m.wls)
CancerSurvival$rest <- rstudent(m.wls)

p <- ggplot(CancerSurvival, aes(x = yhat, y = rest, color = Organ)) +
   geom_point(alpha = 0.5) + theme_minimal() + 
   labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```
Maybe we could do better. Let's try $p$ = 2 --- i.e., $\text{Var}(Y_i) \propto E(Y_i)^2$.
```{r}
m.ols <- lm(Survival ~ Organ, data = CancerSurvival)

CancerSurvival$w <- 1/predict(m.ols)^2 
m.wls <- lm(Survival ~ Organ, data = CancerSurvival, weights = w)

CancerSurvival$yhat <- predict(m.wls)
CancerSurvival$rest <- rstudent(m.wls)

p <- ggplot(CancerSurvival, aes(x = yhat, y = rest, color = Organ)) +
   geom_point(alpha = 0.5) + theme_minimal() + 
   labs(x = "Predicted Value", y = "Studentized Residual")
plot(p)
```
**Example**: Consider again following data from a study on the effects of fuel reduction on biomass.
```{r}
library(trtools) # for biomass data

m <- lm(suitable ~ -1 + treatment:total, data = biomass)
summary(m)$coefficients

d <- expand.grid(treatment = c("n","y"), total = seq(0, 2767, length = 10))
d$yhat <- predict(m, newdata = d)

p <- ggplot(biomass, aes(x = total, y = suitable, color = treatment)) +
   geom_point() + geom_line(aes(y = yhat), data = d) + theme_minimal() +
   labs(x = "Total Biomass (kg/ha)", 
      y = "Suitable Biomass (kg/ha)",
      color = "Treatment")
plot(p)

biomass$yhat <- predict(m)
biomass$rest <- rstudent(m)

p <- ggplot(biomass, aes(x = yhat, y = rest, color = treatment)) +
   geom_point() + theme_minimal() + 
   labs(x = "Predicted Value", 
      y = "Studentized Residual", 
      color = "Treatment")
plot(p)
```
Here we might also assume that $\text{Var}(Y_i) \propto E(Y_i)^p$, with weights of $w_i = 1/\hat{y}_i$. But here things are a bit more complicated for this model: the $w_i$ depend on the $\hat{y}_i$, the $\hat{y}_i$ depend on the $w_i$. In the model for the `CancerSurvival` data this was not an issue because there the estimates of the model parameters, and thus $\hat{y}_i$, did not depend on the weights so we could use ordinary least squares where all $w_i$ = 1 to get the $\hat{y}_i$. But that is not true for this model. But we can solve this problem using *iteratively weighted least squares*. 
```{r}
biomass$w <- 1 # initial weights are all equal to one
for (i in 1:5) {
  m.wls <- lm(suitable ~ -1 + treatment:total, weights = w, data = biomass)
  print(coef(m.wls)) # optional
  print(biomass$w)   # optional
  biomass$w <- 1 / predict(m.wls)
}
```




