<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Nonlinear Regression and Heteroscedasticity</title>

<script src="site_libs/header-attrs-2.12/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics 436/516</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="lectures.html">Lectures</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Nonlinear Regression and
Heteroscedasticity</h1>
<h3 class="subtitle">Statistics 516, Homework 2</h3>

</div>


<p>You can also download a <a href="hw2.pdf">PDF</a> copy of this
homework assignment.</p>
<p>This homework assignment concerns specifying and the interpreting
(via inference) nonlinear regression models, and methods for accounting
for heteroscedasticty. You will likely need to install several packages
to access the data. You will need to install the
<strong>bootstrap</strong>, <strong>drc</strong>, and
<strong>alr4</strong> packages, as well as the <strong>trtools</strong>
and <strong>ggplot2</strong> packages which you should have already
installed.</p>
<div id="instructions" class="section level2">
<h2>Instructions</h2>
<ol style="list-style-type: decimal">
<li><p>This assignment is due by 5:00 PM on Friday, March 11th. Email me
your homework at <a href="mailto:trjohns@uidaho.edu"
class="email">trjohns@uidaho.edu</a>. If possible, save/export your
homework as a PDF file. Late assignments will be penalized by 10% if
turned-in within 12 hours of the deadline, and 10% more for each
additional 12 hour interval.</p></li>
<li><p>Your solutions must be <strong>typed</strong> and
<strong>very</strong> neatly organized. I will not try to infer your
solutions if they are not clearly presented. Mathematical expressions
need not be typeset perfectly but they should be clear. You may
substitute letters for symbols (e.g., b1 for <span
class="math inline">\(\beta_1\)</span>) and use other shortcuts for
mathematical notation if no meaning is lost.</p></li>
<li><p>You must include with your solutions the relevant R output
<strong>and</strong> R code that created them. Be sure that you provide
sufficient code that I can replicate your results. Include both the code
and the output within the text of your solutions (not in an appendix)
using cut-and-paste. But edit your output so as to provide only that
which is relevant to answering the questions. Use a monospace font
(e.g., Courier or Monaco) for R code and output for clarity. Do not use
a monospace font for text that is not R code or output.</p></li>
<li><p>Plots from R Studio can be exported in various formats or
directly to the clipboard using the “export” menu in the top-left part
of the plot panel.</p></li>
<li><p>It is permitted for you to discuss the homework with other
students in the course. However your work including R code, output, and
written answers must be your own.</p></li>
<li><p>You are very welcome to ask me questions. I will be happy to
clarify what I am asking in any of the questions and will provide you
some help with solving problems by showing you how to work through
similar problems from class. I will also be open to helping with any R
problems. If you email me with a R question, it will usually be helpful
for you to include enough of your R script so that I can replicate your
issue. But please avoid saving all your questions for just before the
assignment is due. I can usually respond quickly to questions, but I
will sometimes need time to respond.</p></li>
</ol>
<div style="page-break-after: always;"></div>
</div>
<div id="modeling-the-potency-of-two-herbicides" class="section level2">
<h2>Modeling the Potency of Two Herbicides</h2>
<p>The data frame <code>S.alba</code> in the <strong>drc</strong>
package contains data from an experiment investigating the potency of
two herbicides, bentazone and glyphosate, for use with white mustard.<a
href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> The data
are shown in the plot below.</p>
<pre class="r"><code>library(ggplot2)
library(drc)
p &lt;- ggplot(S.alba, aes(x = Dose, y = DryMatter)) + theme_minimal() + 
  geom_count(alpha = 0.25) + facet_wrap(~ Herbicide) + 
  labs(x = &quot;Dose (g/ha)&quot;, y = &quot;Dry Matter (g/pot)&quot;) + 
  theme(legend.position = c(0.95, 0.8))
plot(p)</code></pre>
<p><img src="hw2_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" />
Note the use of <code>geom_count</code> here. It can be used instead of
<code>geom_point</code> to make the size of the points proportional to
the number of points at a given location. Pots of plants were randomly
assigned to receive a specified dose of one of the two herbicides. The
amount of dry matter from each pot was later measured.</p>
<p>Assume that the goal of this study is to assess how the dose of each
of the two herbicides affect dry matter. To do this a nonlinear
regression model can be used with dry matter as the response variable,
and dose and herbicide as the explanatory variables. This model will
have the form <span class="math display">\[
  E(M) = \gamma + \frac{\delta - \gamma}{1 + e^{\beta(\log d - \log
\alpha)}},
\]</span> where <span class="math inline">\(M\)</span> and <span
class="math inline">\(d\)</span> are dry matter and dose,
respectively.<a href="#fn2" class="footnote-ref"
id="fnref2"><sup>2</sup></a> The four parameters of this model (i.e.,
<span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span>, <span
class="math inline">\(\delta\)</span>, and <span
class="math inline">\(\gamma\)</span>) have useful interpretations in
terms of how the expected dry matter is related to dose. The parameter
<span class="math inline">\(\delta\)</span> is the expected dry matter
at zero dose, and <span class="math inline">\(\gamma\)</span> is the
asymptote of expected dry matter as dose increases. The parameter <span
class="math inline">\(\alpha\)</span> is the dose value where the
expected dry matter is half way between its maximum value of <span
class="math inline">\(\delta\)</span> and its minimum value of <span
class="math inline">\(\gamma\)</span> — i.e., when <span
class="math inline">\(E(M) = (\delta+\gamma)/2\)</span>.<a href="#fn3"
class="footnote-ref" id="fnref3"><sup>3</sup></a> The parameter <span
class="math inline">\(\beta\)</span> is related to “how quickly” the
expected dry matter decreases as dose increases when dose equals <span
class="math inline">\(\alpha\)</span>. Specifically, it can be shown
that the slope of a tangent line when dose equals <span
class="math inline">\(\alpha\)</span> is <span
class="math inline">\(-\beta(\delta-\gamma)/(4\alpha)\)</span>, so
everything else being equal as <span
class="math inline">\(\beta\)</span> increases the expected dry matter
decreases “more quickly” as dose increases.<a href="#fn4"
class="footnote-ref" id="fnref4"><sup>4</sup></a> The plot below shows
this model with <span class="math inline">\(\alpha\)</span> = 20, <span
class="math inline">\(\beta\)</span> = 5, <span
class="math inline">\(\delta\)</span> = 5, and <span
class="math inline">\(\gamma\)</span> = 2.
<img src="hw2_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" />
The <strong>drc</strong> package provides functions the help automate
the estimation of a variety of nonlinear regression models like this one
for dose-response relationships. But here you will consider how to use
the <code>nls</code> function to estimate this model. Being proficient
at using a function like <code>nls</code> is very useful because then
you are not limited to using only those models programmed by other
authors. A feature of the <strong>drc</strong> package is that it
provides “self-starter” features that find good starting values for you
automatically. But when using <code>nls</code> it is up to you to find
good starting values. Fortunately for this particular model this is not
too difficult. You can relatively easily “eyeball” reasonable starting
values for <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\delta\)</span>, and <span
class="math inline">\(\gamma\)</span> by looking at a plot of the data.
Finding a good starting value for <span
class="math inline">\(\beta\)</span> can be a bit trickier, but here is
one strategy that can be used. Suppose we compute the mean value of
<code>DryMatter</code> for each combination of <code>Herbicide</code>
and <code>Dose</code> as follows.</p>
<pre class="r"><code>library(dplyr)
S.alba %&gt;% group_by(Herbicide, Dose) %&gt;% 
  summarize(drymatter = mean(DryMatter))</code></pre>
<pre><code># A tibble: 15 x 3
# Groups:   Herbicide [2]
   Herbicide   Dose drymatter
   &lt;fct&gt;      &lt;int&gt;     &lt;dbl&gt;
 1 Bentazone      0     3.84 
 2 Bentazone     10     3.72 
 3 Bentazone     20     3.42 
 4 Bentazone     40     1.2  
 5 Bentazone     80     0.75 
 6 Bentazone    160     0.625
 7 Bentazone    320     0.7  
 8 Bentazone    640     0.675
 9 Glyphosate     0     3.96 
10 Glyphosate    10     3.68 
11 Glyphosate    20     3.72 
12 Glyphosate    40     3.22 
13 Glyphosate    80     1.85 
14 Glyphosate   160     1.15 
15 Glyphosate   320     0.9  </code></pre>
<p>We can actually plot these means and connect them with line segments
by “adding” <code>stat_summary</code> to the earlier plot to provide a
very crude approximation to the model as shown below. Note that the
following also “zooms-in” on dose values between 0 and 200.</p>
<pre class="r"><code>p &lt;- p + stat_summary(fun = &quot;mean&quot;, geom = &quot;line&quot;) + 
  scale_x_continuous(limits = c(0, 200))
plot(p)</code></pre>
<p><img src="hw2_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" />
Recall that the slope of the tangent line when dose equals <span
class="math inline">\(\alpha\)</span> is <span
class="math inline">\(-\beta(\delta-\gamma)/(4\alpha)\)</span>. We can
approximate this slope by computing the slope of the line segment that
“contains” what we guess is the value of <span
class="math inline">\(\alpha\)</span>. For example, for the bentazone
herbicide if we guessed that <span class="math inline">\(\alpha\)</span>
was between 20 and 40 g/ha, then the slope of that line segment (using
the means computed above) equals <span class="math inline">\((1.2 -
3.42)/(40 - 20)\)</span>. Thus we might find an approximate value of
<span class="math inline">\(\beta\)</span> to use as a starting value if
we solve for <span class="math inline">\(\beta\)</span> in the equation
<span class="math display">\[
  \frac{1.2-3.42}{40-20} = \frac{-\beta(\delta-\gamma)}{4\alpha},
\]</span> where <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\gamma\)</span>, and <span
class="math inline">\(\delta\)</span> are replaced the values that you
“eyeballed” from the plot of the data to use as starting values. You may
find it useful to use this strategy to find a good starting value for
<span class="math inline">\(\beta\)</span> in your models.</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the nonlinear model described above using
<code>nls</code>. In this model assume that the type of herbicide does
not matter so your model will simply be <span class="math display">\[
  E(M_i) = \gamma + \frac{\delta - \gamma}{1 + e^{\beta(\log d_i - \log
\alpha)}},
\]</span> where <span class="math inline">\(M_i\)</span> and <span
class="math inline">\(d_i\)</span> are the <span
class="math inline">\(i\)</span>-th observations of dry matter and dose,
respectively. To find your starting values you can make a plot of the
data for both herbicides combined by omitting
<code>facet_wrap(~ Herbicide)</code> from the code given earlier to
produce a plot of the raw data without accounting for the type of
herbicide. And to compute the sample means for each dose but not for
each combination of dose use <code>group_by(Dose)</code> instead of
<code>group_by(Herbicide, Dose)</code> in the code given earlier for
computing these means. Give the parameter estimates and their standard
errors using the <code>summary</code> function, and plot the model by
adding a smooth curve to the plot to show the estimated expected
response as a function of dose. Note that if you add this curve to the
original plot then the data frame of predicted values must include the
type of herbicide even though it is not part of your model (see the
first problem from the in-class exercise with the Michaelis-Menten
model).</p></li>
<li><p>Estimate a nonlinear model where the <span
class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span>, and <span
class="math inline">\(\gamma\)</span> parameters vary by herbicide, but
<span class="math inline">\(\delta\)</span> does not, using the
<code>nls</code> function. This model can be written case-wise as <span
class="math display">\[
  E(M_i) =
  \begin{cases}
\gamma_b + \frac{\delta - \gamma_b}{1 + e^{\beta_b\left(\log d_i - \log
\alpha_b\right)}},
   &amp; \text{if the herbacide used was bentazone}, \\
\gamma_g + \frac{\delta - \gamma_g}{1 + e^{\beta_g\left(\log d_i - \log
\alpha_g\right)}},
   &amp; \text{if the herbacide used was glyphosate}.
  \end{cases}  
\]</span> The rationale for this model is that when the dose is zero
there should be no difference in the expected response as a function of
the type of herbicide, so <span class="math inline">\(\delta\)</span>
should not depend on the type of herbicide used. Report the estimates
and standard errors of the seven parameters using the
<code>summary</code> function. Also plot this model with the raw data by
adding a smooth curve to the first plot shown above to show the
estimated expected response as a function of dose and type of
herbicide.</p></li>
<li><p>A researcher might like to make inferences about the difference
in the <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span>, and <span
class="math inline">\(\gamma\)</span> parameters between the two
herbicides. Use the <code>lincon</code> function to produce estimates,
standard errors, confidence intervals, and tests concerning <span
class="math inline">\(\alpha_b-\alpha_g\)</span>, <span
class="math inline">\(\beta_b-\beta_g\)</span>, and <span
class="math inline">\(\gamma_b-\gamma_g\)</span>.</p></li>
</ol>
</div>
<div id="jevons-gold-sovereigns" class="section level2">
<h2>Jevon’s Gold Sovereigns</h2>
<p>The data frame <code>jevons</code> in the <strong>alr4</strong>
package contains summary statistics on the weights of <a
href="https://en.wikipedia.org/wiki/Sovereign_(British_coin)">sovereigns</a>
(i.e., British gold coins) that were collected from circulation in
Manchester, England. These data are from a paper by the 19th century
economist and philosopher <a
href="https://en.wikipedia.org/wiki/William_Stanley_Jevons">William
Stanley Jevons</a>.<a href="#fn5" class="footnote-ref"
id="fnref5"><sup>5</sup></a> This data frame (shown below) gives the
mean and standard deviation of the weights of five samples of sovereigns
that vary by age (in decades).</p>
<pre class="r"><code>library(alr4)
jevons</code></pre>
<pre><code>  Age   n Weight      SD   Min   Max
1   1 123  7.973 0.01409 7.900 7.999
2   2  78  7.950 0.02272 7.892 7.993
3   3  32  7.928 0.03426 7.848 7.984
4   4  17  7.896 0.04057 7.827 7.965
5   5  24  7.873 0.05353 7.757 7.961</code></pre>
<p>We do not have the original data, so for the purpose of this exercise
you will create an artificial data set that produces data with the same
sample sizes, means, and standard deviations.<a href="#fn6"
class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<pre class="r"><code>library(dplyr)
library(tidyr)

set.seed(123)
coins &lt;- jevons %&gt;% uncount(n) %&gt;% 
  group_by(Age) %&gt;% mutate(y = rnorm(n(), Weight, SD)) %&gt;%
  mutate(y = SD * (y - mean(y))/sd(y) + Weight) %&gt;%
  dplyr::select(Age, y) %&gt;% rename(Weight = y)</code></pre>
<p>We can confirm that these artificial data give the same means and
standard deviations as the original data.</p>
<pre class="r"><code>coins %&gt;% group_by(Age) %&gt;% 
  summarize(n = n(), meanweight = mean(Weight), sdweight = sd(Weight))</code></pre>
<pre><code># A tibble: 5 x 4
    Age     n meanweight sdweight
  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;
1     1   123       7.97   0.0141
2     2    78       7.95   0.0227
3     3    32       7.93   0.0343
4     4    17       7.90   0.0406
5     5    24       7.87   0.0535</code></pre>
<p>The figure below shows a plot of the simulated data.</p>
<pre class="r"><code>p &lt;- ggplot(coins, aes(x = factor(Age), y = Weight)) + theme_minimal() + 
  geom_hline(yintercept = 7.9876) + 
  geom_hline(yintercept = 7.9379, linetype = 3) + 
  geom_dotplot(binaxis = &quot;y&quot;, binwidth = 0.001, method = &quot;histodot&quot;) +
  coord_flip() + labs(x = &quot;Age (decades)&quot;, y = &quot;Weight (g)&quot;) 
plot(p)</code></pre>
<p><img src="hw2_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" />
The solid line shows the intended standard weight of newly minted
sovereigns (7.9876 g), and the dotted line shows the minimum legal
weight (7.9379 g). Perhaps not unsurprisingly it can be seen that, on
average, older sovereigns have less weight, presumably due to wear while
in circulation. But note also that the <em>variability</em> of weight
appears to increase with the age of the coins. This may be due to
differences in how much the coins are in circulation. Some coins are
frequently being exchanged thus losing more material, whereas others may
being exchanged less and thus not losing as much material. In this
problem you will consider various ways of dealing with the
heteroscedasticity as well as the consequences of failing to account for
heteroscedasticity.</p>
<ol style="list-style-type: decimal">
<li><p>Estimate a linear model using the <code>lm</code> function with
<code>Weight</code> as the response variable and <code>Age</code> as the
explanatory variable. In your model treat age as a <em>factor</em>
(i.e., a categorical variable) and not a quantitative variable. You can
do this by either using <code>factor(Age)</code> instead of
<code>Age</code> in the model formula, or by creating a new variable
such as <code>coins$Agef &lt;- factor(coins$Age)</code> which will
coerce the variable into a factor. Use either <code>contrast</code>
<em>or</em> functions from the <strong>emmeans</strong> package to
produce estimates, standard errors, and confidence intervals for (a) the
expected weight of coins from each age group and (b) the difference in
the expected weight between the the newest coins (i.e., age of one
decade) and the other four groups of coins.<a href="#fn7"
class="footnote-ref" id="fnref7"><sup>7</sup></a> For this model you
should find that the estimated expected weights are equal the
corresponding sample means, and that the estimated differences in the
expected weights are equal to the differences in the corresponding
sample means.</p></li>
<li><p>Assume that the variances vary by decade so that <span
class="math display">\[
Y_i =
\begin{cases}
  \sigma_1^2, &amp; \text{if the $i$-th observation is of a coin one
decade old}, \\
  \sigma_2^2, &amp; \text{if the $i$-th observation is of a coin two
decades old}, \\
  \sigma_3^2, &amp; \text{if the $i$-th observation is of a coin three
decades old}, \\
  \sigma_4^2, &amp; \text{if the $i$-th observation is of a coin four
decades old}, \\
  \sigma_5^2, &amp; \text{if the $i$-th observation is of a coin five
decades old}.
\end{cases}
\]</span> There are a couple of different ways to account for this kind
of variance structure. One is to use <em>weighted</em> least squares
where the weights are estimated as the reciprocals of the sample
variances. We discussed how to compute these weights using functions
from the <strong>dplyr</strong> package. Another approach is to use a
parametric model where the five variances are effectively estimated from
the data. We discussed how to do this with using the <code>gls</code>
function from the <strong>nlme</strong> package. Use both of these
approaches and for each show the parameter estimates and their standard
errors as given by <code>summary</code>, and use either the
<code>contrast</code> function or functions from the
<strong>emmeans</strong> package to produce estimates, standard errors,
and confidence intervals for (a) the expected weight of coins from each
age group and (b) the difference in the expected weight between the the
newest coins, just as you did in the previous problem.<a href="#fn8"
class="footnote-ref" id="fnref8"><sup>8</sup></a></p></li>
<li><p>Compare the estimates and standard errors for estimating the
model parameters as well as the expected weight and differences in
expected weight when accounting for heteroscedasticity as you did in the
last problem, and when not accounting for heteroscedasticity as you did
in the first problem. Discuss briefly how failing to account
heteroscedasticity (i.e., incorrectly assuming homoscedasticity) may
affect your inferences.</p></li>
</ol>
</div>
<div id="mortality-of-confused-flour-beetles-from-carbon-disulphide"
class="section level2">
<h2>Mortality of Confused Flour Beetles from Carbon Disulphide</h2>
<p>The data frame <code>bliss</code> in the <strong>trtools</strong>
package are from an experiment investigating the effect of gaseous
carbon disulphide (<span class="math inline">\(\text{CS}_2\)</span>) on
the mortality of <a
href="https://en.wikipedia.org/wiki/Confused_flour_beetle">confused
flour beetles (<em>Tribolium confusum</em>)</a>.<a href="#fn9"
class="footnote-ref" id="fnref9"><sup>9</sup></a> Cloth cages of batches
of approximately thirty beetles were suspended in a flask above a fixed
volume of liquid carbon disulphide. The number of dead beetles after
five hours of exposure was recorded. The figure below shows the
proportion of dead beetles by concentration of carbon disulphide. Note
that there are two observations for each dose.</p>
<pre class="r"><code>library(trtools) 
library(ggplot2)
library(ggrepel)

bliss$proportion &lt;- paste(bliss$dead, &quot;/&quot;, bliss$exposed, sep = &quot;&quot;)

p &lt;- ggplot(bliss, aes(x = concentration, y = dead/exposed)) +
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) + 
  labs(x = &quot;Concentration of Carbon Disulphide (mg/liter)&quot;,
    y = &quot;Proportion of Beetles Dying&quot;)
plot(p)</code></pre>
<p><img src="hw2_files/figure-html/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" />
A naive approach to modeling these data would be to use linear
regression where the proportion is the response variable.</p>
<pre class="r"><code>m &lt;- lm(dead/exposed ~ concentration, data = bliss)
cbind(summary(m)$coefficients, confint(m))</code></pre>
<pre><code>              Estimate Std. Error t value  Pr(&gt;|t|)    2.5 %   97.5 %
(Intercept)   -1.73277   0.159636  -10.85 3.352e-08 -2.07515 -1.39038
concentration  0.03717   0.002516   14.77 6.225e-10  0.03178  0.04257</code></pre>
<pre class="r"><code>d &lt;- data.frame(concentration = seq(49.06, 76.54, length = 100))
d$yhat &lt;- predict(m, newdata = d)

p &lt;- ggplot(bliss, aes(x = concentration, y = dead/exposed)) +
  geom_line(aes(y = yhat), data = d) + 
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) + 
  labs(x = &quot;Concentration of Carbon Disulphide (mg/liter)&quot;,
    y = &quot;Proportion of Beetles Dying&quot;)
plot(p)</code></pre>
<p><img src="hw2_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" />
This model is probably not adequate for two reasons. One is that the
relationship between the expected of dead beetles and concentration is
probably not linear. Secondly, proportions tend to exhibit
heteroscedasticity where the variance of a proportion tends to decrease
as its expected value gets farther from 0.5. As we will discuss in
lecture, if the number of dead beetles has a <em>binomial
distribution</em>, then it can be shown that <span
class="math display">\[
  \text{Var}(P) = E(P)[1-E(P)]/m,
\]</span> where <span class="math inline">\(P\)</span> is the proportion
(i.e., <code>dead/exposed</code>) and <span
class="math inline">\(m\)</span> is the denominator of the proportion
(i.e., <code>exposed</code>). This implies that the variance of <span
class="math inline">\(P\)</span> decreases as <span
class="math inline">\(E(P)\)</span> gets farther from 0.5, and also
decreases as <span class="math inline">\(m\)</span> increases.</p>
<p>These data will be used in lecture to demonstrate logistic
regression, but for this problem you will consider modeling the data
using nonlinear regression. Later we will also discuss the relationship
between logistic and nonlinear regression.</p>
<ol style="list-style-type: decimal">
<li><p>Consider the nonlinear regression model <span
class="math display">\[
  E(P_i) = \frac{1}{1 + e^{-\beta_0 - \beta_1d_i}},
\]</span> where <span class="math inline">\(P_i\)</span> and <span
class="math inline">\(d_i\)</span> are the <span
class="math inline">\(i\)</span>-th observations of the proportion of
dead beetles (i.e., <code>dead/exposed</code>) and the concentration,
respectively.<a href="#fn10" class="footnote-ref"
id="fnref10"><sup>10</sup></a> Use the <code>nls</code> function to
estimate this nonlinear regression model. For starting values you can
cheat and use the parameter estimates from a logistic regression model
estimated as followed.</p>
<pre class="r"><code>m &lt;- glm(cbind(dead, exposed - dead) ~ concentration, 
  family = binomial, data = bliss)
summary(m)$coefficients</code></pre>
<pre><code>              Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept)   -14.8084    1.28976  -11.48 1.633e-30
concentration   0.2492    0.02138   11.65 2.250e-31</code></pre>
<p>The two estimates reported above are estimates of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> from the logistic regression
model. You can use these as your starting values for <code>nls</code>.
The estimates you obtain using nonlinear regression should be similar
but not necessarily equal to those shown above. Report the parameter
estimates and their standard errors by showing the output from
<code>summary</code>. Also plot the estimated model by adding a curve to
the plot shown above.<a href="#fn11" class="footnote-ref"
id="fnref11"><sup>11</sup></a></p></li>
<li><p>As described above, the number of dead beetles has a binomial
distribution then the variance of <span
class="math inline">\(P_i\)</span> is <span class="math display">\[
  \text{Var}(P_i) = E(P_i)[1-E(P_i)]/m_i,
\]</span> where <span class="math inline">\(m_i\)</span> is the number
of exposed beetles for the <span class="math inline">\(i\)</span>-th
observation. Since <span class="math inline">\(E(P_i)\)</span> is
unknown it can be estimated as the predicted value <span
class="math inline">\(\hat{Y_i}\)</span>. Use an iteratively weighted
least squares algorithm with weights implied by the variance above to
estimate the model shown above and show the parameter estimates and
standard errors using <code>summary</code>. You should find that the
parameter estimates will be equal to or very close to those obtained
using the <code>glm</code> function above, but the standard errors will
be somewhat different.</p></li>
</ol>
</div>
<div
id="estimating-bias-in-field-measurements-in-defects-in-the-alaska-pipeline"
class="section level2">
<h2>Estimating Bias in Field Measurements in Defects in the Alaska
Pipeline</h2>
<p>The data frame <code>pipeline</code> in the <strong>alr4</strong>
package is from a study of the bias of field measurements of defects in
the <a
href="https://en.wikipedia.org/wiki/Trans-Alaska_Pipeline_System">Alaska
pipeline</a>. That data includes field and laboratory measurements of
the number of defects in observational units from the pipeline.<a
href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<pre class="r"><code>p &lt;- ggplot(pipeline, aes(x = Lab, y = Field)) + theme_minimal() +
  geom_abline(intercept = 0, slope = 1, linetype = 3) + geom_point() + 
  labs(x = &quot;Laboratory Measurement&quot;, y = &quot;Field Measurement&quot;)
plot(p)</code></pre>
<p><img src="hw2_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" />
Assume that the laboratory measurements are very accurate and can be
treated as the “true” number of defects. Field measurements are faster
and cheaper than laboratory measurements, but are more prone to
measurement error (both systematic error or <em>bias</em>, and random
measurement error). The figure above suggests that the field
measurements tend to underestimate the number of defects, particularly
as the actual number of defects (as shown by the laboratory measurement)
increases. A regression model can be used to estimate the bias of the
field measurements so that they can be adjusted appropriately (a process
sometimes called <em>calibration</em>). In this problem you will use
linear and nonlinear regression to estimate a calibration model.</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(F_i\)</span> and <span
class="math inline">\(L_i\)</span> denote the field and laboratory
measurements, respectively, for the <span
class="math inline">\(i\)</span>-th observation. If we assume that <span
class="math inline">\(L_i\)</span> is the true number of defects in the
<span class="math inline">\(i\)</span>-th observational unit, then the
bias is the expected difference between <span
class="math inline">\(F_i\)</span> and <span
class="math inline">\(L_i\)</span> which is <span
class="math inline">\(E(F_i - L_i)\)</span>. The figures suggests that
the bias tends to increase as <span class="math inline">\(L_i\)</span>
increases, so one possible model might be that the bias is proportional
to <span class="math inline">\(L_i\)</span>. This can be written as
<span class="math display">\[
  E(F_i - L_i) = \theta L_i,
\]</span> where <span class="math inline">\(\theta\)</span> is the
constant of proportionality. If <span class="math inline">\(\theta &lt;
0\)</span> then the field measurements tend to <em>underestimate</em>
the number of defects by <span class="math inline">\((1 -
\theta)100\)</span>% (assuming that <span class="math inline">\(\theta
&gt; 0\)</span>), and if <span class="math inline">\(\theta &gt;
0\)</span> then the field measurements tend to <em>overestimate</em> the
number of defects by <span class="math inline">\((\theta -
1)100\)</span>%. The model shown above is linear so it can be estimated
using <code>lm</code>. There are several ways to do this. One is to use
<span class="math inline">\(F_i - L_i\)</span> as the response variable
and estimate a model with <span class="math inline">\(L_i\)</span> as
the explanatory variable but <em>without</em> a constant term (i.e.,
“intercept”).<a href="#fn13" class="footnote-ref"
id="fnref13"><sup>13</sup></a> A second approach is to write the model
as <span class="math display">\[
  E(F_i) = L_i + \theta L_i
\]</span> if we regard <span class="math inline">\(L_i\)</span> as a
fixed and not random variable so that <span class="math inline">\(E(F_i
- L_i) = E(F_i) - L_i\)</span>.<a href="#fn14" class="footnote-ref"
id="fnref14"><sup>14</sup></a> This model is a special case of the
linear model <span class="math display">\[
  E(F_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2},
\]</span> where <span class="math inline">\(\beta_0\)</span> = 0, <span
class="math inline">\(\beta_1\)</span> = 1, <span
class="math inline">\(\beta_2 = \theta\)</span>, and <span
class="math inline">\(x_{i1} = x_{i2} = L_i\)</span>. This model is a
little strange in that we have two explanatory variables that are the
same variable, but <span class="math inline">\(\beta_1\)</span> is not
estimated but instead is fixed at one. This can be done by specifying an
<em>offset</em> in your model (see footnote).<a href="#fn15"
class="footnote-ref" id="fnref15"><sup>15</sup></a> One last approach is
to write the model as <span class="math display">\[
  E(F_i) = \gamma L_i
\]</span> where <span class="math inline">\(\gamma = 1 + \theta\)</span>
since <span class="math display">\[
  E(F_i) = L_i + \theta L_i = (1 + \theta)L_i = \gamma L_i.
\]</span> Estimate the three models described above in the way described
and show the estimate and standard error of the model parameter (i.e.,
<span class="math inline">\(\theta\)</span> or <span
class="math inline">\(\gamma\)</span>) using <code>summary</code>. The
first two models should give you the same estimate and standard error of
<span class="math inline">\(\theta\)</span>, and the last model should
give you an estimate of <span class="math inline">\(\gamma\)</span> that
equals the estimate of <span class="math inline">\(1 + \theta\)</span>
from the previous models.</p></li>
<li><p>Plot the model you estimated in the previous problem with the raw
data to show a plot like that given earlier but with a line showing the
estimated expected field measurement as a linear function of laboratory
measurement. You will want to use either the second or third model you
estimated in the previous problem to do this since the first model uses
the difference in the field and laboratory measurements as the response
variable and so the predicted values from that model are not what you
want for plotting purposes.</p></li>
<li><p>Plot the studentized residuals against the predicted values using
either the second or third model you estimated in the first problem. Do
you think that the expected field measurement is a linear function of
the laboratory measurement? Why or why not? Is there any evidence of
heteroscedasticity? Why or why not?</p></li>
<li><p>Consider an alternative nonlinear model where <span
class="math display">\[
  E(F_i) = L_i + \theta_1 L_i^{\theta_2}.
\]</span> This implies that the bias of the field measurements is
proportional to some power <span class="math inline">\(\theta_2\)</span>
of the laboratory measurements. Note that the second model you estimated
in the first problem is a special case of this model where <span
class="math inline">\(\theta_2\)</span> = 1. Estimate the model above
using <code>nls</code>. For your starting values you can use the
estimate of <span class="math inline">\(\theta_1\)</span> you obtained
in the first problem and <span class="math inline">\(\theta_2\)</span> =
1, since that was the value you implicitly used for those models which
can be viewed as an approximation to the model above. Show the parameter
estimates and their standard errors using <code>summary</code>, and plot
the model as a curve with the raw data like you did in the second
problem.</p></li>
<li><p>Plot the standardized residuals against the predicted values
based on the nonlinear model you estimated in the previous problem. You
cannot use <code>rstandard</code> or <code>rstudent</code> with a
<code>nls</code> object, but you can use the <code>nlsint</code>
function from the <strong>trtools</strong> package to produce
standardized residuals. The syntax for a basic plot would something like
the following where <code>m</code> is your model object created using
<code>nls</code>.</p>
<pre class="r"><code>d &lt;- nlsint(m, residuals = TRUE)
plot(d$fit, d$res)</code></pre>
<p>Now consider accounting for any heteroscedasticty in the data by
assuming that the variance of the field measurements is proportional to
some power <span class="math inline">\(p\)</span> of the expected field
measurement so that<br />
<span class="math display">\[
  \text{Var}(F_i) \propto E(Y_i)^p.
\]</span> Use an iteratively weighted least squares algorithm to
estimate the nonlinear model described in the previous problem for
several values of <span class="math inline">\(p\)</span>, starting with
<span class="math inline">\(p\)</span> = 1 and trying increasingly
larger values of increments of 0.5 up to <span
class="math inline">\(p\)</span> = 3. Using residual plots, decide on
what you think is a good value of <span class="math inline">\(p\)</span>
and then show the parameter estimates with standard errors for that
model using <code>summary</code> and give another plot of the residuals
against the predicted values for that model. Also discuss briefly why
you selected that particular value of <span
class="math inline">\(p\)</span>.<a href="#fn16" class="footnote-ref"
id="fnref16"><sup>16</sup></a></p></li>
</ol>
</div>
<div id="anti-inflammatory-hormone-devices-revisited"
class="section level2">
<h2>Anti-Inflammatory Hormone Devices — Revisited</h2>
<p><strong>Note</strong>: This problem is <em>extra credit</em> for
students in Stat 436, but is <em>required</em> for students in Stat
516.</p>
<p>The <code>nls</code> function computationally works very similarly to
the <code>lm</code> function, but the interface is different. The
<code>lm</code> function allows us to specify a model
<em>symbolically</em> via the model formula (i.e., the first argument to
<code>lm</code>), whereas <code>nls</code> requires us to specify the
model <em>mathematically</em>. The <code>nls</code> function can be used
to estimate a linear model. In practice, this is rarely necessary except
maybe in cases where you are using a fairly unusual parameterization of
a linear model that is difficult to express using the model formula
argument to <code>lm</code>. But I think it can be a useful exercise for
the student to use <code>nls</code> to specify a linear model. In this
problem you will use the <code>nls</code> function to replicate several
models for the <code>hormone</code> data from the
<strong>bootstrap</strong> package that were featured in the last
homework assignment.</p>
<p>In the following you are to estimate the model specified with
<code>lm</code> by using <code>nls</code>. To do this I would recommend
that you “decipher” the model from the output of <code>summary</code>
given below, and then write the model case-wise. I would also suggest
you use either the <code>case_when</code> function from the
<strong>dplyr</strong> package or specify indicator variables within the
model itself using the <code>==</code> operator (see the second problem
from the <a href="lecture-02-25-2022.html">in-class exercise on February
25th</a> for an example of doing this with another model — for example,
an indicator variable for Lot A would be specified as
<code>Lot == "A"</code>). I would recommend against using the
<code>ifelse</code> function. If you do this correctly then the output
from <code>summary</code> when applied to the model object created using
<code>nls</code> should match that created by <code>lm</code>. When you
do this make sure that the parameter estimates are in the same order
(this can be controlled by the order you specify the parameter starting
values). Note that since these models are all linear you do not need to
specify good starting values. It is fine to specify them all as zero or
some other number (as long as they are not very large in absolute
value).</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ Lot, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)   23.078      1.962 11.7630 1.887e-11
LotB          -1.011      2.775 -0.3644 7.187e-01
LotC           5.844      2.775  2.1065 4.581e-02</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ -1 + Lot, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>     Estimate Std. Error t value  Pr(&gt;|t|)
LotA    23.08      1.962   11.76 1.887e-11
LotB    22.07      1.962   11.25 4.717e-11
LotC    28.92      1.962   14.74 1.583e-13</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ Lot:hrs, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept) 35.01572   0.736247  47.560 1.783e-24
LotA:hrs    -0.07728   0.005146 -15.016 2.238e-13
LotB:hrs    -0.05566   0.003142 -17.714 6.696e-15
LotC:hrs    -0.05722   0.007423  -7.709 8.045e-08</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ Lot + hrs + Lot:hrs, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>             Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept) 33.360055   1.211583 27.5343 5.787e-18
LotB         1.846061   1.612797  1.1446 2.652e-01
LotC         3.833616   1.933112  1.9831 6.058e-02
hrs         -0.068296   0.007272 -9.3911 5.753e-09
LotB:hrs     0.012010   0.008291  1.4486 1.622e-01
LotC:hrs    -0.006222   0.014670 -0.4241 6.758e-01</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ -1 + Lot + Lot:hrs, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>         Estimate Std. Error t value  Pr(&gt;|t|)
LotA     33.36006   1.211583  27.534 5.787e-18
LotB     35.20612   1.064509  33.073 1.340e-19
LotC     37.19367   1.506316  24.692 5.341e-17
LotA:hrs -0.06830   0.007272  -9.391 5.753e-09
LotB:hrs -0.05629   0.003982 -14.136 3.361e-12
LotC:hrs -0.07452   0.012740  -5.849 8.330e-06</code></pre></li>
</ol>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Christensen, M. G., Teicher, H. B., &amp; Streibig, J.
C. (2003). Linking fluorescence induction curve and biomass in herbicide
screening. <em>Pest Management Science</em>, <em>59</em>, 1303–1310.<a
href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Note that <span class="math inline">\(e^x\)</span> is
the <a
href="https://en.wikipedia.org/wiki/Exponential_function">exponential
function</a> where <span class="math inline">\(e \approx\)</span> 2.718
is <a
href="https://en.wikipedia.org/wiki/E_(mathematical_constant)">Euler’s
number</a>. This function is also written as <span
class="math inline">\(\exp(x)\)</span>, and in R it is written as
<code>exp(x)</code>.<a href="#fnref2"
class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Because <span class="math inline">\(\log(0)\)</span> is
not defined, <span class="math inline">\(E(M)\)</span> is not defined
<em>mathematically</em> if the dose equals <span
class="math inline">\(\alpha\)</span>. But computers will typically
evaluate <span class="math inline">\(\log(0)\)</span> as <span
class="math inline">\(-\infty\)</span> because of the one-sided limit
<span class="math inline">\(\lim_{x \to 0+} \log(x) = -\infty\)</span>.
And for a similar reason <span
class="math inline">\(e^{-\infty}\)</span> is evaluated as 0 by
computers.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This can be seen by differentiation to show that <span
class="math display">\[
  \left.\frac{\partial E(M)}{\partial d}\right|_{d = \alpha} =
\frac{-\beta(\delta-\gamma)}{4\alpha}.
\]</span><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Jevons, W. S. (1868). On the condition of the metallic
currency of the United Kingdom, with reference to the question of
international coinage. <em>Journal of the Statistical Society of
London</em>, <em>31</em>, 426–464.<a href="#fnref5"
class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>There are a couple of things to note here. One is the
use of <code>set.seed</code>. The simulated data are generated using the
random number generators in R. This initializes the state of the random
number generator so that anyone using this code would produce the
<em>same</em> random numbers. The other thing to note is the use of
<code>dplyr::select</code>. The function <code>select</code> from the
<strong>dplyr</strong> package is used to select certain variables from
a data frame (and thus deselect others). But there is a function of the
same name in the <strong>MASS</strong> package that does something very
different. The <strong>MASS</strong> package is frequently loaded with
other packages, so to avoid potential conflicts I will often use
<code>dplyr::select</code> out of habit to avoid problems.<a
href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>For an example of how to specify comparisons between one
level and the other levels with functions in the
<strong>emmeans</strong> package, see the example from the lecture on <a
href="lecture-02-18-2022.html">February 18</a> where I use the
<code>trt.vs.ctrl</code> contrast method with the <code>contrast</code>
function from the <strong>emmeans</strong> package (not the
<strong>trtools</strong> package).<a href="#fnref7"
class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Both the <code>contrast</code> function from the
<strong>trtools</strong> package and functions from the
<strong>emmeans</strong> package should give the same estimates and
standard errors. They will give somewhat different confidence intervals
and p-values, however, because of how the two functions compute the
degrees of freedom by default. They can be brought into agreement by
using some extra options, but for the purpose of this problem that is
not necessary.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>The data are featured in Bliss, C. I. (1935). The
calculation of the dosage-mortality curve. <em>Annals of Applied
Biology</em>, <em>22</em>, 134–167. But the original source is Strand,
A. L. (1930). Measuring the toxicity of insect fumigants. <em>Industrial
and Engineering Chemistry: Analytical Edition</em>, <em>2</em>, 4–8.<a
href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Note that <span class="math inline">\(e^x\)</span> is
the <a
href="https://en.wikipedia.org/wiki/Exponential_function">exponential
function</a> where <span class="math inline">\(e \approx\)</span> 2.718
is <a
href="https://en.wikipedia.org/wiki/E_(mathematical_constant)">Euler’s
number</a>. This function is also written as <span
class="math inline">\(\exp(x)\)</span>, and in R it is written as
<code>exp(x)</code>.<a href="#fnref10"
class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Note that when using <code>ggplot</code> the
<em>order</em> that you specify the various geometric objects matters.
For example, if <code>geom_line</code> appears before
<code>geom_label_repel</code> then the point labels will be shown in
front of rather than behind the curve.<a href="#fnref11"
class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>The nature of the observational units and the
measurement of the number of defects is not clear. The observational
units may be select portions of the pipeline, but it is not clear if
these units were removed from the pipeline and brought to a laboratory,
or if only the data from the field was brought back to the laboratory
for more thorough analysis. Also since the laboratory values are not all
integers these measurements might be the number of defects per unit area
or volume. Finally, note that in the help file (see
<code>?pipeline</code>) the <code>Lab</code> variable is incorrectly
labeled as “Number of defects measured in the field.”<a href="#fnref12"
class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>I have given several examples in lecture of how to
estimate a model with <code>lm</code> that does not include an
constant/intercept term.<a href="#fnref13"
class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>From a design perspective, <span
class="math inline">\(L_i\)</span> may not be fixed since the values are
not necessarily selected by the researchers. But in regression we
frequently regard all variables except for the response variable as
fixed. Technically what we are doing is <em>conditioning</em> on the
values of the explanatory variables, so even if they are random we are
only considering the distribution of the response variable
<em>given</em> those values of the explanatory variables.<a
href="#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>An offset is an explanatory variable that has a <span
class="math inline">\(\beta_j\)</span> fixed at one. This can be done by
using <code>offset(variable)</code> in your model formula. For example,
consider the model <span class="math display">\[
  E(V_i) = \beta_0 + \beta_1g_i + \beta_2h_i,
\]</span> where <span class="math inline">\(V_i\)</span> is tree volume,
<span class="math inline">\(g_i\)</span> is girth, and <span
class="math inline">\(h_i\)</span> is height. We can estimate this model
as follows.</p>
<pre class="r"><code>m &lt;- lm(Volume ~ Girth + Height, data = trees)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept) -57.9877     8.6382  -6.713 2.750e-07
Girth         4.7082     0.2643  17.816 8.223e-17
Height        0.3393     0.1302   2.607 1.449e-02</code></pre>
<p>But if I wanted <span class="math inline">\(\beta_1\)</span> = 1 then
I could do the following.</p>
<pre class="r"><code>m &lt;- lm(Volume ~ offset(Girth) + Height, data = trees)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)  -80.935    23.6206  -3.426 0.0018484
Height         1.288     0.3097   4.157 0.0002608</code></pre>
<p>Note that no inferences for <span
class="math inline">\(\beta_1\)</span> are given by <code>summary</code>
because we are assuming we know it is one so there is nothing to
infer.<a href="#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>There is not necessarily a correct value of <span
class="math inline">\(p\)</span> here, although some values may be
clearly better than others.<a href="#fnref16"
class="footnote-back">↩︎</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
