---
output:
  html_document: 
    theme: readable
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "03-04-2024"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment = "", echo = TRUE, fig.height = 4, message = FALSE, out.width = "100%", fig.align = "center", cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

```{r packages, echo = FALSE}
library(tidyverse)
suppressWarnings(library(kableExtra))
```

```{r utilities, echo = FALSE}
source("../../utilities.R")
```

```{r options, echo = FALSE}
options(digits = 4, width = 100)
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`

## Proportions as Response Variables

Consider the following data from an experiment that exposed batches of beetles to carbon disulphide.
```{r, message = FALSE}
library(trtools) 
library(ggplot2)
library(ggrepel) # used for geom_label_repel (see below)

bliss$proportion <- paste(bliss$dead, "/", bliss$exposed, sep = "")
bliss

p <- ggplot(bliss, aes(x = concentration, y = dead/exposed)) +
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) + 
  labs(x = "Concentration of Carbon Disulphide (mg/liter)",
    y = "Proportion of Beetles Dying")
plot(p)
```
The interest here is in modeling the *proportion* of dead beetles as a response variable.

A proportion $Y_i$ can be defined as $Y_i = C_i/m_i$ where $C_i$ is a count and $m_i$ is a total so that $C_i = 0, 1, \dots, m_i$ and $Y_i = 0, 1/m_i, 2/m_i, \dots, 1$. Note that proportions are not quite the same as rates. Proportions are bounded between zero and one, but rates are only bounded below by zero. 

1. Proportions may require nonlinear models because $0 \le E(Y_i) \le 1$.

2. Proportions tend to exhibit heteroscedasticty with variance depending on $E(Y_i)$ and $m_i$. The variance of $Y_i$ tends to be smaller as $E(Y_i)$ gets closer to zero or one, and is inversely proportional to $m_i$. 

3. Non-normal discrete distribution.

## The Binomial Distribution

Assume $m$ independent "trials" with a probability of a "success" on each trial of $p$ (and thus the probability of a "failure" is $1-p$). The number of successes then has a *binomial distribution* such that
$$
  P(C = c) = \binom{m}{c}p^c(1-p)^{m-c}
$$
where
$$
  \binom{m}{c} = \frac{m!}{c!(m-c)!}.
$$
The possible values of $C$ are $0, 1, \dots, m$. Note that $\binom{m}{c}$ is the number of outcomes where we can have a count of $c$ out of $m$, and $p^c(1-p)^{m-c}$ is the probability of each of these outcomes.

**Example**: Suppose that the probability of observing a seed germinate under certain conditions is 0.2, and we observe four seeds. Let $C$ be the number of seeds that germinate. Then $m$ = 4 and $p$ = 0.2. The probability that, say, $C$ = 3 is then
$$
  P(C = 3) = \underbrace{\frac{4!}{3!(4-3)!}}_{4}\underbrace{0.2^3(1-0.2)^{4-3}}_{0.0064} = 0.0246.
$$
There are four outcomes that give three successes, and each of these outcomes has a probability of 0.0064.
```{r, echo = FALSE}
Outcome <- c("SSSF","SSFS","SFSS","FSSS")
Probability <- c("$0.2 \\times 0.2 \\times 0.2 \\times 0.8$","$0.2 \\times 0.2 \\times 0.2 \\times 0.8$","$0.2 \\times 0.2 \\times 0.2 \\times 0.8$","$0.2 \\times 0.2 \\times 0.2 \\times 0.8$")
d <- data.frame(Outcome,Probability)
ktbl(d)
```
The proportion is obtained as $Y = C/m$. 

The figures below show several binomial distributions for different values of $m$ and $p$. 
```{r, fig.height = 6, echo = FALSE}
d <- expand.grid(y = rep(0:20, 9), p = c(0.2, 0.5, 0.8), m = c(5, 10, 20))
d$probability <- with(d, dbinom(y, m, p))
d$m <- with(d, paste("m =", m))
d$m <- factor(d$m, levels = c("m = 5", "m = 10", "m = 20"))
d$p <- with(d, paste("p =", p))
d <- subset(d, probability > 0)
p <- ggplot(d, aes(x = y, y = probability))
p <- p + facet_grid(p ~ m, scales = "free_x", space = "free_x")
p <- p + geom_point() + theme_minimal()
p <- p + geom_segment(aes(yend = 0, xend = y))
p <- p + labs(x = "Count", y = "Probability")
plot(p)
```
The figures below show the distributions of the proportion $C/m$. 
```{r, fig.height = 6, echo = FALSE}
d <- expand.grid(y = rep(0:20, 9), p = c(0.2, 0.5, 0.8), m = c(5, 10, 20))
d$probability <- with(d, dbinom(y, m, p))
d$y <- d$y/d$m
d$m <- with(d, paste("m =", m))
d$m <- factor(d$m, levels = c("m = 5", "m = 10", "m = 20"))
d$p <- with(d, paste("p =", p))
d <- subset(d, probability > 0)
p <- ggplot(d, aes(x = y, y = probability))
p <- p + facet_grid(p ~ m, scales = "free_x", space = "free_x")
p <- p + geom_point() + theme_minimal()
p <- p + geom_segment(aes(yend = 0, xend = y))
p <- p + labs(x = "Proportion", y = "Probability")
plot(p)
```
It can be shown that 
$$
  E(C) = mp \ \ \ \text{and} \ \ \ \text{Var}(C) = mp(1-p).
$$
Then for the *proportion* $Y = C/m$ it follows that 
$$
  E(Y) = p \ \ \ \text{and} \ \ \ \text{Var}(Y) = p(1-p)/m.
$$
This is because $E(Y) = E(C/m) = E(C)/m = mp/m = p$ and $\text{Var}(Y) = \text{Var}(C/m) = \text{Var}(C)/m^2 = mp(1-p)/m^2 = p(1-p)/m$. Note that the variance is at its maximum when $p$ = 0.5 and gets smaller as $p$ moves away from 0.5 toward $p$ = 0 or $p$ = 1. 
```{r, echo = FALSE}
d <- data.frame(x = seq(0, 1, length = 1000)) %>% 
  mutate(y = x*(1-x))
p <- ggplot(d, aes(x = x, y = y)) + theme_classic()
p <- p + geom_line() + labs(x = tex("$p$"), y = tex("$p(1-p)/m$"))
p <- p + scale_x_continuous(breaks = seq(0, 1, by = 0.1))
p <- p + scale_y_continuous(breaks = c(0, 0.25), labels = c(0, tex("$0.25/m$")))
plot(p)
```
An important special case of the binomial distribution is the *Bernoulli distribution* where $m = 1$ so that $C = 0,1$ and $Y = 0,1$. 

## Binomial Generalized Linear Models

Assume that each $C_1, C_2, \dots, C_n$ has a binomial distribution with parameters $p_1, p_2, \dots, p_n$ and $m_1, m_2, \dots, m_n$, respectively, but $m_1, m_2, \dots, m_n$ are observed/known). A binomial GLM will then specify the expected value of $Y_i = C_i/m_i$ as
$$
  g[E(Y_i)] = \eta_i \ \ \ \text{or} \ \ \ E(Y_i) = g^{-1}(\eta_i),
$$
where $\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik}$. 

Recall that $E(Y_i) = p_i$ so we are effectively specifying a model for the *probability of a success*. The variance of $Y_i$ is then
$$
  \text{Var}(Y_i) = E(Y_i)[1-E(Y_i)]/m_i = p_i(1-p_i)/m_i,
$$
so that $0 \le \text{Var}(Y_i) \le 0.25m_i$. Like rates, it is preferable to *not* model proportions as response variables without accounting for the denominator $m_i$ since it affects the variance. 

## Logistic Regression

Logistic regression is a binomial generalized linear model that uses a "logit" link function such that
$$
  g[E(Y_i)] = \log\left[\frac{E(Y_i)}{1-E(Y_i)}\right] = \log\left(\frac{p_i}{1-p_i}\right),
$$
and therefore
$$
  E(Y_i) = \frac{e^{\eta_i}}{1+e^{\eta_i}} \ \ \ \text{or} \ \ \ p_i = \frac{e^{\eta_i}}{1+e^{\eta_i}},
$$
where again $\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik}$. Note that this guarantees that $0 < E(Y_i) < 1$. 

**Example**: Consider again the `bliss` data. The `glm` function can be used to estimate the logistic regression model where
$$
  E(Y_i) = \frac{e^{\eta_i}}{1 + e^{\eta_i}},
$$
where $\eta_i = \beta_0 + \beta_1 x_i$ and $x_i$ is the concentration for the $i$-th observation (i.e., the $i$-th batch of beetles).
```{r, message = FALSE}
m <- glm(cbind(dead, exposed - dead) ~  concentration, 
  family = binomial(link = logit), data = bliss)
cbind(summary(m)$coefficients, confint(m))
```
Here the two variables in `cbind` are *the number of times the event occurred* (i.e., $C_i$) and *the number of times the event did not occur* (i.e., $m_i-C_i$). If the variables had been `dead` and `alive`, representing the number of dead and alive beetles, respectively, then we'd write `cbind(dead, alive)`. Also for `family = binomial` the logit link function is the default so you can use `family = binomial` for logistic regression.
```{r}
d <- data.frame(concentration = seq(49, 77, length = 1000))
d$yhat <- predict(m, newdata = d, type = "response")

p <- ggplot(bliss, aes(x = concentration, y = dead/exposed)) + 
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_line(aes(y = yhat), data = d) + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) +
  labs(x = "Concentration of Carbon Disulphide (mg/liter)",
    y = "Proportion of Beetles Dying")
plot(p)
```
Predicted probabilities, with confidence intervals, can also be obtained using `contrast` or `glmint`. Note that the function $e^x/(1+e^x)$ is known to R as `plogis`. 
```{r}
trtools::contrast(m, list(concentration = c(50,60,70)), 
  cnames = c("50 mg/liter","60 mg/liter","70 mg/liter"), tf = plogis)
trtools::glmint(m, newdata = data.frame(concentration = c(50,60,70)))
d <- data.frame(concentration = seq(49, 77, length = 1000))
d <- cbind(d, trtools::glmint(m, newdata = d))
head(d)
p <- ggplot(bliss, aes(x = concentration, y = dead/exposed)) + 
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_line(aes(y = fit), data = d) + 
  geom_line(aes(y = low), data = d, color = grey(0.75)) +
  geom_line(aes(y = upp), data = d, color = grey(0.75)) +
  geom_label_repel(aes(label = proportion), box.padding = 0.75) +
  labs(x = "Concentration of Carbon Disulphide (mg/liter)",
    y = "Proportion of Beetles Dying")
plot(p)
```

## Parameter and Contrast Interpretation: Odds Ratios

A logistic regression model can be written as
$$
  \frac{p_i}{1-p_i} = \exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik})
$$
where $p_i/(1-p_i)$ is the *odds* of the event. The *odds* is simply the ratio of the probability of the event occurring ($p_i$) to the probability of the event not occurring ($1-p_i$).

Odds are sometimes stated in "fractional form" as two numbers separated by a colon or other character (e.g., an odds of 1.5 might be written as "3:2" or "three to two"). Note that in its fractional form the odds $a:b$ implies a probability of $a/(a+b)$. 
```{r, echo = FALSE}
p <- sort(c(0.01, 0.1, 0.25, 1/3, 0.4, 0.5, 0.6, 2/3, 0.75, 0.9, 0.99))
o <- p/(1 - p)
z <- as.character(MASS::fractions(p/(1-p)))
for (i in 1:length(z)) {
   if (grepl("/", z[i])) {
      z[i] <- paste(strsplit(z[i], "/")[[1]], collapse = ":")
   } else {
      z[i] <- paste(z[i], ":1", sep = "")   
   }
}
prb <- rep(NA, length(p))
for (i in 1:length(z)) {
   if (round(p[i],2) != p[i]) {
      prb[i] <- as.character(MASS::fractions(p[i]))
   } else {
      prb[i] <- as.character(p[i])
   }
}

d <- data.frame(Probability = prb, Numeric = round(o,2), Fractional = z)
ktbl(d) %>% add_header_above(c(" " = 1, "Odds" = 2))
```

It is important to note that probabilities and odds are related but not equal.
```{r, echo = FALSE, fig.height = 6}
par(mai = c(1, 1, 0.25, 0.25))
x <- seq(0, 0.8, length = 100)
y <- x/(1-x)
prob <- c(0, 0.25, 0.5, 0.75)
odds <- prob/(1-prob)
plot(x, y, type = "l", bty = "L", xaxt = "n", yaxt = "n", xlab = "Probability", ylab = "Odds")
axis(1, prob)
axis(2, odds, MASS::fractions(odds))
for (i in 1:length(prob)) {
  lines(c(prob[i], prob[i]), c(0, odds[i]), lty = 3)
  lines(c(0, prob[i]), c(odds[i], odds[i]), lty = 3)
}
```
Let $O_i$ be the odds for the $i$-th observation. Then $O_i = p_i/(1-p_i)$ and $p_i = O_i/(1 + O_i)$. Note that $0 \le p_i \le 1$ but $0 \le O_i \le \infty$. 

We can write a logistic regression model in terms of the *odds* of an event as
$$
    O_i = \exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik}),
$$
or 
$$
    O_i = e^{\beta_0}e^{\beta_1x_{i1}}e^{\beta_2x_{i2}}\cdots e^{\beta_kx_{ik}}.
$$
Here we can use `contrast` to make inferences about the *odds* of death.
```{r}
trtools::contrast(m, list(concentration = c(50,60,70)), 
  cnames = c("50 mg/liter","60 mg/liter","70 mg/liter"), tf = exp)
```
We can even plot the estimated odds of death.
```{r}
d <- data.frame(concentration = seq(49, 77, length = 1000))
d$yhat <- predict(m, newdata = d, type = "response")
d$odds <- d$yhat / (1 - d$yhat)

p <- ggplot(d, aes(x = concentration, y = odds)) + 
  geom_line() + theme_minimal() + 
  labs(x = "Concentration of Carbon Disulphide (mg/liter)",
    y = "Odds of Beetles Dying")
plot(p)
```
The model for the odds is "log-linear" like the model for expected counts in Poisson regression. To interpret the parameters of a logistic regression model, we can use *odds ratios* which are similar to rate ratios in Poisson regression.  

### Odds Ratio: Quantitative Explanatory Variable

Suppose we have the logistic regression model
$$
  O_i = \exp(\beta_0 + \beta_1 x) = e^{\beta_0}e^{\beta_1x},
$$
were $x_i$ is a quantitative explanatory variable. Consider the odds at $x$ and $x+1$ for arbitrary $x$, 
$$
  O_a = e^{\beta_0}e^{\beta_1(x+1)} \ \ \ \text{and} \ \ \ O_b = e^{\beta_0}e^{\beta_1x}.
$$
Then the *odds ratio* is 
$$
  \frac{O_a}{O_b} = \frac{e^{\beta_0}e^{\beta_1(x+1)}}{e^{\beta_0}e^{\beta_1x}} = 
  \frac{e^{\beta_0}e^{\beta_1x}e^{\beta_1}}{e^{\beta_0}e^{\beta_1x}} = e^{\beta_1} 
  \Leftrightarrow O_a = O_be^{\beta_1},
$$
so that an increase $x$ by one unit changes the odds by a factor of $e^{\beta_1}$. Also, we can compute the percent change in the odds as 
$$
  100\% \times [O_a/O_b - 1],
$$
where $O_a/O_b = e^{\beta_1}$ is the odds ratio. Again, the sign tells us if this is a percent increase or decrease in the odds.

**Example**: Consider again the model for the `bliss` data.
```{r, message = FALSE}
cbind(summary(m)$coefficients, confint(m))
exp(cbind(coef(m), confint(m)))
trtools::contrast(m, tf = exp,
  a = list(concentration = 2), 
  b = list(concentration = 1))
```
An odds ratio is then simply the ratio of the odds at two different values of an explanatory variable. We could compute the odds ratio, for example, for an increase of 1, 5, 10, and 20 mg/liter. 
```{r}
trtools::contrast(m, tf = exp,
  a = list(concentration = c(1,5,10,20)),
  b = list(concentration = 0),
  cnames = c("+1 mg/liter", "+5 mg/liter", "+10 mg/liter", "+20 mg/liter"))
```
Suppose that we model instead the probability of *survival* rather than death. 
```{r, message = FALSE}
m <- glm(cbind(exposed - dead, dead) ~ concentration, 
  family = binomial, data = bliss)
cbind(summary(m)$coefficients, confint(m))
exp(cbind(coef(m), confint(m)))
trtools::contrast(m, tf = exp,
  a = list(concentration = 2), 
  b = list(concentration = 1))
```
Note the "symmetry" of logistic regression. Whether we model the probability of the event or its complement is just a matter of parameterization. 

### Odds Ratio: Categorical Explanatory Variable

Suppose we have the model
$$
  O_i = \exp(\beta_0 + \beta_1 x) = e^{\beta_0}e^{\beta_1x},
$$
were $x$ is an indicator variable so that
$$
  x = 
  \begin{cases}
    1, & \text{if the observation is from group $a$}, \\
    0, & \text{if the observation is from group $b$}, 
  \end{cases}
$$
so that the model can be written as
$$
  O_i = 
  \begin{cases}
    e^{\beta_0}e^{\beta_1}, & \text{if the observation is from group $a$}, \\
    e^{\beta_0},            & \text{if the observation is from group $b$}.
  \end{cases}
$$
So we can write the odds as 
$$
  O_a = e^{\beta_0}e^{\beta_1} \ \ \ \text{and} \ \ \ O_b = e^{\beta_0}.
$$
The *odds ratio* is then 
$$
 \frac{O_a}{O_b} = \frac{e^{\beta_0}e^{\beta_1}}{e^{\beta_0}} = e^{\beta_1} \ \ \ \text{or} \ \ \
 \frac{O_b}{O_a} = \frac{e^{\beta_0}}{e^{\beta_0}e^{\beta_1}} = \frac{1}{e^{\beta_1}} = e^{-\beta_1}.
$$
So the odds for group $a$ is $e^{\beta_1}$ times that for group $b$, and the odds for group $b$ is $e^{-\beta_1} = 1/e^{\beta_1}$ times that for group $a$. We can compute how much larger (or smaller) $O_a$ is relative to $O_b$ with
$$
  100\% \times [O_a/O_b - 1],
$$
where $O_a/O_b = e^{\beta_1}$ is the odds ratio. The sign tells us if $O_a$ is a percent larger or smaller than $O_b$. 

**Example**: Consider the following data from a study that investigated the effect of non-indigenous brook trout on the survival of salmon.
```{r, message = FALSE, fig.height = 2}
library(abd) # for BrookTrout data
p <- ggplot(BrookTrout, aes(x = trout, y = salmon.survived/salmon.released)) + 
  geom_point() + ylim(0, 0.5) + coord_flip() + theme_minimal() +
  labs(x = "Presence/Absence of\n Brook Trout",
    y = "Proportion of Released Salmon Surviving")
plot(p)
m <- glm(cbind(salmon.survived, salmon.released - salmon.survived) ~ trout,
  data = BrookTrout, family = binomial)
cbind(summary(m)$coefficients, confint(m))
exp(cbind(coef(m), confint(m)))
trtools::contrast(m, a = list(trout = "present"), b = list(trout = "absent"), tf = exp)
trtools::contrast(m, a = list(trout = "absent"), b = list(trout = "present"), tf = exp)
```
Recall that estimated probabilities can be computed using `contrast` with `tf = plogis`.
```{r}
trtools::contrast(m, a = list(trout = c("present","absent")), 
    tf = plogis, cnames = c("prob @ present","prob @ absent"))
```
Similarly the estimated odds can be computed if `tf = exp`.
```{r}
trtools::contrast(m, a = list(trout = c("present","absent")), 
    tf = exp, cnames = c("odds @ present","odds @ absent"))
```
The odds ratios are then simply a ratio of these odds.

**Example**: Consider the following study of the germination of five varieties of soybean seeds. Note that each observation was the number of seeds that *failed* to germinate out of 100 seeds.
```{r, message = FALSE}
head(faraway::soybean, 10)
p <- ggplot(faraway::soybean, aes(x = variety, y = (100-failure)/100)) +
  geom_jitter(height = 0, width = 0.1) + theme_minimal() + 
  labs(x = "Soybean Variety", y = "Proportion Germinated")
plot(p)
m <- glm(cbind(100 - failure, failure) ~ variety, family = binomial, data = faraway::soybean)
exp(cbind(coef(m), confint(m)))
# compute odds ratio of germination for arasan, fermate, semesan, and spergon versus check
trtools::contrast(m, tf = exp,
  a = list(variety = c("arasan","fermate","semesan","spergon")),
  b = list(variety = "check"),
  cnames = c("arasan/check","fermate/check","semesan/check","spergon/check"))
```

## Aggregated Versus Binary Responses

Suppose the observations in the `bliss` data were for individual beetles.
```{r}
blissbin <- bliss %>% mutate(alive = exposed - dead) %>% 
  select(concentration, dead, alive) %>% 
  pivot_longer(cols = c(dead,alive), names_to = "state", values_to = "count") %>%
  uncount(count)
head(blissbin)
```
We can specify the response variable as follows.
```{r}
m <- glm(state == "dead" ~ concentration, family = binomial, data = blissbin)
summary(m)$coefficients
```
Of if the response variable is binary we can specify the model as follows.
```{r}
blissbin <- blissbin %>% mutate(y = ifelse(state == "dead", 1, 0))
m <- glm(y ~ concentration, family = binomial, data = blissbin)
summary(m)$coefficients
m <- glm(cbind(y, 1-y) ~ concentration, family = binomial, data = blissbin)
summary(m)$coefficients
```
Note that our parameter estimates and other inferences are the same as what we obtained with the aggregated data.
```{r}
head(bliss)
m <- glm(cbind(dead, exposed - dead) ~ concentration, 
  family = binomial, data = bliss)
```
It is usually not necessary to transform aggregate data into binary data, but it is sometimes useful to transform binary data into aggregate data. Here is how that can be done. Note that any explanatory variables (separated by commas) are listed in `group_by` and the response variable is listed in `count`.
```{r}
blissagg <- blissbin %>% group_by(concentration) %>% count(state) %>% 
  pivot_wider(names_from = state, values_from = n, values_fill = 0)
blissagg
m <- glm(cbind(dead, alive) ~ concentration, family = binomial, data = blissagg)
summary(m)$coefficients
```