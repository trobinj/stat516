---
output:
  html_document:
    theme: readable
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "02-23-2022"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
  - \usepackage{array}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", message = FALSE, out.width = "100%", fig.align = "center", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

```{r packages, echo = FALSE}
library(ggplot2)
library(trtools)
library(dplyr)
library(tidyr)
```

```{r utilities, echo = FALSE}
source("../../utilities.R")
```

```{r options, echo = FALSE}
options(digits = 4, width = 90)
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`

## Iteratively Weighted Least Squares 

*Iteratively weighted least squares* can be used when we assume that the variance is proportional to a function of the mean so that
$$
  \text{Var}(Y_i) \propto h[E(Y_i)],
$$
where $h$ is some specified function, implying that our weights should be 
$$
  w_i = \frac{1}{h[E(Y_i)]}.
$$
Because $E(Y_i)$ is unknown we can use the estimate $\hat{y}_i$ to obtain weights
$$
  w_i = \frac{1}{h(\hat{y}_i)}.
$$  
Because $\hat{y}_i$ depends on the weights used in the weighted least squares algorithm, and $w_i$ depends on $\hat{y}_i$, we can use the following algorithm known as *iteratively weighted least squares*.

1. Estimate the model using *ordinary least squares* where all $w_i$ = 1. 

0. Compute weights as $w_i = 1/h(\hat{y}_i)$.

0. Estimate the model using *weighted least squares* with the weights $w_i = 1/h(\hat{y}_i)$.

The second and third steps can be repeated until the estimates and thus the weights stop changing. Typically only a few iterations are necessary.

**Example**: Consider again following data from a study on the effects of fuel reduction on biomass.
```{r}
library(trtools) # for biomass data

m.ols <- lm(suitable ~ -1 + treatment:total, data = biomass)
summary(m.ols)$coefficients

d <- expand.grid(treatment = c("n","y"), total = seq(0, 2767, length = 10))
d$yhat <- predict(m.ols, newdata = d)

p <- ggplot(biomass, aes(x = total, y = suitable, color = treatment)) + 
  geom_point() + geom_line(aes(y = yhat), data = d) + theme_minimal() + 
  labs(x = "Total Biomass (kg/ha)", y = "Suitable Biomass (kg/ha)",
    color = "Treatment")
plot(p)

biomass$yhat <- predict(m.ols)
biomass$rest <- rstudent(m.ols)

p <- ggplot(biomass, aes(x = yhat, y = rest, color = treatment)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Predicted Value", y = "Studentized Residual", 
    color = "Treatment")
plot(p)
```
Assume that $\text{Var}(Y_i) \propto E(Y_i)$, which means the weights should be $w_i = 1/E(Y_i)$. We can program the iteratively weighted least squares algorithm as follows.
```{r}
biomass$w <- 1 # initial weights are all equal to one
for (i in 1:5) {
  m.wls <- lm(suitable ~ -1 + treatment:total, weights = w, data = biomass)
  print(coef(m.wls)) # optional
  biomass$w <- 1 / predict(m.wls)
}
```
Now let's take a look at the residuals. 
```{r}
biomass$yhat <- predict(m.wls)
biomass$rest <- rstudent(m.wls)

p <- ggplot(biomass, aes(x = yhat, y = rest, color = treatment)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Predicted Value", y = "Studentized Residual", 
    color = "Treatment")
plot(p)
```
That may not be quite enough. Suppose we assume that $\text{Var}(Y_i) \propto E(Y_i)^p$ where $p$ = 2.
```{r}
biomass$w <- 1 # initial weights are all equal to one
for (i in 1:5) {
  m.wls <- lm(suitable ~ -1 + treatment:total, weights = w, data = biomass)
  biomass$w <- 1 / predict(m.wls)^2
}
```
Now let's take a look at the residuals. 
```{r}
biomass$yhat <- predict(m.wls)
biomass$rest <- rstudent(m.wls)

p <- ggplot(biomass, aes(x = yhat, y = rest, color = treatment)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Predicted Value", y = "Studentized Residual", 
    color = "Treatment")
plot(p)
```
Better. Maybe too much? We could try $p$ = 1.5 or something like that. The residuals do get a little strange for higher predicted values, but we'll leave it here.

The model is $E(S_i) = \beta_1n_it_i + \beta_2y_it_i$, where $n_i$ and $y_i$ are indicator variables for if the $i$-th plot was treated or not by fuel reduction. We can also write the model as
$$
  E(S_i) = 
  \begin{cases}
    \beta_1t_i, & \text{if the $i$-th plot was not treated by fuel reduction,} \\
    \beta_2t_i, & \text{if the $i$-th plot was treated by fuel reduction}.
  \end{cases}
$$
We can use $\beta_2-\beta_1$ for inferences about the treatment effect. 
```{r}
lincon(m.ols, a = c(-1,1)) 
lincon(m.wls, a = c(-1,1))
```
The `contrast` function from the **trtools** package can also do this. It can make inferences for a *difference of differences*.
```{r}
contrast(m.wls, 
  a = list(treatment = "y", total = 1),
  b = list(treatment = "y", total = 0),
  u = list(treatment = "n", total = 1),
  v = list(treatment = "n", total = 0))
```
This estimates $E(Y_a) - E(Y_b) - [E(Y_u) - E(Y_v)]$. This can also be done using the `emtrends` function from the **emmeans** package.
```{r}
library(emmeans)
emtrends(m.wls, ~treatment, var = "total") # estimate slopes
pairs(emtrends(m.wls, ~ treatment, var = "total")) # estimate difference between slopes
```
Recall that both the **emmeans** and **trtools** packages have a `contrast` function. To avoid conflicts or having to use `trtools::contrast` to call te `contrast` function from the **trtools** package later, we can unload the **emmeans** package usin `detach`.
```{r}
detach(package:emmeans)
```
Yet another approach to compare the slopes is to change the parameterization. Consider the following model.
```{r}
m.wls <- lm(suitable ~ -1 + total + total:treatment, weights = w, data = biomass)
summary(m.wls)$coefficients
```
From `summary` we can see that this model can be written as
$$
  E(S_i) = \beta_1t_i + \beta_2t_in_i,
$$

where $n_i$ is an indicator variable where $n_i$ = 1 if the treatment was not appiled to the $i$-th plot, add $n_i$ = 0 otherwise, so we can also write the model as
$$
  E(S_i) = 
  \begin{cases}
  (\beta_1 + \beta_2)t_i, & \text{if the $i$-th plot was not treated by fuel reduction}, \\
  \beta_1t_i, & \text{if the $i$-th plot was treated by fuel reduction}.
  \end{cases}
$$
Note that the meaning of $\beta_1$ and $\beta_2$ have changed here. The slopes of the lines with and without treatment are $\beta_1$ and $\beta_1 + \beta_2$, respectively, and the difference between the slopes is $\beta_1 - (\beta_1 + \beta_2) = -\beta_2$. So inferences for $\beta_2$ are for the difference in the slopes (after we reverse the sign). Although not necessary, we can change the reference category to avoid having to reverse the sign.
```{r}
biomass$treatment <- relevel(biomass$treatment, ref = "y")
m.wls <- lm(suitable ~ -1 + total + total:treatment, weights = w, data = biomass)
summary(m.wls)$coefficients
```
Now the model can be written as
$$
  E(S_i) = \beta_1t_i + \beta_2t_in_i,
$$
or
$$
  E(S_i) = 
  \begin{cases}
  \beta_1t_i, & \text{if the $i$-th plot was not treated by fuel reduction}, \\
  (\beta_1+\beta_2)t_i, & \text{if the $i$-th plot was treated by fuel reduction}.
  \end{cases}
$$
Note: For some reason the reference category (`y`) is getting an indicator variable here, where normally it does not. I am not sure if this is a bug or intentional, but it appears to be due to the somewhat unusual parameterization I am using.

## Parametric Models for Heteroscedasticity

**Example**: Consider the following data where variability appears to vary by treatment.
```{r, warning = FALSE}
library(trtools) # for pulse data
p <- ggplot(pulse, aes(x = pulse1, y = pulse2, color = treatment)) +
  geom_point() + theme_minimal() + 
  labs(x = "Pulse Before", y = "Pulse After", color = "Treatment") +
  theme(legend.position = c(0.85,0.2))
plot(p)
```
There is one case with missing values on `pulse1` and `pulse2`.
```{r}
subset(pulse, !complete.cases(pulse)) # show observations with missing data
```
This will cause problems so we are going to remove it.
```{r}
pulse <- subset(pulse, complete.cases(pulse)) # overwrite pulse with only complete cases
```
Let's consider a simple linear model.
```{r}
m <- lm(pulse2 ~ treatment + pulse1 + treatment:pulse1, data = pulse)
summary(m)$coefficients
pulse$yhat <- predict(m)
pulse$rest <- rstudent(m)
p <- ggplot(pulse, aes(x = yhat, y = rest, color = treatment)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Predicted Value", y = "Studentized Residual", 
    color = "Treatment") + 
  theme(legend.position = c(0.8,0.2))
plot(p)
```
Consider that the model assumed by `lm` is
\begin{align}
  E(Y_i) & = \beta_0 + \beta_1t_i + \beta_2x_i + \beta_3t_ix_i, \\
  \text{Var}(Y_i) & = \sigma^2,
\end{align}
where $Y_i$ is the second pulse measurement, $t_i$ is an indicator variable for the treatment (i.e., $t_i$ = 1 if the $i$-th observation was from the sitting treatment condition, and $t_i$ = 0 otherwise), and $x_i$ is the first pulse measurement. Maybe it would make sense to have something like
$$
  \text{Var}(Y_i) = 
  \begin{cases}
    \sigma^2_s, & \text{if the $i$-th observation is from the sitting treatment}, \\
    \sigma^2_r, & \text{if the $i$-th observation is from the running treatment}.
  \end{cases}
$$
We can estimate such a model using the `gls` function from the **nlme** package.
```{r}
library(nlme) # should come with R 
m <- gls(pulse2 ~ treatment + pulse1 + treatment:pulse1, data = pulse, 
  method = "ML", weights = varIdent(form = ~ 1|treatment))
summary(m)
```
Note the different syntax for extracting standardized residuals. 
```{r}
pulse$yhat <- predict(m)
pulse$resz <- residuals(m, type = "p") # note different syntax
p <- ggplot(pulse, aes(x = yhat, y = resz, color = treatment)) + 
  geom_point() + theme_minimal() + 
  labs(x = "Predicted Value", y = "Standardized Residual", 
    color = "Treatment")
plot(p)
```
Here is an example with the `CancerSurvival` data.
```{r}
library(Stat2Data)
data(CancerSurvival)
m <- gls(Survival ~ Organ, data = CancerSurvival, 
  method = "ML", weights = varIdent(form = ~ 1|Organ))
summary(m)
CancerSurvival$yhat <- predict(m)
CancerSurvival$resz <- residuals(m, type = "p") 
p <- ggplot(CancerSurvival, aes(x = yhat, y = resz, color = Organ)) +
  geom_point() + theme_minimal() + 
  labs(x = "Predicted Value", y = "Standardized Residual", color = "Organ")
plot(p)
```
Comments about parametric models for heteroscedasticity.

**Advantages**: Potentially very effective *if* we can specify an accurate model for the variance.

**Disadvantages**: If we do not specify an accurate model for the variance, it may bias estimation of parameters concerning the expected response. 

## Heteroscedastic Consistent Standard Errors

The idea is to estimate the model parameters using ordinary least squares, but estimate the standard errors in such a way that we do not assume heteroscedasticity. This is sometimes called *heteroscedastic consistent standard errors*, *robust standard errors*, or *sandwich estimators*.
  
**Example**: Consider again the cancer survival data.
```{r}
m <- lm(Survival ~ Organ, data = CancerSurvival)
```
The **sandwich** package provides resources for using heteroscedastic-consistent standard errors. Technically, what is being estimated is the *covariance matrix* of the parameter estimators. 
```{r}
library(sandwich) # for vcovHC used below
vcov(m)   # bad estimate if there is heteroscedasticity
vcovHC(m) # better estimate if there is heteroscedasticity
```
The square root of the diagonal elements are the standard errors.
```{r}
sqrt(diag(vcov(m)))   # bad estimates of the standard errors
sqrt(diag(vcovHC(m))) # better estimates of the standard errors
```
But the usual way to interface with the functions in the **sandwich** package is through other functions. 
```{r, echo = 2:5}
cbind(summary(m)$coefficients, confint(m)) # bad standard error estimates
confint(m) # bad confidence intervals due to bad standard error estimates
library(lmtest) # for coeftest and coefci used below
coeftest(m, vcov = vcovHC) # better standard error estimates
coefci(m, vcov = vcovHC)   # better confidence intervals
```
Both `lincon` and `contrast` will accept a `fcov` argument to provide a function to estimate standard errors.
```{r}
lincon(m, fcov = vcovHC) 
organs <- sort(unique(CancerSurvival$Organ)) # sorted organ names
contrast(m, a = list(Organ = organs),
  cnames = organs, fcov = vcovHC)
lincon(m, a = c(1,0,0,0,1), fcov = vcovHC)   
```
You can use a similar approach with the `emmeans` function from the **emmeans** package, but there the argument is `vcov`.
```{r}
library(emmeans)
emmeans(m, ~Organ, vcov = vcovHC)
pairs(emmeans(m, ~Organ, vcov = vcovHC), adjust = "none", infer = TRUE)
```
Use the function `waldtest` in place of `anova` when using heteroscedastic-consistent standard errors. 
```{r}
m.full <- lm(Survival ~ Organ, data = CancerSurvival)
m.null <- lm(Survival ~ 1, data = CancerSurvival)
waldtest(m.null, m.full, vcov = vcovHC)
```
Comments about heteroscedastic-consistent standard errors:

**Advantages**: Does not require us to specify a variance structure/function. We let the data inform the estimator. 

**Disadvantages**: Highly dependent on the data to help produce better estimates of the standard errors, and tends to work well only if $n$ is relatively large. 

Note: There are a variety of variations of the "sandwich" estimator. Different estimators can be specified through the `type` argument to `vcovHC` so instead of writing `vcov = vcovHC` or `fcov = vcovHC` we write `vcov = function(m) vcovHC(m, type = "HC0")` or `vcov = function(m) vcovHC(m, type = "HC0")` if we wanted to use that particular type of estimator (sometimes called "White's estimator").
