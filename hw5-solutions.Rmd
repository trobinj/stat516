---
title: "Categorical Response Variables, Marginal and Mixed Effect Models, and Review"
subtitle: Statistics 516, Homework 5 (Solutions)
output:
  html_document:
    theme: readable
  pdf_document: default
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{array}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", message = FALSE, out.width = "100%", fig.align = "center", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "tikz"))
```

```{r, echo = FALSE}
options(digits = 4, width = 90)
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](hw5-solutions.pdf) copy of this homework assignment.", sep = ""), "")`

## Dental Fissure Growth Data

The data frame `potthoffroy` in the **mice** package is from a study of the growth of the distance from the center of the pituitary gland to the pteryomaxillary fissure in boys and girls.[^potthoff] The pituitary-pteryomaxillary distance was measured four times at ages 8, 10, 12, and 14 in several boys and girls. These longitudinal data are stored in wide form.
```{r}
library(mice)
head(potthoffroy)
```
For plotting and modeling we need to put them into long form. This is done below. Also a quantitative age variable is created from the column labels of the wide form data. 
```{r}
library(dplyr)
library(tidyr)
dental <- potthoffroy %>% rename(subject = id) %>% 
  pivot_longer(cols = c(d8,d10,d12,d14), names_to = "obs", values_to = "distance") %>% 
  mutate(age = as.numeric(substr(obs, 2, nchar(obs)))) %>% dplyr::select(-obs)
head(dental)
```
The plot below shows the observations with line segments connecting the observations from the same subject. 
```{r}
library(ggplot2)
p <- ggplot(dental, aes(x = age, y = distance)) + theme_minimal() + 
  geom_line(aes(group = subject), alpha = 0.25) + geom_point() + 
  facet_wrap(~ sex) + labs(x = "Age (years)", y = "Distance (mm)")
plot(p)
```
These data were featured in the lecture on [fixed effects models](lecture-04-25-2022.html).[^dental] The fixed effects approach is not useful here for making inferences regarding sex, so you will consider some other approaches that are more useful. When using the `geeglm` and `lmer` functions you might find it useful that these data are very similar in structure to the `Sitka` data featured in lecture where we have `age` instead of `Time`, `sex` instead of `ozone`, and `subject` instead of `tree`. Note that unlike the `Sitka` data we do not need to transform the response variable or the time/age variable. For each of the problems below report the `summary` output so that I can verify that you specified and estimated the model correctly. 

1. Estimate a linear model using the `lm` function with distance as the response variable and age and `sex` as the explanatory variables, with an interaction. Note that this is arguably not an appropriate model here as it does not account for the lack of independence of observations from the same subject, but you will use it for comparison. Using either the `contrast` function or functions from the **emmeans** package, estimate (a) the difference in the expected distance between boys and girls at ages of 8, 10, 12, and 14, (b) the rate of change in the expected distance per year for boys and for girls, and (c) the difference in the rate of change in the expected distance per year between boys and girls. 

    **Solution**: We can estimate the model as follows.
    ```{r}
    m <- lm(distance ~ sex * age, data = dental)
    summary(m)$coefficients
    ```
    Here I will show how to make the various inferences based on this models using both the `contrast` function and the **emmeans** package, but in the later problems I will only use one or the other. Here are the comparisons between boys and girls at four ages. 
    ```{r}
    trtools::contrast(m,
      a = list(sex = "M", age = c(8,10,12,14)),
      b = list(sex = "F", age = c(8,10,12,14)),
      cnames = paste(c(8,10,12,14), "years"))
    ```
    This can also be done using the **emmeans** package. 
    ```{r}
    library(emmeans)
    pairs(emmeans(m, ~sex|age, at = list(age = c(8,10,12,14))), 
      infer = TRUE, reverse = TRUE)
    ```
    We can estimate the rate of change in the expected distance for boys and girls as follows.
    ```{r}
    trtools::contrast(m, 
      a = list(sex = c("F","M"), age = 9),
      b = list(sex = c("F","M"), age = 8),
      cnames = c("girls","boys"))
    emtrends(m, ~sex, var = "age", infer = TRUE)
    ```
    And here is the comparison of the rates of change between boys and girls.
    ```{r}
    trtools::contrast(m, 
      a = list(sex = "M", age = 9),
      b = list(sex = "M", age = 8),
      u = list(sex = "F", age = 9),
      v = list(sex = "F", age = 8))
    pairs(emtrends(m, ~sex, var = "age"), infer = TRUE, reverse = TRUE)
    ```

0. Estimate a *marginal* linear model using the `geeglm` function from the **geepack** package with `distance` as the response variable and age and sex as the explanatory variables, with an interaction. Note that subject is your `id` variable and your correlation structure should be specified as exchangeable as was done in lecture. Using this model make the same inferences that you did in the previous problem using the `contrast` function or the **emmeans** package. Note that the syntax for these inferences should be the same as what you used in the previous problem. 

    **Solution**: We can estimate the model as follows.
    ```{r}
    library(geepack)
    m <- geeglm(distance ~ sex * age, data = dental,
      family = gaussian(link = identity), 
      id = subject, corstr = "exchangeable")
    summary(m)
    ```
    Here are the comparisons between boys and girls at four ages. 
    ```{r}
    trtools::contrast(m,
      a = list(sex = "M", age = c(8,10,12,14)),
      b = list(sex = "F", age = c(8,10,12,14)),
      cnames = paste(c(8,10,12,14), "years"))
    ```
    We can estimate the rate of change in the expected distance for boys and girls as follows.
    ```{r}
    emtrends(m, ~sex, var = "age", infer = TRUE)
    ```
    And here is the comparison of the rates of change between boys and girls.
    ```{r}
    pairs(emtrends(m, ~sex, var = "age"), infer = TRUE, reverse = TRUE)
    ```

0. Estimate a linear *mixed effects* model using the `lmer` function from the **lme4** package with distance as the response variable and age and `sex` as the explanatory variables, with an interaction. Specify a random "main effect" for subject. Using this model make the same inferences that you did in the previous problems using the `contrast` function or the **emmeans** package. Note that the syntax for these inferences should be the same as what you used in the previous problem.

    **Solution**: We can estimate the model as follows.
    ```{r}
    library(lme4)
    m <- lmer(distance ~ sex * age + (1 | subject), data = dental)
    summary(m)
    ```
    Here are the comparisons between boys and girls at four ages. 
    ```{r}
    trtools::contrast(m,
      a = list(sex = "M", age = c(8,10,12,14)),
      b = list(sex = "F", age = c(8,10,12,14)),
      cnames = paste(c(8,10,12,14), "years"))
    ```
    We can estimate the rate of change in the expected distance for boys and girls as follows.
    ```{r}
    emtrends(m, ~sex, var = "age", infer = TRUE)
    ```
    And here is the comparison of the rates of change between boys and girls.
    ```{r}
    pairs(emtrends(m, ~sex, var = "age"), infer = TRUE, reverse = TRUE)
    ```

0. Compare the inferences you obtained in the three problems above, paying particular attention to the estimates and their standard errors. Discuss briefly the similarities and differences in the inferences for the three approaches. 

    **Solution**: When comparing boys and girls at different ages we do not see any real differences in the estimates. Using a marginal or mixed model produces somewhat higher standard errors at ages 10 and 12. When estimating the rates of change we get substantially smaller standard errors from the marginal and mixed models, although the point estimates are very similar. This is also the case when we compare the rates of change between boys and girls. For a study like this it could be important to account for the effect of subject in your inferences by using either the marginal or mixed model. Failing to account for such effects as in the first model estimating using `lm` can lead to biased standard errors. 

[^potthoff]: Potthoff, R. F., & Roy, S. N. (1964). A generalized multivariate analysis of variance model useful especially for growth curve problems. *Biometrika*, *51(3)*, 313--326.

[^dental]: These data have appeared in many articles and books, and are included in several R packages. In lecture I used the data frame form the **heavy** package, but this package requires compilation of some C code which may not be able to do if you do not have a compiler installed. 

## Swedish Speed Limit Study --- Revisited, Again

Once again consider the data from the [third](hw3-solutions.html) and [fourth](hw4-solutions.html) homework assignments (review the study description given in the third homework assignment). 
```{r}
library(SMPracticals)
library(dplyr)
library(tidyr)
limitstudy <- limits %>% 
  rename(limit_1961 = lim1, limit_1962 = lim2, y_1961 = y1, y_1962 = y2) %>%
  pivot_longer(cols = -day, names_to = c(".value", "year"), names_sep = "_") %>%
  mutate(limit = factor(limit, levels = c(0,1), labels = c("no","yes")))
head(limitstudy)
```
Here is a plot of the number of accidents for each year and day. 
```{r}
library(ggplot2)
p <- ggplot(limitstudy, aes(x = day, y = y, color = limit)) + 
  theme_minimal() + geom_point() + facet_grid(year ~ .) + 
  scale_x_discrete(breaks = seq(1, 92, by = 7)) + 
  labs(x = "Day", y = "Number of Accidents", color = "Speed\nLimit")
plot(p)
```
As I discussed in the third homework assignment, there may be an effect of the factor `day`. This is because the days are matched in the sense that a given day in 1961 is the same day of the week and month as the day with the same level in 1962 (e.g., if a given day is a Sunday in 1961 it is also a Sunday in 1962). The accident rate may vary by day due to differences in, for example, traffic (e.g., work days versus weekends and holidays). Plotting the residuals from a Poisson regression model shows some evidence of an association between days that is not captured by the model.
```{r}
m <- glm(y ~ limit + year, family = poisson, data = limitstudy)
limitstudy$residuals <- rstudent(m)

d <- limitstudy %>% dplyr::select(day, year, residuals) %>% 
  pivot_wider(names_from = year, values_from = residuals)
p <- ggplot(d, aes(x = `1961`, y = `1962`)) + theme_minimal() + 
  geom_count() + scale_size_continuous(breaks = 1:2, range = c(1,2.5))
plot(p)
```
Here you will extend your Poisson regression model to account for the lack of independence of observations made on the same day in 1961 and in 1962. For each model report the output of `summary` so that I can verify that you specified and estimated the model correctly. 

1. Estimate a *marginal* Poisson regression model using the `geeglm` function with the number of accidents as your response variables, and limit and year as explanatory variable (with no interaction). Note that day will be your `id` variable and you should specify an exchangeable correlation structure. 

    **Solution**: We can estimate the model as follows.
    ```{r}
    library(geepack)
    m.gee <- geeglm(y ~ limit + year, data = limitstudy,
      family = poisson, id = day, corstr = "exchangeable")
    summary(m.gee)
    ```
    
0. Estimate a *mixed effects* Poisson regression model using the `glmer` function with the number of accidents as your response variables, and limit and year as explanatory variable (with no interaction). Specify your model with a random main effect for day. 

    **Solution**: We can estimate the model as follows.
    ```{r}
    library(lme4)
    m.glmer <- glmer(y ~ limit + year + (1 | day), 
      family = poisson, data = limitstudy)
    summary(m.glmer)
    ```

0. For each of the two models you estimated above, use either the `contrast` function or functions from the **emmeans** package to estimate (a) the expected number of accidents with and without a posted speed limit each year and (b) the rate ratio describing the relationship between whether or not a speed limit was posted and the expected number of accidents (like you did in the previous two homework assignments). Compare your estimates and standard errors to what you got when you used a Poisson regression model in the third homework assignment (using `family = poisson`, not `family = quasipoisson`) in terms of the estimates and either standard errors or confidence intervals for the rate ratios. Briefly discuss similarities and differences between the three models. 

    **Solution**: For comparison I will also estimate the Poisson regression model here again.
    ```{r}
    m.glm <- glm(y ~ limit + year, family = poisson, data = limitstudy)
    ```
    Now consider the estimated expected number of accidents each year with and without a posted speed limit.
    ```{r}
    library(emmeans)
    emmeans(m.glm, ~limit*year, type = "response")
    emmeans(m.gee, ~limit*year, type = "response")
    emmeans(m.glmer, ~limit*year, type = "response")
    ```
    The estimates are similar but not identical across the three approaches. The standard errors are noticeably smaller when using the generalized linear model. We might be concerned about underestimating the standard errors by failing to account for the effect of day. Now consider the rate ratio for the effect of limit.
    ```{r}
    pairs(emmeans(m.glm, ~limit|year, type = "response"), infer = TRUE)
    pairs(emmeans(m.gee, ~limit|year, type = "response"), infer = TRUE)
    pairs(emmeans(m.glmer, ~limit|year, type = "response"), infer = TRUE)
    ```
    The generalized linear model produces a somewhat smaller estimate of the rate ratio than the marginal or mixed model. We may be underestimating the rate ratio by failing to account for the effect of day. The standard errors are fairly similar. 

## Audit Study of Discrimination in Hiring

The data frame `resume` in the **qss** package are from an [audit study](https://en.wikipedia.org/wiki/Audit_study) of hiring discrimination based on perceived gender and race.[^bertrand] Resumes were sent in response to job advertisements, and the researchers recorded whether or not the job application resulted in a call-back. The applicants were fictitious with randomly-assigned names that were selected so that they would be likely identified as white or African-American, and as female or male.[^resumenames] Use the command
```{r, eval = FALSE}
devtools::install_github("kosukeimai/qss-package", build_vignettes = TRUE)
```
to install the **qss** package. Note that you will need to have the **devtools** package installed to do this, but you should have it installed already since it is necessary to install **trtools**. 
```{r}
library(qss)
data(resume)
head(resume)
```
For plotting and modeling it is useful to aggregate the data to show the number of call backs and the total number of resumes for each combination of name, sex, and race. 
```{r}
library(dplyr)
resumeagg <- resume %>% group_by(firstname, sex, race) %>% 
  summarize(callbacks = sum(call), applications = n()) %>% 
  mutate(sexrace = interaction(sex, race, sep = " and "))
head(resumeagg)
```
Note the use of the `interaction` function here to create a new variable `sexrace` for every combination of the variables `sex` and `race`. This is used in the plot below. The plot shows the proportion of job applications that received a call-back for each name.[^freescales]
```{r}
library(ggplot2)
p <- ggplot(resumeagg, aes(x = firstname, y = callbacks/applications)) + 
  theme_minimal() + coord_flip() + geom_point() + 
  facet_grid(sexrace ~ ., scales = "free_y") + 
  labs(y = "Proportion of Applications Receiving Call-Back", x = NULL)
plot(p)
```
The focus here would be on the relationship between the probability (or odds) of a call-back as a function of the likely perceived race and gender of the applicant. But there may be an effect of the *name* as well. The name may have an effect because (a) some names may be more or less likely to be perceived as from an applicant of a certain race or gender, and (b) names may differ in terms of other perceptions such as social class. Here you will consider some methods for accounting for the effect of name while making inferences about the effect of (likely perceived) gender and race. Be sure to report the output of `summary` for each model so that I can verify that you specified and estimated the model correctly.

1. Estimate a logistic regression model using the `glm` function that you can use to obtain odds ratios for the effect of gender and race on the odds that an application will get a call-back. Include an interaction between gender and race. Note that this model may not be appropriate since it does not account for an effect of name, but it will be used for comparison.

    **Solution**: Here is how to estimate the logistic regression model.
    ```{r}
    m.glm <- glm(cbind(callbacks, applications - callbacks) ~ sex * race,
      family = binomial, data = resumeagg)
    summary(m.glm)$coefficients
    ```

0. Estimate a *marginal* logistic regression model using the `geeglm` function that you can use to obtain odds ratios for the effect of gender and race on the odds that an application will get a call-back. Include an interaction between gender and race. Use first name as your `id` variable and specify an exchangeable correlation structure. The data will first need to be sorted by `firstname` for using with the `geeglm` function. You can do this using 
    ```{r}
    resumeagg <- resumeagg %>% arrange(firstname)
    resumeagg
    ```
*before* estimating the model.

    **Solution**: This problem was dropped.

0. Estimate a *mixed effects* logistic regression model using the `glmer` function that you can use to obtain odds ratios for the effect of gender and race on the odds that an application will get a call-back. Include an interaction between gender and race. Specify a random main effect for first name in your model. Report the output of `summary` for this model.

    **Solution**: The model can be estimated as follows.
    ```{r}
    library(lme4)
    m.glmer <- glmer(cbind(callbacks, applications - callbacks) ~ sex * race + (1 | firstname),
      family = binomial, data = resumeagg)
    summary(m.glmer)
    ```
 
0. Use either the `contrast` function or functions from the **emmeans** package to estimate the odds ratios for the effects of gender and race for each of the three models you estimated. Note that since there is an interaction you should have two odds ratios for each explanatory variable (i.e., the odds ratios for the effect gender will be computed for each race, and the odds ratio for race will be computed for each gender). Focusing on the estimates and either the standard errors or confidence intervals for the odds ratios, briefly describe how these odds ratios compare across the three models. 

    **Solution**: Here are the odds ratios for the effect of gender for each model.
    ```{r}
    library(emmeans)
    pairs(emmeans(m.glm, ~sex|race, type = "response"), infer = TRUE)
    pairs(emmeans(m.glmer, ~ sex|race, type = "response"), infer = TRUE)
    ```
    And here are the odds ratios for the effect of race for each model.
    ```{r}
    pairs(emmeans(m.glm, ~race|sex, type = "response"), infer = TRUE)
    pairs(emmeans(m.glmer, ~ race|sex, type = "response"), infer = TRUE)
    ```
    Interestingly in this particular case the point estimates and standard errors for the odds ratios are fairly similar for the generalized linear and the generalized linear mixed models. 
    
[^freescales]: The `scales = "free_y"` argument lets the scale of the variable (here `firstname`) to be "free" for each facet. If it was omitted then every name would be listed for every value of `sexrace`, but that is not necessary here since only certain names go with each value of `sexrace` (i.e., the name factor is *nested* within the combinations of the sex and race factors). Try removing the argument and see what happens to the plot to better understand what it does. 

[^bertrand]: Bertrand, M. & Mullainathan, S. (2004). Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination. *American Economic Review*, *94*, 991--1013.

[^resumenames]: This study is actually a bit more complicated because the researchers also manipulated other characteristics of the resumes such as education, experience, awards, and other information that was included or omitted from the resume. Also the data include information about the jobs for which the "candidates" applied such as skill and experience requirements, and the type of industry. For the purpose of this exercise we will ignore these other variables. 

## A Sequential Model for Vaccine Efficacy

An [article](https://www.covid-datascience.com/post/israeli-data-how-can-efficacy-vs-severe-disease-be-strong-when-60-of-hospitalized-are-vaccinated) published on [covid-datascience.com](https://www.covid-datascience.com) discussed some misunderstandings about data on the efficacy of the COVID-19 vaccine in Israel.[^efficacy] One issue was that the apparent efficacy of the vaccine is underestimated if one fails to account for age. The reason is that older people were much more likely to be vaccinated, but older people are also more likely to develop a severe infection requiring hospitalization. Here the variable age is an example of what is sometimes called a *suppressor variable* because it reduces the apparent effect of another explanatory variable if it is not also included as an explanatory variable. For this problem I have created a fictional data set that demonstrates this phenomenon. 
```{r}
vaccine <- data.frame(
  age = c("<50","<50","50+","50+"),
  vaccine = c("n","y","n","y"),
  uninfected = c(7932, 9635, 800, 8516),
  infected = c(1817, 514, 193, 456),
  hospitalized = c(96, 6, 19, 15)
)
vaccine
```
These data are in aggregated form, showing the number of subjects for each combination of age (less than 50 or 50 or more years in age) and vaccination (yes or no) classified as uninfected, infected with COVID-19, and hospitalized due to COVID-19. Note that people who are hospitalized are also infected, so the infected category is only for people who are infected but not hospitalized. For each model you estimate below, report the output of `summary` so that I can verify that you specified and estimated the model correctly. 

1. Estimate *two* logistic regression models using the `glm` function: one model for the probability that someone will get COVID-19 (i.e., classified as `infected` or `hospitalized` versus `uninfected`) and another for the probability that someone who gets infected will be hospitalized (i.e., `hospitalized` versus `infected`). Use only whether or not someone was vaccinated as your explanatory variable. Note that since the data are in aggregate form you can specify the "response variable" in `glm` as `cbind(infected + hospitalized, uninfected)` for the first model, and `cbind(hospitalized, infected)` for the second model. Estimate the odds ratio for the effect of vaccine for each model, and write a sentence that summarizes how to interpret each odds ratio.

    **Solution**: Here are the two logistic regression models with odds ratios.
    ```{r}
    m1 <- glm(cbind(infected + hospitalized, uninfected) ~ vaccine,
      family = binomial, data = vaccine)
    summary(m1)$coefficients
    exp(cbind(coef(m1), confint(m1)))
    m2 <- glm(cbind(hospitalized, infected) ~ vaccine,
      family = binomial, data = vaccine)
    summary(m2)$coefficients
    exp(cbind(coef(m2), confint(m2)))
    ```
    From the estimated odds ratios we can conclude that (a) the odds that a vaccinated person will become infected is about 88% less than someone who is not vaccinated, and (b) the odds that an infected vaccinated person will require hospitalization is about 62% less than someone who is not vaccinated.

0. Estimate a *sequential* regression model using the `vglm` function from the **VGAM** package with the status as your three-category response variable (i.e., uninfected, infected, or hospitalized, in that order) and vaccine as your only explanatory variable. You should find that the parameter estimates correspond to what you obtained with the two logistic regression models in the previous problem.

    **Solution**: Here is how to estimate the sequential model.
    ```{r}
    library(VGAM)
    m <- vglm(cbind(uninfected, infected, hospitalized) ~ vaccine,
      family = cratio(link = "logitlink"), data = vaccine)
    summary(m)
    ```
    Note that the parameter estimates are the same as when using the two logistic regression models. 

0. Estimate the sequential regression model but now including age as a second explanatory variable (do not include an interaction between vaccine and age). Report and interpret in a sentence the odds ratios for the effect of vaccination on (a) infection, and (b) hospitalization of an infected person. 

    **Solution**: Here is how to estimate the sequential model, now including age as an explanatory variable.
    ```{r}
    m <- vglm(cbind(uninfected, infected, hospitalized) ~ vaccine + age,
      family = cratio(link = "logitlink"), data = vaccine)
    trtools::lincon(m) # more compact output
    trtools::lincon(m, tf = exp) # exponentiate for odds ratios
    ```
    Now controlling for age, we find that vaccination reduces the odds of infection by about 78% (about the same as when not accounting for age), and reduces the odds of hospitalization after infection by about 71% (a larger effect then when not accounting for age). The variable age was suppressing the effect of vaccination on the odds of hospitalization after infection.  

[^efficacy]: [Vaccine efficacy](https://en.wikipedia.org/wiki/Vaccine_efficacy) is an example of what we could a *marginal effect* in a binomial regression model. It is the (estimated) percent reduction in the probability of an infection from vaccination.
