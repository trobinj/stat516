<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Nonlinear Regression and Heteroscedasticity</title>

<script src="site_libs/header-attrs-2.13/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics 436/516</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="lectures.html">Lectures</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Nonlinear Regression and
Heteroscedasticity</h1>
<h3 class="subtitle">Statistics 516, Homework 2 (Solutions)</h3>

</div>


<p>You can also download a <a href="hw2-solutions.pdf">PDF</a> copy of
this homework assignment.</p>
<div id="modeling-the-potency-of-two-herbicides" class="section level2">
<h2>Modeling the Potency of Two Herbicides</h2>
<p>The data frame <code>S.alba</code> in the <strong>drc</strong>
package contains data from an experiment investigating the potency of
two herbicides, bentazone and glyphosate, for use with white mustard.<a
href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> The data
are shown in the plot below.</p>
<pre class="r"><code>library(ggplot2)
library(drc)
p &lt;- ggplot(S.alba, aes(x = Dose, y = DryMatter)) + theme_minimal() + 
  geom_count(alpha = 0.25) + facet_wrap(~ Herbicide) + 
  labs(x = &quot;Dose (g/ha)&quot;, y = &quot;Dry Matter (g/pot)&quot;) + 
  theme(legend.position = c(0.95, 0.8))
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" />
Note the use of <code>geom_count</code> here. It can be used instead of
<code>geom_point</code> to make the size of the points proportional to
the number of points at a given location. Pots of plants were randomly
assigned to receive a specified dose of one of the two herbicides. The
amount of dry matter from each pot was later measured.</p>
<p>Assume that the goal of this study is to assess how the dose of each
of the two herbicides affect dry matter. To do this a nonlinear
regression model can be used with dry matter as the response variable,
and dose and herbicide as the explanatory variables. This model will
have the form <span class="math display">\[
  E(M) = \gamma + \frac{\delta - \gamma}{1 + e^{\beta(\log d - \log
\alpha)}},
\]</span> where <span class="math inline">\(M\)</span> and <span
class="math inline">\(d\)</span> are dry matter and dose,
respectively.<a href="#fn2" class="footnote-ref"
id="fnref2"><sup>2</sup></a> The four parameters of this model (i.e.,
<span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span>, <span
class="math inline">\(\delta\)</span>, and <span
class="math inline">\(\gamma\)</span>) have useful interpretations in
terms of how the expected dry matter is related to dose. The parameter
<span class="math inline">\(\delta\)</span> is the expected dry matter
at zero dose, and <span class="math inline">\(\gamma\)</span> is the
asymptote of expected dry matter as dose increases. The parameter <span
class="math inline">\(\alpha\)</span> is the dose value where the
expected dry matter is half way between its maximum value of <span
class="math inline">\(\delta\)</span> and its minimum value of <span
class="math inline">\(\gamma\)</span> — i.e., when <span
class="math inline">\(E(M) = (\delta+\gamma)/2\)</span>.<a href="#fn3"
class="footnote-ref" id="fnref3"><sup>3</sup></a> The parameter <span
class="math inline">\(\beta\)</span> is related to “how quickly” the
expected dry matter decreases as dose increases when dose equals <span
class="math inline">\(\alpha\)</span>. Specifically, it can be shown
that the slope of a tangent line when dose equals <span
class="math inline">\(\alpha\)</span> is <span
class="math inline">\(-\beta(\delta-\gamma)/(4\alpha)\)</span>, so
everything else being equal as <span
class="math inline">\(\beta\)</span> increases the expected dry matter
decreases “more quickly” as dose increases.<a href="#fn4"
class="footnote-ref" id="fnref4"><sup>4</sup></a> The plot below shows
this model with <span class="math inline">\(\alpha\)</span> = 20, <span
class="math inline">\(\beta\)</span> = 5, <span
class="math inline">\(\delta\)</span> = 5, and <span
class="math inline">\(\gamma\)</span> = 2.
<img src="hw2-solutions_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" />
The <strong>drc</strong> package provides functions the help automate
the estimation of a variety of nonlinear regression models like this one
for dose-response relationships. But here you will consider how to use
the <code>nls</code> function to estimate this model. Being proficient
at using a function like <code>nls</code> is very useful because then
you are not limited to using only those models programmed by other
authors. A feature of the <strong>drc</strong> package is that it
provides “self-starter” features that find good starting values for you
automatically. But when using <code>nls</code> it is up to you to find
good starting values. Fortunately for this particular model this is not
too difficult. You can relatively easily “eyeball” reasonable starting
values for <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\delta\)</span>, and <span
class="math inline">\(\gamma\)</span> by looking at a plot of the data.
Finding a good starting value for <span
class="math inline">\(\beta\)</span> can be a bit trickier, but here is
one strategy that can be used. Suppose we compute the mean value of
<code>DryMatter</code> for each combination of <code>Herbicide</code>
and <code>Dose</code> as follows.</p>
<pre class="r"><code>library(dplyr)
S.alba %&gt;% group_by(Herbicide, Dose) %&gt;% 
  summarize(drymatter = mean(DryMatter))</code></pre>
<pre><code># A tibble: 15 x 3
# Groups:   Herbicide [2]
   Herbicide   Dose drymatter
   &lt;fct&gt;      &lt;int&gt;     &lt;dbl&gt;
 1 Bentazone      0     3.84 
 2 Bentazone     10     3.72 
 3 Bentazone     20     3.42 
 4 Bentazone     40     1.2  
 5 Bentazone     80     0.75 
 6 Bentazone    160     0.625
 7 Bentazone    320     0.7  
 8 Bentazone    640     0.675
 9 Glyphosate     0     3.96 
10 Glyphosate    10     3.68 
11 Glyphosate    20     3.72 
12 Glyphosate    40     3.22 
13 Glyphosate    80     1.85 
14 Glyphosate   160     1.15 
15 Glyphosate   320     0.9  </code></pre>
<p>We can actually plot these means and connect them with line segments
by “adding” <code>stat_summary</code> to the earlier plot to provide a
very crude approximation to the model as shown below. Note that the
following also “zooms-in” on dose values between 0 and 200.</p>
<pre class="r"><code>p &lt;- p + stat_summary(fun = &quot;mean&quot;, geom = &quot;line&quot;) + 
  scale_x_continuous(limits = c(0, 200))
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" />
Recall that the slope of the tangent line when dose equals <span
class="math inline">\(\alpha\)</span> is <span
class="math inline">\(-\beta(\delta-\gamma)/(4\alpha)\)</span>. We can
approximate this slope by computing the slope of the line segment that
“contains” what we guess is the value of <span
class="math inline">\(\alpha\)</span>. For example, for the bentazone
herbicide if we guessed that <span class="math inline">\(\alpha\)</span>
was between 20 and 40 g/ha, then the slope of that line segment (using
the means computed above) equals <span class="math inline">\((1.2 -
3.42)/(40 - 20)\)</span>. Thus we might find an approximate value of
<span class="math inline">\(\beta\)</span> to use as a starting value if
we solve for <span class="math inline">\(\beta\)</span> in the equation
<span class="math display">\[
  \frac{1.2-3.42}{40-20} = \frac{-\beta(\delta-\gamma)}{4\alpha},
\]</span> where <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\gamma\)</span>, and <span
class="math inline">\(\delta\)</span> are replaced the values that you
“eyeballed” from the plot of the data to use as starting values. You may
find it useful to use this strategy to find a good starting value for
<span class="math inline">\(\beta\)</span> in your models.</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the nonlinear model described above using
<code>nls</code>. In this model assume that the type of herbicide does
not matter so your model will simply be <span class="math display">\[
  E(M_i) = \gamma + \frac{\delta - \gamma}{1 + e^{\beta(\log d_i - \log
\alpha)}},
\]</span> where <span class="math inline">\(M_i\)</span> and <span
class="math inline">\(d_i\)</span> are the <span
class="math inline">\(i\)</span>-th observations of dry matter and dose,
respectively. To find your starting values you can make a plot of the
data for both herbicides combined by omitting
<code>facet_wrap(~ Herbicide)</code> from the code given earlier to
produce a plot of the raw data without accounting for the type of
herbicide. And to compute the sample means for each dose but not for
each combination of dose use <code>group_by(Dose)</code> instead of
<code>group_by(Herbicide, Dose)</code> in the code given earlier for
computing these means. Give the parameter estimates and their standard
errors using the <code>summary</code> function, and plot the model by
adding a smooth curve to the plot to show the estimated expected
response as a function of dose. Note that if you add this curve to the
original plot then the data frame of predicted values must include the
type of herbicide even though it is not part of your model (see the
first problem from the in-class exercise with the Michaelis-Menten
model).</p>
<p><strong>Solution</strong>: First I will make a plot of the data
combined across the two herbicides as well as producing some summary
statistics to help find the starting value of <span
class="math inline">\(\beta\)</span>.</p>
<pre class="r"><code>p &lt;- ggplot(S.alba, aes(x = Dose, y = DryMatter)) + 
  theme_minimal() + geom_count(alpha = 0.25) + 
  labs(x = &quot;Dose (g/ha)&quot;, y = &quot;Dry Matter (g/pot)&quot;) + 
  theme(legend.position = c(0.95, 0.8))
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>S.alba %&gt;% group_by(Dose) %&gt;% 
  summarize(drymatter = mean(DryMatter))</code></pre>
<pre><code># A tibble: 8 x 2
   Dose drymatter
  &lt;int&gt;     &lt;dbl&gt;
1     0     3.9  
2    10     3.7  
3    20     3.58 
4    40     2.21 
5    80     1.3  
6   160     0.888
7   320     0.8  
8   640     0.675</code></pre>
<p>Based on this plot I am going to guess starting values of <span
class="math inline">\(\delta\)</span> = 4, <span
class="math inline">\(\gamma\)</span> = 0.5, and <span
class="math inline">\(\alpha\)</span> = 50. Then the starting value of
<span class="math inline">\(\beta\)</span> would be the solution to
<span class="math display">\[
   \frac{1.3 - 2.21}{80 - 40} = \frac{-\beta(4 - 0.5)}{4(50)},
\]</span> which is <span class="math display">\[
   \beta = -\frac{1.3 - 2.21}{80 - 40} \times \frac{4(50)}{4 - 0.5} =
1.3.
\]</span> Now we can estimate the model as follows.</p>
<pre class="r"><code>m &lt;- nls(DryMatter ~ gamma + (delta - gamma) / 
  (1 + exp(beta * (log(Dose) - log(alpha)))), data = S.alba,
  start = list(delta = 4, gamma = 0.5, alpha = 50, beta = 1.3))
summary(m)$coefficients</code></pre>
<pre><code>      Estimate Std. Error t value  Pr(&gt;|t|)
delta   3.8883     0.1233  31.528 1.057e-40
gamma   0.7904     0.1409   5.611 4.633e-07
alpha  39.4534     4.0197   9.815 2.189e-14
beta    2.6165     0.6294   4.157 9.764e-05</code></pre>
<p>Here is a plot of the estimated model.</p>
<pre class="r"><code>d &lt;- expand.grid(Dose = seq(0, 640, length = 200), 
  Herbicide = c(&quot;Bentazone&quot;,&quot;Glyphosate&quot;))
d$yhat &lt;- predict(m, newdata = d)
p &lt;- p + geom_line(aes(y = yhat), data = d)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" />
My starting values were a bit off, but not so much to cause
problems.</p></li>
<li><p>Estimate a nonlinear model where the <span
class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span>, and <span
class="math inline">\(\gamma\)</span> parameters vary by herbicide, but
<span class="math inline">\(\delta\)</span> does not, using the
<code>nls</code> function. This model can be written case-wise as <span
class="math display">\[
  E(M_i) =
  \begin{cases}
\gamma_b + \frac{\delta - \gamma_b}{1 + e^{\beta_b\left(\log d_i - \log
\alpha_b\right)}},
   &amp; \text{if the herbacide used was bentazone}, \\
\gamma_g + \frac{\delta - \gamma_g}{1 + e^{\beta_g\left(\log d_i - \log
\alpha_g\right)}},
   &amp; \text{if the herbacide used was glyphosate}.
  \end{cases}  
\]</span> The rationale for this model is that when the dose is zero
there should be no difference in the expected response as a function of
the type of herbicide, so <span class="math inline">\(\delta\)</span>
should not depend on the type of herbicide used. Report the estimates
and standard errors of the seven parameters using the
<code>summary</code> function. Also plot this model with the raw data by
adding a smooth curve to the first plot shown above to show the
estimated expected response as a function of dose and type of
herbicide.</p>
<p><strong>Solution</strong>: We could try to come up with starting
values for each herbicide separately. I guessed <span
class="math inline">\(\delta\)</span> = 4, <span
class="math inline">\(\gamma_b = \gamma_g\)</span> = 0.5, <span
class="math inline">\(\alpha_b\)</span> = 30, and <span
class="math inline">\(\alpha_g\)</span> = 70. Solving for <span
class="math inline">\(\beta_b\)</span> and <span
class="math inline">\(\beta_g\)</span> as in the previous problem based
on these starting values gives <span class="math inline">\(\beta_b
\approx\)</span> 3.8 and <span class="math inline">\(\beta_g\)</span> =
2.74. But you could also use the estimates from the previous model as I
did below.</p>
<pre class="r"><code>library(dplyr) # for case_when function
m &lt;- nls(DryMatter ~ case_when(
  Herbicide == &quot;Bentazone&quot;  ~ gamma_b + (delta - gamma_b) /
    (1 + exp(beta_b * (log(Dose) - log(alpha_b)))),
  Herbicide == &quot;Glyphosate&quot; ~ gamma_g + (delta - gamma_g) /
    (1 + exp(beta_g * (log(Dose) - log(alpha_g))))
), start = list(delta = 3.9, gamma_b = 0.8, gamma_g = 0.8, 
  alpha_b = 39, alpha_g = 39, beta_b = 2.6, beta_g = 2.6),
data = S.alba)
summary(m)$coefficients</code></pre>
<pre><code>        Estimate Std. Error t value  Pr(&gt;|t|)
delta     3.8424    0.07518  51.108 8.732e-52
gamma_b   0.6810    0.09453   7.204 1.033e-09
gamma_g   0.9038    0.17951   5.035 4.515e-06
alpha_b  28.9821    2.07810  13.946 1.235e-20
alpha_g  62.5509    6.38619   9.795 3.918e-14
beta_b    4.9963    1.00048   4.994 5.253e-06
beta_g    2.8203    0.66661   4.231 7.949e-05</code></pre>
<p>Note that this code can get a little tedious. We can simplify it
somewhat by (a) creating a function for the model and (b) specifying the
starting values outside the function. This is easier to read and
debug.</p>
<pre class="r"><code>f &lt;- function(x, alpha, beta, delta, gamma) {
  gamma + (delta - gamma) / (1 + exp(beta * (log(x) - log(alpha))))
}

startvalues &lt;- list(delta = 3.9, gamma_b = 0.8, gamma_g = 0.8, 
  alpha_b = 39, alpha_g = 39, beta_b = 2.6, beta_g = 2.6)

m &lt;- nls(DryMatter ~ case_when(
  Herbicide == &quot;Bentazone&quot;  ~ f(Dose, alpha_b, beta_b, delta, gamma_b),
  Herbicide == &quot;Glyphosate&quot; ~ f(Dose, alpha_g, beta_g, delta, gamma_g)
), start = startvalues, data = S.alba)
summary(m)$coefficients</code></pre>
<pre><code>        Estimate Std. Error t value  Pr(&gt;|t|)
delta     3.8424    0.07518  51.108 8.732e-52
gamma_b   0.6810    0.09453   7.204 1.033e-09
gamma_g   0.9038    0.17951   5.035 4.515e-06
alpha_b  28.9821    2.07810  13.946 1.235e-20
alpha_g  62.5509    6.38619   9.795 3.918e-14
beta_b    4.9963    1.00048   4.994 5.253e-06
beta_g    2.8203    0.66661   4.231 7.949e-05</code></pre>
<p>Here is the plot.</p>
<pre class="r"><code>d &lt;- expand.grid(Dose = seq(0, 640, length = 200), 
  Herbicide = c(&quot;Bentazone&quot;,&quot;Glyphosate&quot;))
d$yhat &lt;- predict(m, newdata = d)
p &lt;- ggplot(S.alba, aes(x = Dose, y = DryMatter)) + theme_minimal() + 
  geom_count(alpha = 0.25) + facet_wrap(~ Herbicide) + 
  geom_line(aes(y = yhat), data = d) + 
  labs(x = &quot;Dose (g/ha)&quot;, y = &quot;Dry Matter (g/pot)&quot;) + 
  theme(legend.position = c(0.95, 0.8))
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" /></p></li>
<li><p>A researcher might like to make inferences about the difference
in the <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span>, and <span
class="math inline">\(\gamma\)</span> parameters between the two
herbicides. Use the <code>lincon</code> function to produce estimates,
standard errors, confidence intervals, and tests concerning <span
class="math inline">\(\alpha_b-\alpha_g\)</span>, <span
class="math inline">\(\beta_b-\beta_g\)</span>, and <span
class="math inline">\(\gamma_b-\gamma_g\)</span>.</p>
<p><strong>Solution</strong>: Here is how we can make inferences about
the differences between the parameters of the two herbicides.</p>
<pre class="r"><code>library(trtools) # for lincon
lincon(m, a = c(0,0,0,1,-1,0,0)) # alpha_b - alpha_g</code></pre>
<pre><code>                   estimate    se  lower  upper tvalue df    pvalue
(0,0,0,1,-1,0,0),0   -33.57 6.602 -46.77 -20.37 -5.085 61 3.756e-06</code></pre>
<pre class="r"><code>lincon(m, a = c(0,0,0,0,0,1,-1)) # beta_b - beta_g</code></pre>
<pre><code>                   estimate   se   lower upper tvalue df  pvalue
(0,0,0,0,0,1,-1),0    2.176 1.16 -0.1445 4.496  1.875 61 0.06557</code></pre>
<pre class="r"><code>lincon(m, a = c(0,1,-1,0,0,0,0)) # gamma_b - gamma_g</code></pre>
<pre><code>                   estimate     se   lower  upper tvalue df pvalue
(0,1,-1,0,0,0,0),0  -0.2228 0.2028 -0.6283 0.1826 -1.099 61  0.276</code></pre>
<p>The difference between <span class="math inline">\(\alpha_b\)</span>
and <span class="math inline">\(\alpha_g\)</span> is statistically
significant. This parameter would often be the focus of a study like
this as it represents the “potency” of the herbicide in a sense by
showing how much is necessary to reduce the expected response by half of
how much it will be reduced as dose increases. Here bentazone appears to
be more potent than glyphosate.</p></li>
</ol>
</div>
<div id="jevons-gold-sovereigns" class="section level2">
<h2>Jevon’s Gold Sovereigns</h2>
<p>The data frame <code>jevons</code> in the <strong>alr4</strong>
package contains summary statistics on the weights of <a
href="https://en.wikipedia.org/wiki/Sovereign_(British_coin)">sovereigns</a>
(i.e., British gold coins) that were collected from circulation in
Manchester, England. These data are from a paper by the 19th century
economist and philosopher <a
href="https://en.wikipedia.org/wiki/William_Stanley_Jevons">William
Stanley Jevons</a>.<a href="#fn5" class="footnote-ref"
id="fnref5"><sup>5</sup></a> This data frame (shown below) gives the
mean and standard deviation of the weights of five samples of sovereigns
that vary by age (in decades).</p>
<pre class="r"><code>library(alr4)
jevons</code></pre>
<pre><code>  Age   n Weight      SD   Min   Max
1   1 123  7.973 0.01409 7.900 7.999
2   2  78  7.950 0.02272 7.892 7.993
3   3  32  7.928 0.03426 7.848 7.984
4   4  17  7.896 0.04057 7.827 7.965
5   5  24  7.873 0.05353 7.757 7.961</code></pre>
<p>We do not have the original data, so for the purpose of this exercise
you will create an artificial data set that produces data with the same
sample sizes, means, and standard deviations.<a href="#fn6"
class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<pre class="r"><code>library(dplyr)
library(tidyr)

set.seed(123)
coins &lt;- jevons %&gt;% uncount(n) %&gt;% 
  group_by(Age) %&gt;% mutate(y = rnorm(n(), Weight, SD)) %&gt;%
  mutate(y = SD * (y - mean(y))/sd(y) + Weight) %&gt;%
  dplyr::select(Age, y) %&gt;% rename(Weight = y)</code></pre>
<p>We can confirm that these artificial data give the same means and
standard deviations as the original data.</p>
<pre class="r"><code>coins %&gt;% group_by(Age) %&gt;% 
  summarize(n = n(), meanweight = mean(Weight), sdweight = sd(Weight))</code></pre>
<pre><code># A tibble: 5 x 4
    Age     n meanweight sdweight
  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;
1     1   123       7.97   0.0141
2     2    78       7.95   0.0227
3     3    32       7.93   0.0343
4     4    17       7.90   0.0406
5     5    24       7.87   0.0535</code></pre>
<p>The figure below shows a plot of the simulated data.</p>
<pre class="r"><code>p &lt;- ggplot(coins, aes(x = factor(Age), y = Weight)) + theme_minimal() + 
  geom_hline(yintercept = 7.9876) + 
  geom_hline(yintercept = 7.9379, linetype = 3) + 
  geom_dotplot(binaxis = &quot;y&quot;, binwidth = 0.001, method = &quot;histodot&quot;) +
  coord_flip() + labs(x = &quot;Age (decades)&quot;, y = &quot;Weight (g)&quot;) 
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-17-1.png" width="100%" style="display: block; margin: auto;" />
The solid line shows the intended standard weight of newly minted
sovereigns (7.9876 g), and the dotted line shows the minimum legal
weight (7.9379 g). Perhaps not unsurprisingly it can be seen that, on
average, older sovereigns have less weight, presumably due to wear while
in circulation. But note also that the <em>variability</em> of weight
appears to increase with the age of the coins. This may be due to
differences in how much the coins are in circulation. Some coins are
frequently being exchanged thus losing more material, whereas others may
being exchanged less and thus not losing as much material. In this
problem you will consider various ways of dealing with the
heteroscedasticity as well as the consequences of failing to account for
heteroscedasticity.</p>
<ol style="list-style-type: decimal">
<li><p>Estimate a linear model using the <code>lm</code> function with
<code>Weight</code> as the response variable and <code>Age</code> as the
explanatory variable. In your model treat age as a <em>factor</em>
(i.e., a categorical variable) and not a quantitative variable. You can
do this by either using <code>factor(Age)</code> instead of
<code>Age</code> in the model formula, or by creating a new variable
such as <code>coins$Agef &lt;- factor(coins$Age)</code> which will
coerce the variable into a factor. Use either <code>contrast</code>
<em>or</em> functions from the <strong>emmeans</strong> package to
produce estimates, standard errors, and confidence intervals for (a) the
expected weight of coins from each age group and (b) the difference in
the expected weight between the the newest coins (i.e., age of one
decade) and the other four groups of coins.<a href="#fn7"
class="footnote-ref" id="fnref7"><sup>7</sup></a> For this model you
should find that the estimated expected weights are equal the
corresponding sample means, and that the estimated differences in the
expected weights are equal to the differences in the corresponding
sample means.</p>
<p><strong>Solution</strong>: First here is the model.</p>
<pre class="r"><code>m.ols &lt;- lm(Weight ~ factor(Age), data = coins)
summary(m.ols)$coefficients</code></pre>
<pre><code>             Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)    7.9725   0.002413 3304.468 0.000e+00
factor(Age)2  -0.0222   0.003873   -5.732 2.658e-08
factor(Age)3  -0.0449   0.005310   -8.456 1.783e-15
factor(Age)4  -0.0763   0.006924  -11.020 1.491e-23
factor(Age)5  -0.0995   0.005971  -16.664 2.559e-43</code></pre>
<p>We can make inferences about the expected weight for each age group
as follows.</p>
<pre class="r"><code>trtools::contrast(m.ols, a = list(Age = 1:5), 
  cnames = paste(&quot;Age&quot;, 1:5))</code></pre>
<pre><code>      estimate       se lower upper tvalue  df pvalue
Age 1    7.973 0.002413 7.968 7.977   3304 269      0
Age 2    7.950 0.003030 7.944 7.956   2624 269      0
Age 3    7.928 0.004730 7.918 7.937   1676 269      0
Age 4    7.896 0.006490 7.883 7.909   1217 269      0
Age 5    7.873 0.005462 7.862 7.884   1441 269      0</code></pre>
<pre class="r"><code>library(emmeans)
emmeans(m.ols, ~Age)</code></pre>
<pre><code> Age emmean       SE  df lower.CL upper.CL
   1  7.973 0.002413 269    7.968    7.977
   2  7.950 0.003030 269    7.944    7.956
   3  7.928 0.004730 269    7.918    7.937
   4  7.896 0.006490 269    7.883    7.909
   5  7.873 0.005462 269    7.862    7.884

Confidence level used: 0.95 </code></pre>
<p>And we can make inferences comparing the expected weight for the
first decade group with the other four age groups.</p>
<pre class="r"><code>trtools::contrast(m.ols, 
  a = list(Age = 1),
  b = list(Age = 2:5),
  cnames = c(&quot;1-2&quot;,&quot;1-3&quot;,&quot;1-4&quot;,&quot;1-5&quot;))</code></pre>
<pre><code>    estimate       se   lower   upper tvalue  df    pvalue
1-2   0.0222 0.003873 0.01457 0.02983  5.732 269 2.658e-08
1-3   0.0449 0.005310 0.03445 0.05535  8.456 269 1.783e-15
1-4   0.0763 0.006924 0.06267 0.08993 11.020 269 1.491e-23
1-5   0.0995 0.005971 0.08774 0.11126 16.664 269 2.559e-43</code></pre>
<pre class="r"><code>emmeans::contrast(emmeans(m.ols, ~ Age), method = &quot;trt.vs.ctrl&quot;, 
  ref = 1, adjust = &quot;none&quot;, reverse = TRUE, infer = TRUE)</code></pre>
<pre><code> contrast estimate      SE  df lower.CL upper.CL t.ratio p.value
 1 - 2      0.0222 0.00387 269   0.0146   0.0298   5.732  &lt;.0001
 1 - 3      0.0449 0.00531 269   0.0345   0.0554   8.456  &lt;.0001
 1 - 4      0.0763 0.00692 269   0.0627   0.0899  11.020  &lt;.0001
 1 - 5      0.0995 0.00597 269   0.0877   0.1113  16.664  &lt;.0001

Confidence level used: 0.95 </code></pre></li>
<li><p>Assume that the variances vary by decade so that <span
class="math display">\[
Y_i =
\begin{cases}
  \sigma_1^2, &amp; \text{if the $i$-th observation is of a coin one
decade old}, \\
  \sigma_2^2, &amp; \text{if the $i$-th observation is of a coin two
decades old}, \\
  \sigma_3^2, &amp; \text{if the $i$-th observation is of a coin three
decades old}, \\
  \sigma_4^2, &amp; \text{if the $i$-th observation is of a coin four
decades old}, \\
  \sigma_5^2, &amp; \text{if the $i$-th observation is of a coin five
decades old}.
\end{cases}
\]</span> There are a couple of different ways to account for this kind
of variance structure. One is to use <em>weighted</em> least squares
where the weights are estimated as the reciprocals of the sample
variances. We discussed how to compute these weights using functions
from the <strong>dplyr</strong> package. Another approach is to use a
parametric model where the five variances are effectively estimated from
the data. We discussed how to do this with using the <code>gls</code>
function from the <strong>nlme</strong> package. Use both of these
approaches and for each show the parameter estimates and their standard
errors as given by <code>summary</code>, and use either the
<code>contrast</code> function or functions from the
<strong>emmeans</strong> package to produce estimates, standard errors,
and confidence intervals for (a) the expected weight of coins from each
age group and (b) the difference in the expected weight between the the
newest coins, just as you did in the previous problem.<a href="#fn8"
class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p><strong>Solution</strong>: For the weighted leasts squares approach
we first compute the weights using the sample variances and add them to
the data frame.</p>
<pre class="r"><code>library(dplyr)
coins &lt;- coins %&gt;% group_by(Age) %&gt;% 
  mutate(w = 1 / var(Weight))</code></pre>
<p>Next we use weighted least squares.</p>
<pre class="r"><code>m.wls &lt;- lm(Weight ~ factor(Age), weights = w, data = coins)
summary(m.wls)$coefficients</code></pre>
<pre><code>             Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)    7.9725   0.001270 6275.323 0.000e+00
factor(Age)2  -0.0222   0.002869   -7.738 2.050e-13
factor(Age)3  -0.0449   0.006188   -7.256 4.277e-12
factor(Age)4  -0.0763   0.009921   -7.690 2.772e-13
factor(Age)5  -0.0995   0.011000   -9.045 3.065e-17</code></pre>
<p>Next I will produce the inferences for the expected weight for each
age group and the differences in the expected weights between the first
decade and later age groups. Here I will only use functions from the
<strong>emmeans</strong> package, but the <code>contrast</code> function
from the <strong>trtools</strong> package would produce the same
results.</p>
<pre class="r"><code>emmeans(m.wls, ~ Age)</code></pre>
<pre><code> Age emmean       SE  df lower.CL upper.CL
   1  7.972 0.001270 269    7.970    7.975
   2  7.950 0.002573 269    7.945    7.955
   3  7.928 0.006056 269    7.916    7.940
   4  7.896 0.009840 269    7.877    7.916
   5  7.873 0.010927 269    7.851    7.895

Confidence level used: 0.95 </code></pre>
<pre class="r"><code>emmeans::contrast(emmeans(m.wls, ~ Age), method = &quot;trt.vs.ctrl&quot;, 
  ref = 1, adjust = &quot;none&quot;, reverse = TRUE, infer = TRUE)</code></pre>
<pre><code> contrast estimate      SE  df lower.CL upper.CL t.ratio p.value
 1 - 2      0.0222 0.00287 269   0.0165   0.0278   7.738  &lt;.0001
 1 - 3      0.0449 0.00619 269   0.0327   0.0571   7.256  &lt;.0001
 1 - 4      0.0763 0.00992 269   0.0568   0.0958   7.690  &lt;.0001
 1 - 5      0.0995 0.01100 269   0.0778   0.1212   9.045  &lt;.0001

Confidence level used: 0.95 </code></pre>
<p>Now I will estimate the parametric model.</p>
<pre class="r"><code>library(nlme)
m.gls &lt;- gls(Weight ~ factor(Age), data = coins, method = &quot;ML&quot;,
  weights = varIdent(form = ~ 1 | Age))
summary(m.gls)</code></pre>
<pre><code>Generalized least squares fit by maximum likelihood
  Model: Weight ~ factor(Age) 
  Data: coins 
    AIC   BIC logLik
  -1312 -1276  665.9

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | Age 
 Parameter estimates:
    1     2     3     4     5 
1.000 1.609 2.403 2.805 3.734 

Coefficients:
              Value Std.Error t-value p-value
(Intercept)   7.972  0.001277    6243       0
factor(Age)2 -0.022  0.002878      -8       0
factor(Age)3 -0.045  0.006150      -7       0
factor(Age)4 -0.076  0.009718      -8       0
factor(Age)5 -0.099  0.010871      -9       0

 Correlation: 
             (Intr) fc(A)2 fc(A)3 fc(A)4
factor(Age)2 -0.444                     
factor(Age)3 -0.208  0.092              
factor(Age)4 -0.131  0.058  0.027       
factor(Age)5 -0.117  0.052  0.024  0.015

Standardized residuals:
     Min       Q1      Med       Q3      Max 
-2.61356 -0.66557 -0.09847  0.64688  3.09641 

Residual standard error: 0.01403 
Degrees of freedom: 274 total; 269 residual</code></pre>
<p>Here are the inferences about the expected weights for each age group
and comparing the age groups with the first.</p>
<pre class="r"><code>emmeans(m.gls, ~ Age)</code></pre>
<pre><code> Age emmean       SE     df lower.CL upper.CL
   1  7.972 0.001277 121.20    7.970    7.975
   2  7.950 0.002580  78.09    7.945    7.955
   3  7.928 0.006016  33.64    7.915    7.940
   4  7.896 0.009634  17.49    7.876    7.916
   5  7.873 0.010796  24.78    7.851    7.895

Degrees-of-freedom method: satterthwaite 
Confidence level used: 0.95 </code></pre>
<pre class="r"><code>emmeans::contrast(emmeans(m.gls, ~ Age), method = &quot;trt.vs.ctrl&quot;, 
  ref = 1, adjust = &quot;none&quot;, reverse = TRUE, infer = TRUE)</code></pre>
<pre><code> contrast estimate      SE    df lower.CL upper.CL t.ratio p.value
 1 - 2      0.0222 0.00288 117.1   0.0165   0.0279   7.713  &lt;.0001
 1 - 3      0.0449 0.00615  36.7   0.0324   0.0574   7.301  &lt;.0001
 1 - 4      0.0763 0.00972  18.1   0.0559   0.0967   7.851  &lt;.0001
 1 - 5      0.0995 0.01087  25.5   0.0771   0.1219   9.153  &lt;.0001

Degrees-of-freedom method: satterthwaite 
Confidence level used: 0.95 </code></pre></li>
<li><p>Compare the estimates and standard errors for estimating the
model parameters as well as the expected weight and differences in
expected weight when accounting for heteroscedasticity as you did in the
last problem, and when not accounting for heteroscedasticity as you did
in the first problem. Discuss briefly how failing to account
heteroscedasticity (i.e., incorrectly assuming homoscedasticity) may
affect your inferences.</p>
<p><strong>Solution</strong>: You should see that the estimates of the
parameters (and thus the expected weights and their differences) are the
same whether we use ordinary (i.e., unweighted) least squares, weighted
least squares, or a parametric model. It is important to note that this
is not always the case. Using weights or a parametric model may produce
different parameter estimates. It depends on the model. The standard
errors, however, are affected by if we account for heteroscedasticity.
Perhaps the most useful comparison is to look at the estimates of the
expected weights and their differences. First consider the estimates of
the expected weights.</p>
<pre class="r"><code>emmeans(m.ols, ~ Age)</code></pre>
<pre><code> Age emmean       SE  df lower.CL upper.CL
   1  7.973 0.002413 269    7.968    7.977
   2  7.950 0.003030 269    7.944    7.956
   3  7.928 0.004730 269    7.918    7.937
   4  7.896 0.006490 269    7.883    7.909
   5  7.873 0.005462 269    7.862    7.884

Confidence level used: 0.95 </code></pre>
<pre class="r"><code>emmeans(m.wls, ~ Age)</code></pre>
<pre><code> Age emmean       SE  df lower.CL upper.CL
   1  7.972 0.001270 269    7.970    7.975
   2  7.950 0.002573 269    7.945    7.955
   3  7.928 0.006056 269    7.916    7.940
   4  7.896 0.009840 269    7.877    7.916
   5  7.873 0.010927 269    7.851    7.895

Confidence level used: 0.95 </code></pre>
<pre class="r"><code>emmeans(m.gls, ~ Age)</code></pre>
<pre><code> Age emmean       SE     df lower.CL upper.CL
   1  7.972 0.001277 121.20    7.970    7.975
   2  7.950 0.002580  78.09    7.945    7.955
   3  7.928 0.006016  33.64    7.915    7.940
   4  7.896 0.009634  17.49    7.876    7.916
   5  7.873 0.010796  24.78    7.851    7.895

Degrees-of-freedom method: satterthwaite 
Confidence level used: 0.95 </code></pre>
<p>Note that the standard errors when using the weighted least squares
and parametric model are fairly similar, but more dissimilar to those
when we do not account for the heteroscedasticity when using ordinary
least squares. The standard errors of the expected counts are larger for
older coins when accounting for the heteroscedasticity. The standard
error for the five decade group is nearly ten times that of the one
decade group when accounting for heteroscedasticity, but only about
twice as much when not accounting for heteroscedasticity. Now consider
the estimates of the differences in the expected weights.</p>
<pre class="r"><code>emmeans::contrast(emmeans(m.ols, ~ Age), method = &quot;trt.vs.ctrl&quot;, 
  ref = 1, adjust = &quot;none&quot;, reverse = TRUE, infer = TRUE)</code></pre>
<pre><code> contrast estimate      SE  df lower.CL upper.CL t.ratio p.value
 1 - 2      0.0222 0.00387 269   0.0146   0.0298   5.732  &lt;.0001
 1 - 3      0.0449 0.00531 269   0.0345   0.0554   8.456  &lt;.0001
 1 - 4      0.0763 0.00692 269   0.0627   0.0899  11.020  &lt;.0001
 1 - 5      0.0995 0.00597 269   0.0877   0.1113  16.664  &lt;.0001

Confidence level used: 0.95 </code></pre>
<pre class="r"><code>emmeans::contrast(emmeans(m.wls, ~ Age), method = &quot;trt.vs.ctrl&quot;, 
  ref = 1, adjust = &quot;none&quot;, reverse = TRUE, infer = TRUE)</code></pre>
<pre><code> contrast estimate      SE  df lower.CL upper.CL t.ratio p.value
 1 - 2      0.0222 0.00287 269   0.0165   0.0278   7.738  &lt;.0001
 1 - 3      0.0449 0.00619 269   0.0327   0.0571   7.256  &lt;.0001
 1 - 4      0.0763 0.00992 269   0.0568   0.0958   7.690  &lt;.0001
 1 - 5      0.0995 0.01100 269   0.0778   0.1212   9.045  &lt;.0001

Confidence level used: 0.95 </code></pre>
<pre class="r"><code>emmeans::contrast(emmeans(m.gls, ~ Age), method = &quot;trt.vs.ctrl&quot;, 
  ref = 1, adjust = &quot;none&quot;, reverse = TRUE, infer = TRUE)</code></pre>
<pre><code> contrast estimate      SE    df lower.CL upper.CL t.ratio p.value
 1 - 2      0.0222 0.00288 117.1   0.0165   0.0279   7.713  &lt;.0001
 1 - 3      0.0449 0.00615  36.7   0.0324   0.0574   7.301  &lt;.0001
 1 - 4      0.0763 0.00972  18.1   0.0559   0.0967   7.851  &lt;.0001
 1 - 5      0.0995 0.01087  25.5   0.0771   0.1219   9.153  &lt;.0001

Degrees-of-freedom method: satterthwaite 
Confidence level used: 0.95 </code></pre>
<p>Note that with the exception of the second decade, the standard
errors for the difference in the expected weight for each later decade
and the first decade are larger when accounting for heteroscedasticity.
Overall we should be concerned about <em>overestimating</em> or
<em>underestimating</em> standard errors when failing to account for
heteroscedasticity. Whether that makes and qualitative differences in
our inferences depends on the situation, however. Here the changes in
the confidence intervals and tests are not very dramatic, but they could
be in other cases.</p></li>
</ol>
</div>
<div id="mortality-of-confused-flour-beetles-from-carbon-disulphide"
class="section level2">
<h2>Mortality of Confused Flour Beetles from Carbon Disulphide</h2>
<p>The data frame <code>bliss</code> in the <strong>trtools</strong>
package are from an experiment investigating the effect of gaseous
carbon disulphide (<span class="math inline">\(\text{CS}_2\)</span>) on
the mortality of <a
href="https://en.wikipedia.org/wiki/Confused_flour_beetle">confused
flour beetles (<em>Tribolium confusum</em>)</a>.<a href="#fn9"
class="footnote-ref" id="fnref9"><sup>9</sup></a> Cloth cages of batches
of approximately thirty beetles were suspended in a flask above a fixed
volume of liquid carbon disulphide. The number of dead beetles after
five hours of exposure was recorded. The figure below shows the
proportion of dead beetles by concentration of carbon disulphide. Note
that there are two observations for each dose.</p>
<pre class="r"><code>library(trtools) 
library(ggplot2)
library(ggrepel)

bliss$proportion &lt;- paste(bliss$dead, &quot;/&quot;, bliss$exposed, sep = &quot;&quot;)

p &lt;- ggplot(bliss, aes(x = concentration, y = dead/exposed)) +
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) + 
  labs(x = &quot;Concentration of Carbon Disulphide (mg/liter)&quot;,
    y = &quot;Proportion of Beetles Dying&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-28-1.png" width="100%" style="display: block; margin: auto;" />
A naive approach to modeling these data would be to use linear
regression where the proportion is the response variable.</p>
<pre class="r"><code>m &lt;- lm(dead/exposed ~ concentration, data = bliss)
cbind(summary(m)$coefficients, confint(m))</code></pre>
<pre><code>              Estimate Std. Error t value  Pr(&gt;|t|)    2.5 %   97.5 %
(Intercept)   -1.73277   0.159636  -10.85 3.352e-08 -2.07515 -1.39038
concentration  0.03717   0.002516   14.77 6.225e-10  0.03178  0.04257</code></pre>
<pre class="r"><code>d &lt;- data.frame(concentration = seq(49.06, 76.54, length = 100))
d$yhat &lt;- predict(m, newdata = d)

p &lt;- ggplot(bliss, aes(x = concentration, y = dead/exposed)) +
  geom_line(aes(y = yhat), data = d) + 
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) + 
  labs(x = &quot;Concentration of Carbon Disulphide (mg/liter)&quot;,
    y = &quot;Proportion of Beetles Dying&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-29-1.png" width="100%" style="display: block; margin: auto;" />
This model is probably not adequate for two reasons. One is that the
relationship between the expected of dead beetles and concentration is
probably not linear. Secondly, proportions tend to exhibit
heteroscedasticity where the variance of a proportion tends to decrease
as its expected value gets farther from 0.5. As we will discuss in
lecture, if the number of dead beetles has a <em>binomial
distribution</em>, then it can be shown that <span
class="math display">\[
  \text{Var}(P) = E(P)[1-E(P)]/m,
\]</span> where <span class="math inline">\(P\)</span> is the proportion
(i.e., <code>dead/exposed</code>) and <span
class="math inline">\(m\)</span> is the denominator of the proportion
(i.e., <code>exposed</code>). This implies that the variance of <span
class="math inline">\(P\)</span> decreases as <span
class="math inline">\(E(P)\)</span> gets farther from 0.5, and also
decreases as <span class="math inline">\(m\)</span> increases.</p>
<p>These data will be used in lecture to demonstrate logistic
regression, but for this problem you will consider modeling the data
using nonlinear regression. Later we will also discuss the relationship
between logistic and nonlinear regression.</p>
<ol style="list-style-type: decimal">
<li><p>Consider the nonlinear regression model <span
class="math display">\[
  E(P_i) = \frac{1}{1 + e^{-\beta_0 - \beta_1d_i}},
\]</span> where <span class="math inline">\(P_i\)</span> and <span
class="math inline">\(d_i\)</span> are the <span
class="math inline">\(i\)</span>-th observations of the proportion of
dead beetles (i.e., <code>dead/exposed</code>) and the concentration,
respectively.<a href="#fn10" class="footnote-ref"
id="fnref10"><sup>10</sup></a> Use the <code>nls</code> function to
estimate this nonlinear regression model. For starting values you can
cheat and use the parameter estimates from a logistic regression model
estimated as followed.</p>
<pre class="r"><code>m &lt;- glm(cbind(dead, exposed - dead) ~ concentration, 
  family = binomial, data = bliss)
summary(m)$coefficients</code></pre>
<pre><code>              Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept)   -14.8084    1.28976  -11.48 1.633e-30
concentration   0.2492    0.02138   11.65 2.250e-31</code></pre>
<p>The two estimates reported above are estimates of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> from the logistic regression
model. You can use these as your starting values for <code>nls</code>.
The estimates you obtain using nonlinear regression should be similar
but not necessarily equal to those shown above. Report the parameter
estimates and their standard errors by showing the output from
<code>summary</code>. Also plot the estimated model by adding a curve to
the plot shown above.<a href="#fn11" class="footnote-ref"
id="fnref11"><sup>11</sup></a></p>
<p><strong>Solution</strong>: We can estimate the nonlinear model as
follows.</p>
<pre class="r"><code>m &lt;- nls(dead/exposed ~ 1 / (1 + exp(-b0 - b1 * concentration)),
  data = bliss, start = list(b0 = -14.5, b1 = 0.25))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0 -14.5774    1.22178  -11.93 1.007e-08
b1   0.2434    0.02032   11.98 9.570e-09</code></pre>
<p>Here is a plot of the estimated model.</p>
<pre class="r"><code>d &lt;- data.frame(concentration = seq(49.06, 76.54, length = 100))
d$yhat &lt;- predict(m, newdata = d)
p &lt;- ggplot(bliss, aes(x = concentration, y = dead/exposed)) +
  geom_line(aes(y = yhat), data = d) + 
  geom_point() + ylim(0, 1) + theme_minimal() + 
  geom_label_repel(aes(label = proportion), box.padding = 0.75) + 
  labs(x = &quot;Concentration of Carbon Disulphide (mg/liter)&quot;,
    y = &quot;Proportion of Beetles Dying&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-32-1.png" width="100%" style="display: block; margin: auto;" /></p></li>
<li><p>As described above, the number of dead beetles has a binomial
distribution then the variance of <span
class="math inline">\(P_i\)</span> is <span class="math display">\[
  \text{Var}(P_i) = E(P_i)[1-E(P_i)]/m_i,
\]</span> where <span class="math inline">\(m_i\)</span> is the number
of exposed beetles for the <span class="math inline">\(i\)</span>-th
observation. Since <span class="math inline">\(E(P_i)\)</span> is
unknown it can be estimated as the predicted value <span
class="math inline">\(\hat{Y_i}\)</span>. Use an iteratively weighted
least squares algorithm with weights implied by the variance above to
estimate the nonlinear model shown above and show the parameter
estimates and standard errors using <code>summary</code>. You should
find that the parameter estimates will be equal to or very close to
those obtained using the <code>glm</code> function above, but the
standard errors will be somewhat different.</p>
<p><strong>Solution</strong>: Here is how to estimate the same model but
using iteratively weighted least squares.</p>
<pre class="r"><code>bliss$w &lt;- 1
for (i in 1:10) {
  m &lt;- nls(dead/exposed ~ 1 / (1 + exp(-b0 - b1 * concentration)),
    data = bliss, start = list(b0 = -14.5, b1 = 0.25), weights = w)
  bliss$yhat &lt;- predict(m)
  bliss$w &lt;- bliss$exposed / (bliss$yhat * (1 - bliss$yhat))
}
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0 -14.8084    1.13489  -13.05 3.169e-09
b1   0.2492    0.01882   13.24 2.617e-09</code></pre>
<p>The parameter estimates are indeed identical to what we obtain when
using logistic regression, but the standard errors are different. Later
in the course we will see that the approach used here is equivalent to
what is know as a <em>quasi-likelihood</em> approach.</p></li>
</ol>
</div>
<div
id="estimating-bias-in-field-measurements-in-defects-in-the-alaska-pipeline"
class="section level2">
<h2>Estimating Bias in Field Measurements in Defects in the Alaska
Pipeline</h2>
<p>The data frame <code>pipeline</code> in the <strong>alr4</strong>
package is from a study of the bias of field measurements of defects in
the <a
href="https://en.wikipedia.org/wiki/Trans-Alaska_Pipeline_System">Alaska
pipeline</a>. That data includes field and laboratory measurements of
the number of defects in observational units from the pipeline.<a
href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<pre class="r"><code>p &lt;- ggplot(pipeline, aes(x = Lab, y = Field)) + theme_minimal() +
  geom_abline(intercept = 0, slope = 1, linetype = 3) + geom_point() + 
  labs(x = &quot;Laboratory Measurement&quot;, y = &quot;Field Measurement&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-34-1.png" width="100%" style="display: block; margin: auto;" />
Assume that the laboratory measurements are very accurate and can be
treated as the “true” number of defects. Field measurements are faster
and cheaper than laboratory measurements, but are more prone to
measurement error (both systematic error or <em>bias</em>, and random
measurement error). The figure above suggests that the field
measurements tend to underestimate the number of defects, particularly
as the actual number of defects (as shown by the laboratory measurement)
increases. A regression model can be used to estimate the bias of the
field measurements so that they can be adjusted appropriately (a process
sometimes called <em>calibration</em>). In this problem you will use
linear and nonlinear regression to estimate a calibration model.</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(F_i\)</span> and <span
class="math inline">\(L_i\)</span> denote the field and laboratory
measurements, respectively, for the <span
class="math inline">\(i\)</span>-th observation. If we assume that <span
class="math inline">\(L_i\)</span> is the true number of defects in the
<span class="math inline">\(i\)</span>-th observational unit, then the
bias is the expected difference between <span
class="math inline">\(F_i\)</span> and <span
class="math inline">\(L_i\)</span> which is <span
class="math inline">\(E(F_i - L_i)\)</span>. The figures suggests that
the bias tends to increase as <span class="math inline">\(L_i\)</span>
increases, so one possible model might be that the bias is proportional
to <span class="math inline">\(L_i\)</span>. This can be written as
<span class="math display">\[
  E(F_i - L_i) = \theta L_i,
\]</span> where <span class="math inline">\(\theta\)</span> is the
constant of proportionality. If <span class="math inline">\(\theta &lt;
0\)</span> then the field measurements tend to <em>underestimate</em>
the number of defects by <span class="math inline">\((1 -
\theta)100\)</span>% (assuming that <span class="math inline">\(\theta
&gt; 0\)</span>), and if <span class="math inline">\(\theta &gt;
0\)</span> then the field measurements tend to <em>overestimate</em> the
number of defects by <span class="math inline">\((\theta -
1)100\)</span>%. The model shown above is linear so it can be estimated
using <code>lm</code>. There are several ways to do this. One is to use
<span class="math inline">\(F_i - L_i\)</span> as the response variable
and estimate a model with <span class="math inline">\(L_i\)</span> as
the explanatory variable but <em>without</em> a constant term (i.e.,
“intercept”).<a href="#fn13" class="footnote-ref"
id="fnref13"><sup>13</sup></a> A second approach is to write the model
as <span class="math display">\[
  E(F_i) = L_i + \theta L_i
\]</span> if we regard <span class="math inline">\(L_i\)</span> as a
fixed and not random variable so that <span class="math inline">\(E(F_i
- L_i) = E(F_i) - L_i\)</span>.<a href="#fn14" class="footnote-ref"
id="fnref14"><sup>14</sup></a> This model is a special case of the
linear model <span class="math display">\[
  E(F_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2},
\]</span> where <span class="math inline">\(\beta_0\)</span> = 0, <span
class="math inline">\(\beta_1\)</span> = 1, <span
class="math inline">\(\beta_2 = \theta\)</span>, and <span
class="math inline">\(x_{i1} = x_{i2} = L_i\)</span>. This model is a
little strange in that we have two explanatory variables that are the
same variable, but <span class="math inline">\(\beta_1\)</span> is not
estimated but instead is fixed at one. This can be done by specifying an
<em>offset</em> in your model (see footnote).<a href="#fn15"
class="footnote-ref" id="fnref15"><sup>15</sup></a> One last approach is
to write the model as <span class="math display">\[
  E(F_i) = \gamma L_i
\]</span> where <span class="math inline">\(\gamma = 1 + \theta\)</span>
since <span class="math display">\[
  E(F_i) = L_i + \theta L_i = (1 + \theta)L_i = \gamma L_i.
\]</span> Estimate the three models described above in the way described
and show the estimate and standard error of the model parameter (i.e.,
<span class="math inline">\(\theta\)</span> or <span
class="math inline">\(\gamma\)</span>) using <code>summary</code>. The
first two models should give you the same estimate and standard error of
<span class="math inline">\(\theta\)</span>, and the last model should
give you an estimate of <span class="math inline">\(\gamma\)</span> that
equals the estimate of <span class="math inline">\(1 + \theta\)</span>
from the previous models.</p>
<p><strong>Solution</strong>: Here is how to estimate the first
model.</p>
<pre class="r"><code>m &lt;- lm(Field - Lab ~ -1 + Lab, data = pipeline)
summary(m)$coefficients</code></pre>
<pre><code>    Estimate Std. Error t value  Pr(&gt;|t|)
Lab   -0.176    0.01391  -12.66 6.337e-23</code></pre>
<p>Here is how to estimate the second model.</p>
<pre class="r"><code>m &lt;- lm(Field ~ -1 + offset(Lab) + Lab, data = pipeline)
summary(m)$coefficients</code></pre>
<pre><code>    Estimate Std. Error t value  Pr(&gt;|t|)
Lab   -0.176    0.01391  -12.66 6.337e-23</code></pre>
<p>And here is how to estimate the third model.</p>
<pre class="r"><code>m &lt;- lm(Field ~ -1 + Lab, data = pipeline)
summary(m)$coefficients</code></pre>
<pre><code>    Estimate Std. Error t value  Pr(&gt;|t|)
Lab    0.824    0.01391   59.25 4.392e-83</code></pre>
<p>Note that the first two models give the same inferences for <span
class="math inline">\(\theta\)</span> (which is <span
class="math inline">\(\beta_1\)</span> in the notation of a linear
model). In the third model the estimated value of <span
class="math inline">\(\gamma\)</span> equals <span
class="math inline">\(1 + \theta\)</span> which we can also see by using
<code>lincon</code>.</p>
<pre class="r"><code>trtools::lincon(m, a = 1, b = -1)</code></pre>
<pre><code>       estimate      se   lower   upper tvalue  df    pvalue
(1),-1   -0.176 0.01391 -0.2036 -0.1484 -12.66 106 6.337e-23</code></pre>
<p>Of course, you can also see this by simply subtracting one from the
estimate of <span class="math inline">\(\gamma\)</span>.</p></li>
<li><p>Plot the model you estimated in the previous problem with the raw
data to show a plot like that given earlier but with a line showing the
estimated expected field measurement as a linear function of laboratory
measurement. You will want to use either the second or third model you
estimated in the previous problem to do this since the first model uses
the difference in the field and laboratory measurements as the response
variable and so the predicted values from that model are not what you
want for plotting purposes.</p>
<p><strong>Solution</strong>: Here is a plot of the second model. I will
just include code to “add” the line to the earlier plot.</p>
<pre class="r"><code>m &lt;- lm(Field ~ -1 + offset(Lab) + Lab, data = pipeline)
d &lt;- data.frame(Lab = c(0,82))
d$yhat &lt;- predict(m, newdata = d)
p &lt;- p + geom_line(aes(y = yhat), data = d)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-39-1.png" width="100%" style="display: block; margin: auto;" /></p></li>
<li><p>Plot the studentized residuals against the predicted values using
either the second or third model you estimated in the first problem. Do
you think that the expected field measurement is a linear function of
the laboratory measurement? Why or why not? Is there any evidence of
heteroscedasticity? Why or why not?</p>
<p><strong>Solution</strong>: Here is my residual plot (fancier than
necessary).</p>
<pre class="r"><code>d &lt;- data.frame(x = predict(m), y = rstudent(m))
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, y = &quot;Studentized Residual&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-40-1.png" width="100%" style="display: block; margin: auto;" />
There does appear to be some evidence of heteroscedasticity due to the
“megaphone” pattern in the residuals. There is also some evidence of
non-linearity, although it is more subtle. Notice how the residuals for
lower predicted values tend to be greater than zero.</p></li>
<li><p>Consider an alternative nonlinear model where <span
class="math display">\[
  E(F_i) = L_i + \theta_1 L_i^{\theta_2}.
\]</span> This implies that the bias of the field measurements is
proportional to some power <span class="math inline">\(\theta_2\)</span>
of the laboratory measurements. Note that the second model you estimated
in the first problem is a special case of this model where <span
class="math inline">\(\theta_2\)</span> = 1. Estimate the model above
using <code>nls</code>. For your starting values you can use the
estimate of <span class="math inline">\(\theta_1\)</span> you obtained
in the first problem and <span class="math inline">\(\theta_2\)</span> =
1, since that was the value you implicitly used for those models which
can be viewed as an approximation to the model above. Show the parameter
estimates and their standard errors using <code>summary</code>, and plot
the model as a curve with the raw data like you did in the second
problem.</p>
<p><strong>Solution</strong>: The nonlinear model can be estimated as
follows.</p>
<pre class="r"><code>m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2,
  data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
summary(m)$coefficients</code></pre>
<pre><code>        Estimate Std. Error t value  Pr(&gt;|t|)
theta1 -0.000521  0.0007712 -0.6756 5.008e-01
theta2  2.396756  0.3435804  6.9758 2.814e-10</code></pre>
<p>Here is a fresh plot with the this estimated model.</p>
<pre class="r"><code>d &lt;- data.frame(Lab = seq(0, 82))
d$yhat &lt;- predict(m, newdata = d)
p &lt;- ggplot(pipeline, aes(x = Lab, y = Field)) + theme_minimal() +
  geom_abline(intercept = 0, slope = 1, linetype = 3) + 
  geom_point() + geom_line(aes(y = yhat), data = d) + 
  labs(x = &quot;Laboratory Measurement&quot;, y = &quot;Field Measurement&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-42-1.png" width="100%" style="display: block; margin: auto;" /></p></li>
<li><p>Plot the standardized residuals against the predicted values
based on the nonlinear model you estimated in the previous problem. You
cannot use <code>rstandard</code> or <code>rstudent</code> with a
<code>nls</code> object, but you can use the <code>nlsint</code>
function from the <strong>trtools</strong> package to produce
standardized residuals. The syntax for a basic plot would something like
the following where <code>m</code> is your model object created using
<code>nls</code>.</p>
<pre class="r"><code>d &lt;- nlsint(m, residuals = TRUE)
plot(d$fit, d$res)</code></pre>
<p>Now consider accounting for any heteroscedasticty in the data by
assuming that the variance of the field measurements is proportional to
some power <span class="math inline">\(p\)</span> of the expected field
measurement so that<br />
<span class="math display">\[
  \text{Var}(F_i) \propto E(Y_i)^p.
\]</span> Use an iteratively weighted least squares algorithm to
estimate the nonlinear model described in the previous problem for
several values of <span class="math inline">\(p\)</span>, starting with
<span class="math inline">\(p\)</span> = 1 and trying increasingly
larger values of increments of 0.5 up to <span
class="math inline">\(p\)</span> = 3. Using residual plots, decide on
what you think is a good value of <span class="math inline">\(p\)</span>
and then show the parameter estimates with standard errors for that
model using <code>summary</code> and give another plot of the residuals
against the predicted values for that model. Also discuss briefly why
you selected that particular value of <span
class="math inline">\(p\)</span>.<a href="#fn16" class="footnote-ref"
id="fnref16"><sup>16</sup></a></p>
<p><strong>Solution</strong>: First here is the residual plot for the
model estimated in the previous problem. As usual I will put more effort
into the plot than is necessary. You could simply use the code I gave
above.</p>
<pre class="r"><code>library(trtools)
d &lt;- nlsint(m, residuals = TRUE)
d &lt;- data.frame(x = d$fit, y = d$res)
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, y = &quot;Studentized Residual&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-44-1.png" width="100%" style="display: block; margin: auto;" />
The apparent non-linearity appears to be resolved by this model, but
heteroscedasticity is still an issue. Next I will try different variance
structures with iteratively least squares to see if this can be
resolved.</p>
<pre class="r"><code>pipeline$w &lt;- 1
for (i in 1:10) {
  m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2, weights = w,
    data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
  pipeline$w &lt;- 1 / predict(m)
}
d &lt;- nlsint(m, residuals = TRUE)
d &lt;- data.frame(x = d$fit, y = d$res)
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, 
    y = &quot;Studentized Residual&quot;) + ggtitle(&quot;p = 1&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-45-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (i in 1:10) {
  m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2, weights = w,
    data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
  pipeline$w &lt;- 1 / predict(m)^1.5
}
d &lt;- nlsint(m, residuals = TRUE)
d &lt;- data.frame(x = d$fit, y = d$res)
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, 
    y = &quot;Studentized Residual&quot;) + ggtitle(&quot;p = 1.5&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-45-2.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (i in 1:10) {
  m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2, weights = w,
    data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
  pipeline$w &lt;- 1 / predict(m)^2
}
d &lt;- nlsint(m, residuals = TRUE)
d &lt;- data.frame(x = d$fit, y = d$res)
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, 
    y = &quot;Studentized Residual&quot;) + ggtitle(&quot;p = 2&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-45-3.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (i in 1:10) {
  m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2, weights = w,
    data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
  pipeline$w &lt;- 1 / predict(m)^2.5
}
d &lt;- nlsint(m, residuals = TRUE)
d &lt;- data.frame(x = d$fit, y = d$res)
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, 
    y = &quot;Studentized Residual&quot;) + ggtitle(&quot;p = 2.5&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-45-4.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (i in 1:10) {
  m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2, weights = w,
    data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
  pipeline$w &lt;- 1 / predict(m)^3
}
d &lt;- nlsint(m, residuals = TRUE)
d &lt;- data.frame(x = d$fit, y = d$res)
p &lt;- ggplot(d, aes(x = x, y = y)) + theme_minimal() + 
  geom_point() + labs(x = &quot;Predicted Value&quot;, 
    y = &quot;Studentized Residual&quot;) + ggtitle(&quot;p = 3&quot;)
plot(p)</code></pre>
<p><img src="hw2-solutions_files/figure-html/unnamed-chunk-45-5.png" width="100%" style="display: block; margin: auto;" />
It is a judgment call, but I would pick <span
class="math inline">\(p\)</span> = 1.5 as it appears to make the
variability of the standardized residuals the most constant. Here are
the inferences for that model.</p>
<pre class="r"><code>for (i in 1:10) {
  m &lt;- nls(Field ~ Lab + theta1 * Lab^theta2, weights = w,
    data = pipeline, start = list(theta1 = -0.18, theta2 = 1))
  pipeline$w &lt;- 1 / predict(m)^1.5
}
summary(m)$coefficients</code></pre>
<pre><code>        Estimate Std. Error t value  Pr(&gt;|t|)
theta1 -0.000353  0.0005442 -0.6488 5.179e-01
theta2  2.487143  0.3608460  6.8925 4.215e-10</code></pre></li>
</ol>
</div>
<div id="anti-inflammatory-hormone-devices-revisited"
class="section level2">
<h2>Anti-Inflammatory Hormone Devices — Revisited</h2>
<p><strong>Note</strong>: This problem is <em>extra credit</em> for
students in Stat 436, but is <em>required</em> for students in Stat
516.</p>
<p>The <code>nls</code> function computationally works very similarly to
the <code>lm</code> function, but the interface is different. The
<code>lm</code> function allows us to specify a model
<em>symbolically</em> via the model formula (i.e., the first argument to
<code>lm</code>), whereas <code>nls</code> requires us to specify the
model <em>mathematically</em>. The <code>nls</code> function can be used
to estimate a linear model. In practice, this is rarely necessary except
maybe in cases where you are using a fairly unusual parameterization of
a linear model that is difficult to express using the model formula
argument to <code>lm</code>. But I think it can be a useful exercise for
the student to use <code>nls</code> to specify a linear model. In this
problem you will use the <code>nls</code> function to replicate several
models for the <code>hormone</code> data from the
<strong>bootstrap</strong> package that were featured in the last
homework assignment.</p>
<p>In the following you are to estimate the model specified with
<code>lm</code> by using <code>nls</code>. To do this I would recommend
that you “decipher” the model from the output of <code>summary</code>
given below, and then write the model case-wise. I would also suggest
you use either the <code>case_when</code> function from the
<strong>dplyr</strong> package or specify indicator variables within the
model itself using the <code>==</code> operator (see the second problem
from the <a href="lecture-02-25-2022.html">in-class exercise on February
25th</a> for an example of doing this with another model — for example,
an indicator variable for Lot A would be specified as
<code>Lot == "A"</code>). I would recommend against using the
<code>ifelse</code> function. If you do this correctly then the output
from <code>summary</code> when applied to the model object created using
<code>nls</code> should match that created by <code>lm</code>. When you
do this make sure that the parameter estimates are in the same order
(this can be controlled by the order you specify the parameter starting
values). Note that since these models are all linear you do not need to
specify good starting values. It is fine to specify them all as zero or
some other number (as long as they are not very large in absolute
value).</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ Lot, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)   23.078      1.962 11.7630 1.887e-11
LotB          -1.011      2.775 -0.3644 7.187e-01
LotC           5.844      2.775  2.1065 4.581e-02</code></pre>
<p><strong>Solution</strong>: The model is <span
class="math inline">\(E(Y_i) = \beta_0 + \beta_1x_{i1} +
\beta_2x_{i2}\)</span> where <span class="math inline">\(x_{i1}\)</span>
and <span class="math inline">\(x_{i2}\)</span> are indicator variables
for lots B and C, respectively. We can write this model case-wise as
<span class="math display">\[
   E(Y_i) =
   \begin{cases}
     \beta_0, &amp; \text{if the $i$-th observation is for Lot A}, \\
     \beta_0 + \beta_1, &amp; \text{if the $i$-th observation is for Lot
B}, \\
     \beta_0 + \beta_2, &amp; \text{if the $i$-th observation is for Lot
C}.
   \end{cases}
\]</span> Here are a couple of different ways we could estimate this
model with <code>nls</code>.</p>
<pre class="r"><code>library(bootstrap)
library(dplyr) # for case_when
m &lt;- nls(amount ~ b0 + b1*(Lot == &quot;B&quot;) + b2*(Lot == &quot;C&quot;),
  data = hormone, start = list(b0 = 0, b1 = 0, b2 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0   23.078      1.962 11.7630 1.887e-11
b1   -1.011      2.775 -0.3644 7.187e-01
b2    5.844      2.775  2.1065 4.581e-02</code></pre>
<pre class="r"><code>m &lt;- nls(amount ~ case_when(
  Lot == &quot;A&quot; ~ b0,
  Lot == &quot;B&quot; ~ b0 + b1,
  Lot == &quot;C&quot; ~ b0 + b2,
), data = hormone, start = list(b0 = 0, b1 = 0, b2 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0   23.078      1.962 11.7630 1.887e-11
b1   -1.011      2.775 -0.3644 7.187e-01
b2    5.844      2.775  2.1065 4.581e-02</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ -1 + Lot, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>     Estimate Std. Error t value  Pr(&gt;|t|)
LotA    23.08      1.962   11.76 1.887e-11
LotB    22.07      1.962   11.25 4.717e-11
LotC    28.92      1.962   14.74 1.583e-13</code></pre>
<p><strong>Solution</strong>: The model is <span
class="math inline">\(E(Y_i) = \beta_1x_{i1} + \beta_2x_{i2} +
\beta_3x_{i3}\)</span> where <span
class="math inline">\(x_{i1}\)</span>, <span
class="math inline">\(x_{i2}\)</span>, and <span
class="math inline">\(x_{i3}\)</span> are indicator variables for lots
A, B, and C, respectively. We can write this model case-wise as <span
class="math display">\[
   E(Y_i) =
   \begin{cases}
     \beta_1, &amp; \text{if the $i$-th observation is for Lot A}, \\
     \beta_2, &amp; \text{if the $i$-th observation is for Lot B}, \\
     \beta_3, &amp; \text{if the $i$-th observation is for Lot C}.
   \end{cases}
\]</span> Here are a couple of different ways we could estimate this
model with <code>nls</code>.</p>
<pre class="r"><code>m &lt;- nls(amount ~ b1*(Lot == &quot;A&quot;) + b2*(Lot == &quot;B&quot;) + b3*(Lot == &quot;C&quot;),
  data = hormone, start = list(b1 = 0, b2 = 0, b3 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b1    23.08      1.962   11.76 1.887e-11
b2    22.07      1.962   11.25 4.717e-11
b3    28.92      1.962   14.74 1.583e-13</code></pre>
<pre class="r"><code>m &lt;- nls(amount ~ case_when(
  Lot == &quot;A&quot; ~ b1,
  Lot == &quot;B&quot; ~ b2,
  Lot == &quot;C&quot; ~ b3,
), data = hormone, start = list(b1 = 0, b2 = 0, b3 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b1    23.08      1.962   11.76 1.887e-11
b2    22.07      1.962   11.25 4.717e-11
b3    28.92      1.962   14.74 1.583e-13</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ Lot:hrs, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept) 35.01572   0.736247  47.560 1.783e-24
LotA:hrs    -0.07728   0.005146 -15.016 2.238e-13
LotB:hrs    -0.05566   0.003142 -17.714 6.696e-15
LotC:hrs    -0.05722   0.007423  -7.709 8.045e-08</code></pre>
<p><strong>Solution</strong>: The model is <span
class="math inline">\(E(Y_i) = \beta_1x_{i1}h_i + \beta_2x_{i2}h_i +
\beta_3x_{i3}h_i\)</span> where <span
class="math inline">\(x_{i1}\)</span>, <span
class="math inline">\(x_{i2}\)</span>, and <span
class="math inline">\(x_{i3}\)</span> are indicator variables for lots
A, B, and C, respectively, and <span class="math inline">\(h_i\)</span>
is the hours for the <span class="math inline">\(i\)</span>-th
observation. We can write this model case-wise as <span
class="math display">\[
   E(Y_i) =
   \begin{cases}
     \beta_0 + \beta_1h_i, &amp; \text{if the $i$-th observation is for
Lot A}, \\
     \beta_0 + \beta_2h_i, &amp; \text{if the $i$-th observation is for
Lot B}, \\
     \beta_0 + \beta_3h_i, &amp; \text{if the $i$-th observation is for
Lot C}.
   \end{cases}
\]</span> Here are a couple of different ways we could estimate this
model with <code>nls</code>.</p>
<pre class="r"><code>m &lt;- nls(amount ~ b0 + b1*(Lot == &quot;A&quot;)*hrs + 
  b2*(Lot == &quot;B&quot;)*hrs + b3*(Lot == &quot;C&quot;)*hrs,
  data = hormone, start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0 35.01572   0.736247  47.560 1.783e-24
b1 -0.07728   0.005146 -15.016 2.238e-13
b2 -0.05566   0.003142 -17.714 6.696e-15
b3 -0.05722   0.007423  -7.709 8.045e-08</code></pre>
<pre class="r"><code>m &lt;- nls(amount ~ case_when(
  Lot == &quot;A&quot; ~ b0 + b1 * hrs,
  Lot == &quot;B&quot; ~ b0 + b2 * hrs,
  Lot == &quot;C&quot; ~ b0 + b3 * hrs,
), data = hormone, start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0 35.01572   0.736247  47.560 1.783e-24
b1 -0.07728   0.005146 -15.016 2.238e-13
b2 -0.05566   0.003142 -17.714 6.696e-15
b3 -0.05722   0.007423  -7.709 8.045e-08</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ Lot + hrs + Lot:hrs, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>             Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept) 33.360055   1.211583 27.5343 5.787e-18
LotB         1.846061   1.612797  1.1446 2.652e-01
LotC         3.833616   1.933112  1.9831 6.058e-02
hrs         -0.068296   0.007272 -9.3911 5.753e-09
LotB:hrs     0.012010   0.008291  1.4486 1.622e-01
LotC:hrs    -0.006222   0.014670 -0.4241 6.758e-01</code></pre>
<p><strong>Solution</strong>: The model is <span class="math display">\[
  E(Y_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3h_i +
\beta_4x_{i1}h_i + \beta_5x_{i2}h_i,
\]</span> where <span class="math inline">\(x_{i1}\)</span> and <span
class="math inline">\(x_{i2}\)</span> are indicator variables for lots B
and C, respectively, and <span class="math inline">\(h_i\)</span> is
hours for the <span class="math inline">\(i\)</span>-th observation. We
can write this model case-wise as <span class="math display">\[
   E(Y_i) =
   \begin{cases}
     \beta_0 + \beta_3h_i, &amp; \text{if the $i$-th observation is for
Lot A}, \\
     \beta_0 + \beta_1 + (\beta_3 + \beta_4)h_i, &amp; \text{if the
$i$-th observation is for Lot B}, \\
     \beta_0 + \beta_2 + (\beta_3 + \beta_5)h_i, &amp; \text{if the
$i$-th observation is for Lot C}.
   \end{cases}
\]</span> Here are a couple of different ways we could estimate this
model with <code>nls</code>.</p>
<pre class="r"><code>m &lt;- nls(amount ~ b0 + b1*(Lot == &quot;B&quot;) + b2*(Lot == &quot;C&quot;) + b3*hrs + 
  b4*(Lot == &quot;B&quot;)*hrs + b5*(Lot == &quot;C&quot;)*hrs,
  data = hormone, start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0))
summary(m)$coefficients</code></pre>
<pre><code>    Estimate Std. Error t value  Pr(&gt;|t|)
b0 33.360055   1.211583 27.5343 5.787e-18
b1  1.846061   1.612797  1.1446 2.652e-01
b2  3.833616   1.933112  1.9831 6.058e-02
b3 -0.068296   0.007272 -9.3911 5.753e-09
b4  0.012010   0.008291  1.4486 1.622e-01
b5 -0.006222   0.014670 -0.4241 6.758e-01</code></pre>
<pre class="r"><code>m &lt;- nls(amount ~ case_when(
  Lot == &quot;A&quot; ~ b0 + b3*hrs,
  Lot == &quot;B&quot; ~ b0 + b1 + (b3 + b4)*hrs,
  Lot == &quot;C&quot; ~ b0 + b2 + (b3 + b5)*hrs,
), data = hormone, start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0))
summary(m)$coefficients</code></pre>
<pre><code>    Estimate Std. Error t value  Pr(&gt;|t|)
b0 33.360055   1.211583 27.5343 5.787e-18
b1  1.846061   1.612797  1.1446 2.652e-01
b2  3.833616   1.933113  1.9831 6.058e-02
b3 -0.068296   0.007272 -9.3911 5.753e-09
b4  0.012010   0.008291  1.4486 1.622e-01
b5 -0.006222   0.014670 -0.4241 6.758e-01</code></pre></li>
<li><p>Estimate the following model using <code>nls</code> and show the
estimates using <code>summary</code>.</p>
<pre class="r"><code>m &lt;- lm(amount ~ -1 + Lot + Lot:hrs, data = bootstrap::hormone)
summary(m)$coefficients</code></pre>
<pre><code>         Estimate Std. Error t value  Pr(&gt;|t|)
LotA     33.36006   1.211583  27.534 5.787e-18
LotB     35.20612   1.064509  33.073 1.340e-19
LotC     37.19367   1.506316  24.692 5.341e-17
LotA:hrs -0.06830   0.007272  -9.391 5.753e-09
LotB:hrs -0.05629   0.003982 -14.136 3.361e-12
LotC:hrs -0.07452   0.012740  -5.849 8.330e-06</code></pre>
<p><strong>Solution</strong>: The model is <span class="math display">\[
  E(Y_i) = \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} +
\beta_4x_{i1}h_i +
  \beta_5x_{i2}h_i + \beta_5x_{i3}h_i,
\]</span> where <span class="math inline">\(x_{i1}\)</span>, <span
class="math inline">\(x_{i2}\)</span>, and <span
class="math inline">\(x_{i3}\)</span> are indicator variables for lots
A, B, and C, respectively, and <span class="math inline">\(h_i\)</span>
is hours for the <span class="math inline">\(i\)</span>-th observation.
We can write this model case-wise as <span class="math display">\[
   E(Y_i) =
   \begin{cases}
     \beta_1 + \beta_4h_i, &amp; \text{if the $i$-th observation is for
Lot A}, \\
     \beta_2 + \beta_5h_i, &amp; \text{if the $i$-th observation is for
Lot B}, \\
     \beta_3 + \beta_6h_i, &amp; \text{if the $i$-th observation is for
Lot C}.
   \end{cases}
\]</span> Here are a couple of different ways we could estimate this
model with <code>nls</code>.</p>
<pre class="r"><code>m &lt;- nls(amount ~ b1*(Lot == &quot;A&quot;) + b2*(Lot == &quot;B&quot;) + b3*(Lot == &quot;C&quot;) + 
  b4*(Lot == &quot;A&quot;)*hrs + b5*(Lot == &quot;B&quot;)*hrs + b6*(Lot == &quot;C&quot;)*hrs,
  data = hormone, start = list(b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0, b6 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b1 33.36006   1.211583  27.534 5.787e-18
b2 35.20612   1.064509  33.073 1.340e-19
b3 37.19367   1.506316  24.692 5.341e-17
b4 -0.06830   0.007272  -9.391 5.753e-09
b5 -0.05629   0.003982 -14.136 3.361e-12
b6 -0.07452   0.012740  -5.849 8.330e-06</code></pre>
<pre class="r"><code>m &lt;- nls(amount ~ case_when(
  Lot == &quot;A&quot; ~ b1 + b4*hrs,
  Lot == &quot;B&quot; ~ b2 + b5*hrs,
  Lot == &quot;C&quot; ~ b3 + b6*hrs,
), data = hormone, start = list(b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0, b6 = 0))
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b1 33.36006   1.211583  27.534 5.787e-18
b2 35.20612   1.064509  33.073 1.340e-19
b3 37.19367   1.506316  24.692 5.341e-17
b4 -0.06830   0.007272  -9.391 5.753e-09
b5 -0.05629   0.003982 -14.136 3.361e-12
b6 -0.07452   0.012740  -5.849 8.330e-06</code></pre></li>
</ol>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Christensen, M. G., Teicher, H. B., &amp; Streibig, J.
C. (2003). Linking fluorescence induction curve and biomass in herbicide
screening. <em>Pest Management Science</em>, <em>59</em>, 1303–1310.<a
href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Note that <span class="math inline">\(e^x\)</span> is
the <a
href="https://en.wikipedia.org/wiki/Exponential_function">exponential
function</a> where <span class="math inline">\(e \approx\)</span> 2.718
is <a
href="https://en.wikipedia.org/wiki/E_(mathematical_constant)">Euler’s
number</a>. This function is also written as <span
class="math inline">\(\exp(x)\)</span>, and in R it is written as
<code>exp(x)</code>.<a href="#fnref2"
class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Because <span class="math inline">\(\log(0)\)</span> is
not defined, <span class="math inline">\(E(M)\)</span> is not defined
<em>mathematically</em> if the dose equals <span
class="math inline">\(\alpha\)</span>. But computers will typically
evaluate <span class="math inline">\(\log(0)\)</span> as <span
class="math inline">\(-\infty\)</span> because of the one-sided limit
<span class="math inline">\(\lim_{x \to 0+} \log(x) = -\infty\)</span>.
And for a similar reason <span
class="math inline">\(e^{-\infty}\)</span> is evaluated as 0 by
computers.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This can be seen by differentiation to show that <span
class="math display">\[
  \left.\frac{\partial E(M)}{\partial d}\right|_{d = \alpha} =
\frac{-\beta(\delta-\gamma)}{4\alpha}.
\]</span><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Jevons, W. S. (1868). On the condition of the metallic
currency of the United Kingdom, with reference to the question of
international coinage. <em>Journal of the Statistical Society of
London</em>, <em>31</em>, 426–464.<a href="#fnref5"
class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>There are a couple of things to note here. One is the
use of <code>set.seed</code>. The simulated data are generated using the
random number generators in R. This initializes the state of the random
number generator so that anyone using this code would produce the
<em>same</em> random numbers. The other thing to note is the use of
<code>dplyr::select</code>. The function <code>select</code> from the
<strong>dplyr</strong> package is used to select certain variables from
a data frame (and thus deselect others). But there is a function of the
same name in the <strong>MASS</strong> package that does something very
different. The <strong>MASS</strong> package is frequently loaded with
other packages, so to avoid potential conflicts I will often use
<code>dplyr::select</code> out of habit to avoid problems.<a
href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>For an example of how to specify comparisons between one
level and the other levels with functions in the
<strong>emmeans</strong> package, see the example from the lecture on <a
href="lecture-02-18-2022.html">February 18</a> where I use the
<code>trt.vs.ctrl</code> contrast method with the <code>contrast</code>
function from the <strong>emmeans</strong> package (not the
<strong>trtools</strong> package).<a href="#fnref7"
class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Both the <code>contrast</code> function from the
<strong>trtools</strong> package and functions from the
<strong>emmeans</strong> package should give the same estimates and
standard errors. They will give somewhat different confidence intervals
and p-values, however, because of how the two functions compute the
degrees of freedom by default. They can be brought into agreement by
using some extra options, but for the purpose of this problem that is
not necessary.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>The data are featured in Bliss, C. I. (1935). The
calculation of the dosage-mortality curve. <em>Annals of Applied
Biology</em>, <em>22</em>, 134–167. But the original source is Strand,
A. L. (1930). Measuring the toxicity of insect fumigants. <em>Industrial
and Engineering Chemistry: Analytical Edition</em>, <em>2</em>, 4–8.<a
href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Note that <span class="math inline">\(e^x\)</span> is
the <a
href="https://en.wikipedia.org/wiki/Exponential_function">exponential
function</a> where <span class="math inline">\(e \approx\)</span> 2.718
is <a
href="https://en.wikipedia.org/wiki/E_(mathematical_constant)">Euler’s
number</a>. This function is also written as <span
class="math inline">\(\exp(x)\)</span>, and in R it is written as
<code>exp(x)</code>.<a href="#fnref10"
class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Note that when using <code>ggplot</code> the
<em>order</em> that you specify the various geometric objects matters.
For example, if <code>geom_line</code> appears before
<code>geom_label_repel</code> then the point labels will be shown in
front of rather than behind the curve.<a href="#fnref11"
class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>The nature of the observational units and the
measurement of the number of defects is not clear. The observational
units may be select portions of the pipeline, but it is not clear if
these units were removed from the pipeline and brought to a laboratory,
or if only the data from the field was brought back to the laboratory
for more thorough analysis. Also since the laboratory values are not all
integers these measurements might be the number of defects per unit area
or volume. Finally, note that in the help file (see
<code>?pipeline</code>) the <code>Lab</code> variable is incorrectly
labeled as “Number of defects measured in the field.”<a href="#fnref12"
class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>I have given several examples in lecture of how to
estimate a model with <code>lm</code> that does not include an
constant/intercept term.<a href="#fnref13"
class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>From a design perspective, <span
class="math inline">\(L_i\)</span> may not be fixed since the values are
not necessarily selected by the researchers. But in regression we
frequently regard all variables except for the response variable as
fixed. Technically what we are doing is <em>conditioning</em> on the
values of the explanatory variables, so even if they are random we are
only considering the distribution of the response variable
<em>given</em> those values of the explanatory variables.<a
href="#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>An offset is an explanatory variable that has a <span
class="math inline">\(\beta_j\)</span> fixed at one. This can be done by
using <code>offset(variable)</code> in your model formula. For example,
consider the model <span class="math display">\[
  E(V_i) = \beta_0 + \beta_1g_i + \beta_2h_i,
\]</span> where <span class="math inline">\(V_i\)</span> is tree volume,
<span class="math inline">\(g_i\)</span> is girth, and <span
class="math inline">\(h_i\)</span> is height. We can estimate this model
as follows.</p>
<pre class="r"><code>m &lt;- lm(Volume ~ Girth + Height, data = trees)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept) -57.9877     8.6382  -6.713 2.750e-07
Girth         4.7082     0.2643  17.816 8.223e-17
Height        0.3393     0.1302   2.607 1.449e-02</code></pre>
<p>But if I wanted <span class="math inline">\(\beta_1\)</span> = 1 then
I could do the following.</p>
<pre class="r"><code>m &lt;- lm(Volume ~ offset(Girth) + Height, data = trees)
summary(m)$coefficients</code></pre>
<pre><code>            Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)  -80.935    23.6206  -3.426 0.0018484
Height         1.288     0.3097   4.157 0.0002608</code></pre>
<p>Note that no inferences for <span
class="math inline">\(\beta_1\)</span> are given by <code>summary</code>
because we are assuming we know it is one so there is nothing to
infer.<a href="#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>There is not necessarily a correct value of <span
class="math inline">\(p\)</span> here, although some values may be
clearly better than others.<a href="#fnref16"
class="footnote-back">↩︎</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
