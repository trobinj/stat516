---
output:
  html_document: 
    theme: readable
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "04-28-2023"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
output:
  html_document: 
    theme: readable
  pdf_document: default
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, out.width = "100%", fig.align = "center", cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"), comment = "")
```

```{r packages, echo = FALSE}
library(tidyverse)
```

```{r options, echo = FALSE}
options(digits = 4, width = 100)
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`

## Nonlinear Regression With Random Effects

**Example**: The model we specified for the `Sitka` data can be written as
$$
  E(Y_{ij}) = \beta_0 + \beta_1 o_{ij} + \beta_2 w_{ij} + \beta_3 o_{ij}w_{ij} + \delta_i + \gamma_iw_{ij},
$$
where $o_{ij}$ is an indicator for if the observation is from the ozone treatment condition and $w_{ij}$ is weeks. We can also write this model as
$$
  E(Y_{ij}) = \underbrace{\beta_0 + \beta_1 o_{ij} + \delta_i}_{\beta_{0ij}} + \underbrace{(\beta_2 + \beta_3o_{ij} + \gamma_i)}_{\beta_{1ij}}w_i,
$$
or 
$$
  E(Y_{ij}) = \beta_{0ij} + \beta_{1ij}w_{ij},
$$
to show that the model assumes a linear relationship between expected size and weeks, but where the "intercept" $\beta_{0ij}$ depends on the treatment condition and tree, and the "slope" $\beta_{1ij}$ depends on the treatment condition and tree because
\begin{align*}
  \beta_{0ij} & = \beta_0 + \beta_1o_{ij} + \delta_i \\ 
  \beta_{1ij} & = \beta_2 + \beta_3o_{ij} + \gamma_i.
\end{align*}
Models with random effects written in this way are sometimes called "random coefficient" models. The coefficients $\beta_{0ij}$ and $\beta_{1ij}$ are random (due to $\delta_i$ and $\gamma_i$) but may also depend on one or more explanatory variables (such as treatment condition via $o_{ij}$).

The `nlme` function from the **nlme** package can estimate a linear or nonlinear regression model with random coefficients (assuming a normally-distributed response variable and random parameters). We estimated a model for the `Sitka` data as follows.
```{r, message = FALSE}
library(MASS)
library(lme4)
m <- lmer(exp(size) ~ treat * I(Time/7) + (1 + I(Time/7) | tree),
  data = Sitka, REML = FALSE)
summary(m)$coefficients
```
I am using `REML = FALSE` to use maximum likelihood rather than *restricted* maximum likelihood (REML) for estimation so that we can compare the results with `nlme`, which only uses maximum likelihood.
```{r}
library(nlme)
m <- nlme(exp(size) ~ b0 + b1 * I(Time/7),
  fixed = b0 + b1 ~ treat, 
  random = b0 + b1 ~ 1 | tree,
  start = c(0,0,0,0), data = Sitka)
summary(m)
```
The `nlme` function is like `nls` in that it needs starting values for the (fixed) parameters, but since the model is linear we do not need particularly good starting values. 

**Example**: Now consider a *nonlinear* regression model with random effects for the `Loblolly` data that come with R.
```{r}
head(Loblolly)
p <- ggplot(Loblolly, aes(x = age, y = height)) + 
  geom_point(size = 0.5) + facet_wrap(~ Seed, ncol = 7) + 
  ylab("Height (ft)") + xlab("Age (yr)") + theme_minimal()
plot(p)
```
Suppose we want to estimate the nonlinear growth model
$$
  E(H) = \theta_1 + (\theta_2 - \theta_1)e^{-a\log(2)/\theta_3},
$$
where $H$ and $a$ are height and age, respectively, $\theta_1$ is the asymptote as $a \rightarrow \infty$, and $\theta_2$ is an "intercept" parameter, and $\theta_3$ is the age at which the tree is half way between $E(H) = \theta_2$ and $E(H) = \theta_1$. To allow for differences between trees with respect to $\theta_1$ and $\theta_3$ (but not $\theta_2$) we could write the model as
$$
  E(H_{ij}) = \theta_{1i} + (\theta_{2} - \theta_{1i})e^{-a_{ij}\log(2)/\theta_{3i}},
$$
where $H_{ij}$ and $a_{ij}$ are now the height and age of the $j$-th observation of the $i$-th tree.
```{r}
m <- nlme(height ~ t1 + (t2 - t1) * exp(-age * log(2)/t3), 
  fixed = t1 + t2 + t3 ~ 1, 
  random = t1 + t3 ~ 1 | Seed,
  start = c(t1 = 100, t2 = 0, t3 = 15), 
  data = Loblolly)
summary(m)
```
We can plot the estimated growth curves (both per tree and average) as follows.
```{r}
d <- expand.grid(age = seq(0, 50, length = 100), Seed = unique(Loblolly$Seed))

d$yhat.ind <- predict(m, newdata = d, level = 1) # individual tree
d$yhat.avg <- predict(m, newdata = d, level = 0) # average tree

p <- ggplot(Loblolly, aes(x = age, y = height)) + 
  geom_line(aes(y = yhat.ind), data = d, linetype = 3) +
  geom_line(aes(y = yhat.avg), data = d) + 
  geom_point(size = 0.5) + facet_wrap(~ Seed, ncol = 7) + 
  ylab("Height (ft)") + xlab("Age (yr)") + theme_minimal()
plot(p)
```

**Example**: Here are some data from an experiment using a randomized block design on the effect of weed density on yield loss of sunflowers.
```{r}
yieldloss <- read.csv("https://raw.githubusercontent.com/OnofriAndreaPG/agroBioData/master/YieldLossB.csv", header = T)

p <- ggplot(yieldloss, aes(x = density, y = yieldLoss)) + theme_minimal() +
  labs(x = "Density (weeds per square meter)", y = "Yield Loss") + 
  geom_line(aes(group = block), alpha = 0.25) + geom_point(alpha = 0.5)
plot(p)
```
The model suggested for these data has the same form as the Michaelis-Menten model, but with random effects to account for the effect of block.
```{r}
m <- nlme(yieldLoss ~ alpha * density / (beta + density), 
   fixed = list(alpha ~ 1, beta ~ 1),
   random = alpha + beta ~ 1 | block,
   start = c(alpha = 60, beta = 30), data = yieldloss)

summary(m)$tTable
intervals(m)

d <- expand.grid(density = seq(0, 100, length = 100), block = 1:15)
d$yhat <- predict(m, newdata = d)

p <- ggplot(yieldloss, aes(x = density, y = yieldLoss)) + theme_minimal() +
  labs(x = "Density (weeds per square meter)", y = "Yield Loss") + 
  geom_point() + geom_line(aes(y = yhat), data = d) + 
  facet_wrap(~ block, ncol = 5)
plot(p)

p <- ggplot(yieldloss, aes(x = density, y = yieldLoss)) + theme_minimal() +
  labs(x = "Density (weeds per square meter)", y = "Yield Loss") + 
  geom_point(alpha = 0.5) + geom_line(aes(y = yhat, group = block), alpha = 0.25, data = d)
plot(p)
```

**Example**: The data frame `Soybean` from the **nlme** package has data from an experiment looking at soybean growth.
```{r, fig.height = 12}
head(Soybean)
p <- ggplot(Soybean, aes(x = Time, y = weight)) + 
  geom_point() + facet_wrap(~ Plot, ncol = 8) + 
  labs(x = "Time (days after planting)", 
    y = "Weight (average leaf weight per plant in grams)") +
  theme_minimal()
plot(p)
```
```{r}
p <- ggplot(Soybean, aes(x = Time, y = weight)) + 
  geom_point(size = 1) + facet_wrap(~ Variety) + 
  geom_line(aes(group = Plot), linewidth = 0.1) + 
  labs(x = "Time (days after planting)",
    y = "Weight (average leaf weight per plant in grams)") +
  theme_minimal()
plot(p)
```
Consider a logistic growth model which can be written as
$$
  E(W) = \frac{\theta_1}{1 + e^{-(t-\theta_2)/\theta_3}},
$$
where $\theta_1$ is the asymptote as $t \rightarrow \infty$, $\theta_2$ is the time at which the expected weight is $\theta_1/2$, and $\theta_3$ is inversely related to the steepness of the curve at $\theta_2$. We could assume that each parameter varies by plot, and is also affected by variety as follows.
```{r, warning = FALSE}
m <- nlme(weight ~ theta1 / (1 + exp(-(Time - theta2) / theta3)), data = Soybean,
 fixed = theta1 + theta2 + theta3 ~ Variety, 
 random = theta1 + theta2 + theta3 ~ 1 | Plot,
 start = c(20, 0, 60, 0, 10, 0),
 control = nlmeControl(msMaxIter = 1000))
summary(m)$tTable
```
In more complex models getting the inferences you want from a `nlme` object can be a bit tricky. Functions like `contrast` and `emmeans` will not work with a `nlme` object. But you can use the `lincon` function, although you need to tell it how to extract the parameter estimates from `nlme` (it needs to use the `fixef` function). Here we can get results like those returned by `summary`.
```{r}
trtools::lincon(m, fest = fixef)
```
The estimate of mean $\theta_1$ parameter for the F variety is given by `theta1.(Intercept)`. But the estimate of the mean $\theta_1$ parameter for the P variety is the sum of the `theta1.(Intercept)` and `theta1.VarietyP` parameters. This can be obtained as follows.
```{r}
trtools::lincon(m, a = c(1,1,0,0,0,0), fest = fixef)
```
Again we can plot this model as we did with the `Loblolly` data/model, although setting up the data frame is a little more complicated because plots and variety are not crossed.
```{r}
library(dplyr)
library(tidyr)
d <- Soybean %>% dplyr::select(Plot, Variety) %>% unique() %>% 
  group_by(Plot, Variety) %>% tidyr::expand(Time = seq(14, 84, length = 100))
d$yhat.ind <- predict(m, newdata = d, level = 1)
d$yhat.avg <- predict(m, newdata = d, level = 0)

p <- ggplot(Soybean, aes(x = Time, y = weight)) + 
  geom_line(aes(y = yhat.ind, group = Plot), data = d, alpha = 0.125) + 
  geom_line(aes(y = yhat.avg), data = d) + 
  geom_point(size = 1, alpha = 0.25) + facet_wrap(~ Variety) + 
  labs(x = "Time (days after planting)",
    y = "Weight (average leaf weight per plant in grams)") + theme_minimal()
plot(p)
```

## Crossed Random Effects

Crossed random effects might be specified when two (or more) factors modeled as having random effects are crossed (i.e., having a "factorial design" structure). 

**Example**: Six samples of penicillin were tested using 24 plates. The response varaible was the diameter of the zone of inhibition of the growth of a bacteria.
```{r}
p <- ggplot(Penicillin, aes(x = plate, y = diameter)) + 
  geom_point(size = 1) + facet_wrap(~ sample, ncol = 6) + 
  coord_flip() + theme_minimal() + 
  theme(axis.text.x = element_text(size = 5)) + 
  labs(y = "Diameter (mm)", x = "Plate")
plot(p)
```
Let $Y_{ij}$ denote the diameter of inhibition for the $i$-th sample ($i = 1, 2, \dots, 6$) and the $j$-th plate ($j = 1, 2, \dots, 26$). A model might be
$$
  E(Y_{ij}) = \beta_0 + \delta_i + \zeta_j \ \ \text{or} \ \ 
  Y_{ij} = \beta_0 + \delta_i + \zeta_j + e_{ij}.
$$
where $\delta_i$ and $\zeta_j$ are sample-specific and plate-specific effects, respectively. Here we will model both as random effects, each with an independent normal distribution. Note that we don't have any fixed effects.
```{r}
m <- lmer(diameter ~ (1 | plate) + (1 | sample), data = Penicillin)
summary(m)
```

**Example**: Consider the following data from a study that examined mating success with multiple combinations of male and female salamanders.
```{r}
library(hglm.data)
data(salamander)
head(salamander, 12)
```
The question here is how the combination female and male salamanders in terms of population (W = "White Side", R = "Rough Butt") affects mating success, while accounting for individual differences in the salamanders themselves. 
```{r}
salamander$Cross <- relevel(salamander$Cross, ref = "WR")
m <- glmer(Mate ~ Cross + (1 | Male) + (1 | Female),
  family = binomial, data = salamander)
summary(m)
```

## Nested Random Effects

Nested factors occur when they form a *hierarchical* structure. For example, in the `Sitka` data the levels of `tree` are nested within levels of `treat` (i.e., ozone or control), and in the `Soybean` data the levels of `Plot` are nested within levels of `Variety`. Nested random effects when the levels of one factor, modeled as a random effect, are nested within the levels of another factor that is also modeled as a random effect.

**Example**: The `Pastes` data frame from the **lme4** package is from a study of the strength of chemical pastes. Paste was delivered in a cask, and there were three casks per batch, and 10 batches. Two tests were run per cask. 
```{r}
head(Pastes, 12)
```
Note that levels of `cask` are not the same between batches --- i.e., cask `a` in batch `A` is *not* the same as cask `a` in batch `B`, for example. The `sample` variable was created to identify a particular `cask`. We could model these data as
$$
  E(Y_{ijk}) = \beta_0 + \delta_i + \zeta_{ij},
$$
where $Y_{ijk}$ is $k$-th ($k=1,2$) test a paste from the $j$-th ($j=1,2,3$) cask from the $i$-th batch ($i=1,2,\dots,10$). So here $\delta_i$ is the effect of the $i$-th batch, and $\zeta_{ij}$ is the effect of the $j$-th cask from the $i$-th batch. 
```{r}
m <- lmer(strength ~ (1|batch) + (1|cask:batch), data = Pastes)
summary(m)
```
Note that you could use `sample` in place of `cask:batch`.

**Example**: Consider the following data from an experiment on the treatment of arthritis. 
```{r}
myhips <- faraway::hips %>% pivot_longer(cols = c(fbef,faft,rbef,raft), 
   names_to = "obs", values_to = "angle") %>%
   mutate(time = rep(c("before","after"), n()/2)) %>%
   mutate(variable = rep(c("flexion","flexion","rotation","rotation"), n()/4)) %>%
   mutate(time = factor(time, levels = c("before","after")))
head(myhips,10)
p <- ggplot(subset(myhips, variable == "flexion"), aes(x = time, y = angle, fill = side)) + 
   theme_minimal() + geom_dotplot(binaxis = "y", stackdir = "center", binwidth = 1, 
      position = position_dodge(width = 0.5)) + facet_wrap(~ grp) + 
   labs(x = "Observation Time", y = "Hip Rotation Angle", fill = "Hip")
plot(p)
```
Here for each of two response variables (flexion and rotation) we have two observations (before and after) for each hip (side) for each person. Here we specify a random effect for each person and a random effect for each hip within each person. Here we will consider the rotation response variable. Note that I am assuming that there is not, on average, an effect of left versus right side.
```{r}
m <- lmer(angle ~ time * grp + (1|person) + (1|person:side), 
   subset = variable == "rotation", data = myhips)
summary(m)
```
What is the estimated change in expected rotation from before to after treatment in each group?
```{r}
trtools::contrast(m,
  a = list(time = "after", grp = c("control","treat")),
  b = list(time = "before", grp = c("control","treat")),
  cnames = c("control","treat"))
```
The `icc_specs` function from the **specr** package can be used to produce estimates concerning the "variance components" (i.e., the variance due to person, side, and error).
```{r}
specr::icc_specs(m)
```


