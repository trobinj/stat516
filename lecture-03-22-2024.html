<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Friday, Mar 22</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics 436/516</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="lectures.html">Lectures</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Friday, Mar 22</h1>

</div>


<p>You can also download a <a href="lecture-03-22-2024.pdf">PDF</a> copy
of this lecture.</p>
<!-- Comment to add: The emmeans functions specify df = Inf by default when using quasi. But we can override this with df = x argument.  -->
<div id="over-dispersion" class="section level2">
<h2>Over-dispersion</h2>
<p>Over-dispersion can occur for generalized linear models that assume a
Poisson or binomial distribution for the response variable.</p>
<p>When we specify a distribution in a generalized linear model, what we
are actually specifying is the <em>variance structure</em> <span
class="math display">\[
  \text{Var}(Y_i) = \phi V[E(Y_i)],
\]</span> where <span class="math inline">\(\phi\)</span> is the
<em>dispersion parameter</em> and <span class="math inline">\(V\)</span>
is the <em>variance function</em>.</p>
<p><em>Over-dispersion</em> is when <span class="math display">\[
  \text{Var}(Y_i) &gt; \phi V[E(Y_i)],
\]</span> and <em>underdispersion</em> is when <span
class="math display">\[
  \text{Var}(Y_i) &lt; \phi V[E(Y_i)].
\]</span> Over-dispersion is fairly common in practice, but
under-dispersion is relatively rare.</p>
<div id="over-dispersion-in-poisson-regression" class="section level3">
<h3>Over-dispersion in Poisson Regression</h3>
<p>If <span class="math inline">\(Y_i\)</span> has a <em>Poisson</em>
distribution, then <span class="math display">\[
    \text{Var}(Y_i) = E(Y_i),
\]</span> so that it is implicitly assumed that <span
class="math inline">\(\phi = 1\)</span> and <span
class="math inline">\(V(z) = z\)</span>. Over-dispersion occurs if <span
class="math display">\[
    \text{Var}(Y_i) &gt; E(Y_i).
\]</span></p>
</div>
<div id="over-dispersion-in-binomial-regression" class="section level3">
<h3>Over-dispersion in Binomial Regression</h3>
<p>If <span class="math inline">\(C_i\)</span> has a <em>binomial</em>
distribution, and <span class="math inline">\(Y_i = C_i/m_i\)</span>,
then <span class="math display">\[
    \text{Var}(Y_i) = E(Y_i)[1-E(Y_i)]/m_i,
\]</span> so that it is implicitly assumed that <span
class="math inline">\(\phi = 1\)</span> and <span
class="math inline">\(V(z) = z(1-z)/m_i\)</span>. over-dispersion occurs
if <span class="math display">\[
    \text{Var}(Y_i) &gt; E(Y_i)[1-E(Y_i)]/m_i.
\]</span></p>
<p>In general, failing to account for over-dispersion (or a
misspefication of the variance structure in general) may yield incorrect
standard errors (usually too small in the case of over-dispersion),
leading to incorrect test statistics and confidence intervals.</p>
</div>
</div>
<div id="causes-of-over-dispersion" class="section level2">
<h2>Causes of Over-dispersion</h2>
<ol style="list-style-type: decimal">
<li><p>Wrong assumed distribution for the response variable.</p></li>
<li><p>Unobserved explanatory variables that vary over
observations.</p></li>
</ol>
<p>Note: A misspecified <em>mean structure</em> (e.g., failing to
transform an explanatory variable or omitting a strong interaction) may
appear as overdisperson.</p>
<p><strong>Example</strong>: Consider the following data from an
experiment that investigated the proportion of rotifers of two species
remaining in suspension in different solution densities after being put
into a centrifuge.</p>
<pre class="r"><code>myrotifer &lt;- trtools::rotifer

p &lt;- ggplot(myrotifer, aes(x = density, y = y/total)) + 
  geom_point() + facet_wrap(~species) + 
  labs(y = &quot;Proportion of Rotifers\n Remaining in Suspension&quot;,
    x = &quot;Density of Solution&quot;) + theme_minimal()
plot(p)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" />
Logistic regression might be a reasonable model here.</p>
<pre class="r"><code>m &lt;- glm(cbind(y, total - y) ~ species * density,
  family = binomial, data = myrotifer)

d &lt;- expand.grid(species = c(&quot;kc&quot;,&quot;pm&quot;), density = seq(1.02, 1.07, length = 100))
d$yhat &lt;- predict(m, newdata = d, type = &quot;response&quot;)

p &lt;- p + geom_line(aes(y = yhat), data = d)
plot(p)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" />
Do these data exhibit over-dispersion for this model?</p>
</div>
<div id="detection-of-over-dispersion" class="section level2">
<h2>Detection of Over-dispersion</h2>
<p>Standardized residuals can be used to detect over-dispersion. There
are several types for GLMs.</p>
<ol style="list-style-type: decimal">
<li><p>Pearson residuals. Pearson residuals are defined as <span
class="math display">\[
\frac{y_i - \hat{y}_i}{\sqrt{\widehat{\text{Var}}(Y_i)}}.
\]</span><br />
Dividing a Pearson residual by another term to account for the variance
<span class="math inline">\(\hat{y}_i\)</span> creates a
<em>standardized</em> Pearson residual. These are obtained using
<code>rstandard(m, type = "pearson")</code>.</p></li>
<li><p>Deviance residuals. The residual deviance can be decomposed into
a per-observation contribution so that <span class="math inline">\(D =
\sum_{i=1}^n d_i\)</span>. Then the residual deviance is defined as
<span class="math display">\[
  \text{sign}(y_i - \hat{y}_i)\sqrt{d_i},
\]</span> where <span class="math display">\[
  \text{sign}(z) =
  \begin{cases}
  1, &amp; \text{if $z &gt; 0$}, \\
  0, &amp; \text{if $z = 0$}, \\
-1, &amp; \text{if $z &lt; 0$}.
  \end{cases}
\]</span> Dividing a deviance residual by another term to account for
the variance <span class="math inline">\(\hat{y}_i\)</span> creates a
<em>standardized</em> deviance residual. These are obtained using
<code>rstandard(m, type = "deviance")</code>. A numerical approximation
to these residuals obtained when omitting the observation can be
obtained using <code>rstudent(m)</code>.</p></li>
<li><p>Studentized residuals. The function <code>rstudent</code> will
produce <em>approximate</em> studentized residuals for GLMs.</p></li>
</ol>
<p>Comment: If the model is correct the residuals <em>might</em> be
approximately normally distributed with a mean of zero and standard
deviation of one (i.e., “standard normal”), so an excess of values
greater than two (in absolute value) may indicate over-dispersion or
some other problem with the model. But with very <em>coarse</em> data
(e.g., very small counts in a Poisson regression model or proportions
with small <span class="math inline">\(m_i\)</span> in a logistic
regression model), the distribution of these residuals is not
approximately normal.</p>
<p><strong>Example</strong>: Let’s look at the residuals for the
<code>rotifer</code> model.</p>
<pre class="r"><code>par(mfcol = c(1,3))
plot(predict(m), rstandard(m, type = &quot;pearson&quot;), ylim = c(-10, 7), main = &quot;Pearson&quot;)
abline(h = c(-2,2), lty = 2)
plot(predict(m), rstandard(m, type = &quot;deviance&quot;), ylim = c(-10, 7), main = &quot;Deviance&quot;)
abline(h = c(-2,2), lty = 2)
plot(predict(m), rstudent(m), ylim = c(-10, 7), main = &quot;Studentized&quot;)
abline(h = c(-2,2), lty = 2)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" />
Is there an explanation of the over-dispersion?</p>
<p>Another metric is to compare the residual deviance to the residual
degrees of freedom in a GLM with a response variable with either a
Poisson or binomial distribution. If the model is (approximately)
correct then the ratio of the residual deviance to the residual degrees
of freedom is approximately one.</p>
<p><strong>Example</strong>: Consider the residual deviance and residual
degrees of freedom for the rotifer model.</p>
<pre class="r"><code>summary(m)</code></pre>
<pre><code>
Call:
glm(formula = cbind(y, total - y) ~ species * density, family = binomial, 
    data = myrotifer)

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -114.35       4.03  -28.35   &lt;2e-16 ***
speciespm             4.63       6.60    0.70     0.48    
density             108.75       3.86   28.19   &lt;2e-16 ***
speciespm:density    -3.08       6.33   -0.49     0.63    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 3180.99  on 39  degrees of freedom
Residual deviance:  434.02  on 36  degrees of freedom
AIC: 596.6

Number of Fisher Scoring iterations: 5</code></pre>
<p>If the model is correct and there is no over-dispersion, the residual
deviance has approximate a <span class="math inline">\(\chi^2\)</span>
distribution with degrees of freedom equal to the residual degrees of
freedom. We can use this as an informal test for over-dispersion.</p>
<pre class="r"><code>f &lt;- function(x) dchisq(x, 36)
curve(f, from = 0, to = 500, n = 1000) </code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>1 - pchisq(434.02, df = 36)</code></pre>
<pre><code>[1] 0</code></pre>
<p>Residuals are more informative, but the residual deviance is a quick
way to check to see if over-dispersion may be an issue.</p>
<p>Note: For logistic regression, over-dispersion <em>cannot</em> be
diagnosed in this way for <em>binary</em> data (and the residual
deviance may not be reliable if the <span
class="math inline">\(m_i\)</span> are very small).</p>
<p><strong>Example</strong>: Let’s look again a the Poisson regression
model for the trawling data.</p>
<pre class="r"><code>library(COUNT)
data(fishing)

m &lt;- glm(totabund ~ period * meandepth + offset(log(sweptarea)),
  family = poisson, data = fishing)

d &lt;- expand.grid(sweptarea = 1, period = levels(fishing$period), 
  meandepth = seq(800, 5000, length = 100))
d$yhat &lt;- predict(m, newdata = d, type = &quot;response&quot;)

p &lt;- ggplot(fishing, aes(x = meandepth, y = totabund/sweptarea)) +
  geom_point(alpha = 0.5) + facet_wrap(~ period) + theme_minimal() +
  labs(x = &quot;Mean Trawl Depth (meters)&quot;,
    y = &quot;Fish Caught Per Square Meter Trawled&quot;) + 
  geom_line(aes(y = yhat), data = d)
plot(p)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" />
Might there be over-dispersion here?</p>
<pre class="r"><code>summary(m)</code></pre>
<pre><code>
Call:
glm(formula = totabund ~ period * meandepth + offset(log(sweptarea)), 
    family = poisson, data = fishing)

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -3.42e+00   1.49e-02 -229.67   &lt;2e-16 ***
period2000-2002           -7.71e-01   2.97e-02  -25.94   &lt;2e-16 ***
meandepth                 -9.71e-04   7.96e-06 -121.94   &lt;2e-16 ***
period2000-2002:meandepth  1.32e-04   1.52e-05    8.65   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 46176  on 146  degrees of freedom
Residual deviance: 14982  on 143  degrees of freedom
AIC: 15962

Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>fishing$eta &lt;- predict(m)
fishing$res &lt;- rstudent(m)

p &lt;- ggplot(fishing, aes(x = eta, y = res)) + theme_minimal() +
  geom_point(alpha = 0.25) + 
  labs(x = &quot;Predicted Value (log scale)&quot;, 
    y = &quot;Studentized Residual&quot;) + 
  geom_hline(yintercept = c(-2, 2), linetype = 3)
plot(p)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" />
Over-dispersion is not the only issue here. The variance of the
residuals is not constant.</p>
</div>
<div id="solutions-to-over-dispersion" class="section level2">
<h2>Solutions to Over-dispersion</h2>
<p>There are several potential solutions to over-dispersion.</p>
<ol style="list-style-type: decimal">
<li><p>Quasi-likelihood. Specify a variance structure other than the one
implied by a specified distribution.</p></li>
<li><p>Specify a different distribution (possibly outside the
exponential family).</p></li>
<li><p>Use a robust estimator of the standard errors (i.e.,
heteroscedastic consistent standard errors).</p></li>
</ol>
<div id="quasi-likelihood-solutions-to-over-dispersion"
class="section level3">
<h3>Quasi-Likelihood Solutions to Over-dispersion</h3>
<p>The Poisson and binomial distributions assume the variance structures
<span class="math display">\[
  \text{Var}(Y_i) = \phi E(Y_i) \ \ \ \text{and} \ \ \ \text{Var}(Y_i) =
\phi E(Y_i)[1 - E(Y_i)]/m_i,
\]</span> respectively, where the <em>dispersion parameter</em> is
<em>fixed</em> at <span class="math inline">\(\phi = 1\)</span>. One
solution is to allow <span class="math inline">\(\phi\)</span> to be an
unknown parameter to “relax” the variance structure and allow the
variance to be larger than it would be for a Poisson or binomial
distribution. The dispersion parameter can be estimated. R uses <span
class="math display">\[
    \hat{\phi} = \frac{1}{n-p}\sum_{i=1}^n \frac{(y_i -
\hat{y}_i)^2}{\mbox{V}(\hat{y}_i)},
\]</span> which is analogous to the estimate of <span
class="math inline">\(\sigma^2\)</span> in a normal linear model. This
is a quasi-likelihood approach because the variance structures with
<span class="math inline">\(\phi \neq 1\)</span> do not correspond to a
binomial or Poisson distribution. This kind of quasi-likelihood can be
done with <code>glm</code> by using <code>quasipoisson</code> or
<code>quasibinomial</code> instead of <code>poisson</code> or
<code>binomial</code>, respectively, when specifying the
<code>family</code> argument.</p>
<p><strong>Example</strong>: Consider again the rotifer model.</p>
<pre class="r"><code>m.quasi &lt;- glm(cbind(y, total - y) ~ species + density + species:density,
  family = quasibinomial, data = myrotifer)
plot(predict(m.quasi), rstudent(m.quasi), main = &quot;Residual Plot&quot;)
abline(h = c(-2,2), lty = 2)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" />
Note: You <strong>cannot</strong> compare the residual deviance to the
residual degrees of freedom as a diagnostic to determine if using
quasi-likelihood was successful, but standardized residuals are still
appropriate.</p>
<p>How does this impact our inferences?</p>
<pre class="r"><code>m.binom &lt;- glm(cbind(y, total - y) ~ species + density + species:density,
  family = binomial, data = myrotifer)
cbind(summary(m.binom)$coefficients, confint(m.binom))</code></pre>
<pre><code>                  Estimate Std. Error  z value   Pr(&gt;|z|)    2.5 %   97.5 %
(Intercept)       -114.352      4.034 -28.3454 9.534e-177 -122.420 -106.598
speciespm            4.629      6.598   0.7016  4.830e-01   -8.464   17.431
density            108.746      3.857  28.1910 7.535e-175  101.332  116.460
speciespm:density   -3.077      6.329  -0.4862  6.268e-01  -15.354    9.487</code></pre>
<pre class="r"><code>cbind(summary(m.quasi)$coefficients, confint(m.quasi))</code></pre>
<pre><code>                  Estimate Std. Error t value  Pr(&gt;|t|)   2.5 % 97.5 %
(Intercept)       -114.352      14.95 -7.6472 4.736e-09 -146.02 -87.01
speciespm            4.629      24.46  0.1893 8.509e-01  -46.15  51.31
density            108.746      14.30  7.6056 5.358e-09   82.60 139.02
speciespm:density   -3.077      23.46 -0.1312 8.964e-01  -47.81  45.70</code></pre>
<pre class="r"><code># odds ratios for effect of a 0.01 unit increase in density
trtools::contrast(m.binom,
  a = list(species = c(&quot;kc&quot;,&quot;pm&quot;), density = 0.02),
  b = list(species = c(&quot;kc&quot;,&quot;pm&quot;), density = 0.01),
  cnames = c(&quot;kc&quot;,&quot;pm&quot;), tf = exp)</code></pre>
<pre><code>   estimate lower upper
kc    2.967 2.751 3.200
pm    2.877 2.607 3.174</code></pre>
<pre class="r"><code># odds ratios for effect of a 0.01 unit increase in density
trtools::contrast(m.quasi,
  a = list(species = c(&quot;kc&quot;,&quot;pm&quot;), density = 0.02),
  b = list(species = c(&quot;kc&quot;,&quot;pm&quot;), density = 0.01),
  cnames = c(&quot;kc&quot;,&quot;pm&quot;), tf = exp)</code></pre>
<pre><code>   estimate lower upper
kc    2.967 2.220 3.965
pm    2.877 1.973 4.195</code></pre>
<p>Note that point estimates are unchanged, but standard errors, tests,
and confidence intervals are affected.</p>
<p><strong>Example</strong>: Now let’s try the same approach with
trawling data.</p>
<pre class="r"><code>m.quasi &lt;- glm(totabund ~ period * meandepth + offset(log(sweptarea)),
  family = quasipoisson, data = fishing)
plot(predict(m.quasi), rstudent(m.quasi), main = &quot;Residual Plot&quot;)
abline(h = c(-2,2), lty = 2)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" />
That was maybe somewhat less successful. Note the “megaphone” pattern.
The assumed variance structure is <span class="math display">\[
  \text{Var}(Y_i) = \phi E(Y_i).
\]</span> We could relax this by assuming instead <span
class="math display">\[
  \text{Var}(Y_i) = \phi E(Y_i)^p.
\]</span> for some <span class="math inline">\(p &gt; 1\)</span>. If
<span class="math inline">\(p\)</span> = 1, 2, or 3 then we can use
<code>quasi</code>. Here we are using it for <span
class="math inline">\(p\)</span> = 2.</p>
<pre class="r"><code>m.quasi &lt;- glm(totabund ~ period * meandepth + offset(log(sweptarea)),
  family = quasi(link = &quot;log&quot;, variance = &quot;mu^2&quot;), data = fishing)
summary(m.quasi)$coefficients</code></pre>
<pre><code>                            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)               -3.250e+00  1.592e-01 -20.4180 3.187e-44
period2000-2002           -6.041e-01  2.720e-01  -2.2212 2.791e-02
meandepth                 -1.041e-03  5.866e-05 -17.7403 5.988e-38
period2000-2002:meandepth  7.272e-05  9.992e-05   0.7278 4.679e-01</code></pre>
<pre class="r"><code>plot(predict(m.quasi), rstudent(m.quasi), main = &quot;Residual Plot&quot;)
abline(h = c(-2,2), lty = 2)</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" />
Note that <code>quasi(link = "log", variance = "mu")</code> is the same
as <code>quasipoisson</code>. For more options consider
<code>family = tweedie</code>. The <code>tweedie</code> family defines
power functions for link and variance functions of the form <span
class="math display">\[
  E(Y_i)^q = \eta_i \ \ \ \text{and} \ \ \ \text{Var}(Y_i) = \phi
E(Y_i)^p,
\]</span> where <span class="math inline">\(E(Y_i)^0 \equiv \log
E(Y_i)\)</span> when using <code>tweedie</code> (not mathematically of
course — this is just for interface purposes). For example, to replicate
the quasi-likelihood model above we can use the following.</p>
<pre class="r"><code>library(statmod) # for tweedie &quot;family&quot; 
m.tweedie &lt;- glm(totabund ~ period * meandepth + offset(log(sweptarea)),
  family = tweedie(link.power = 0, var.power = 2), data = fishing)
summary(m.tweedie)$coefficients</code></pre>
<pre><code>                            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)               -3.250e+00  1.592e-01 -20.4180 3.187e-44
period2000-2002           -6.041e-01  2.720e-01  -2.2212 2.791e-02
meandepth                 -1.041e-03  5.866e-05 -17.7403 5.988e-38
period2000-2002:meandepth  7.272e-05  9.992e-05   0.7278 4.679e-01</code></pre>
<p>The powers <span class="math inline">\(p\)</span> and <span
class="math inline">\(q\)</span> are not required to be integers when
using <code>tweedie</code>.</p>
<p>Whether or not we use quasi-likelihood will affect the standard
errors, as well as tests and confidence intervals. Failing to account
for substantial over-dispersion can result in biased standard errors,
and thus incorrect tests and confidence intervals. Estimates of
parameters (of functions thereof such as what we get from
<code>contrast</code>) may or may not change, depending on the variance
structure.</p>
<pre class="r"><code>m.poisson &lt;- glm(totabund ~ period * meandepth + offset(log(sweptarea)),
  family = poisson, data = fishing)
# rate ratios for year
trtools::contrast(m.poisson, 
  a = list(sweptarea = 1, meandepth = c(1000,2000,3000,4000,5000), period = &quot;2000-2002&quot;),
  b = list(sweptarea = 1, meandepth = c(1000,2000,3000,4000,5000), period = &quot;1977-1989&quot;),
  cnames = c(&quot;1000m&quot;,&quot;2000m&quot;,&quot;3000m&quot;,&quot;4000m&quot;,&quot;5000m&quot;), tf = exp)</code></pre>
<pre><code>      estimate  lower  upper
1000m   0.5277 0.5100 0.5460
2000m   0.6020 0.5861 0.6183
3000m   0.6869 0.6565 0.7187
4000m   0.7837 0.7293 0.8421
5000m   0.8941 0.8087 0.9885</code></pre>
<pre class="r"><code>trtools::contrast(m.tweedie, 
  a = list(sweptarea = 1, meandepth = c(1000,2000,3000,4000,5000), period = &quot;2000-2002&quot;),
  b = list(sweptarea = 1, meandepth = c(1000,2000,3000,4000,5000), period = &quot;1977-1989&quot;),
  cnames = c(&quot;1000m&quot;,&quot;2000m&quot;,&quot;3000m&quot;,&quot;4000m&quot;,&quot;5000m&quot;), tf = exp)</code></pre>
<pre><code>      estimate  lower  upper
1000m   0.5878 0.4046 0.8540
2000m   0.6321 0.4869 0.8206
3000m   0.6798 0.5173 0.8935
4000m   0.7311 0.4905 1.0897
5000m   0.7863 0.4458 1.3867</code></pre>
</div>
</div>
<div id="inferences-with-quasi-likelihood" class="section level2">
<h2>Inferences With Quasi-Likelihood</h2>
<p>Using quasi-likelihood instead of maximum likelihood changes how
inferences are made in several ways.</p>
<ol style="list-style-type: decimal">
<li><p>The standard errors are multiplied by <span
class="math inline">\(\sqrt{\hat\phi}\)</span>. If <span
class="math inline">\(\hat\phi &gt; 1\)</span> (which it probably is if
over-dispersion is present) then the standard errors will be larger (and
thus failing to account for over-dispersion leads us to usually
underestimate them). Note that this adjustment is made automatically
when using quasi-likelihood.</p></li>
<li><p>Wald confidence intervals and tests for a single parameter or
function of parameters are based on the <span
class="math inline">\(t\)</span> distribution rather than the standard
normal distribution. The <span class="math inline">\(t\)</span>
distribution is believed to provide more accurate results, although it
is still an approximation.</p></li>
<li><p>Using <code>confint</code> or <code>anova</code> use the <span
class="math inline">\(F\)</span> distribution rather than the <span
class="math inline">\(\chi^2\)</span> distribution. The underlying test
statistic is similar to the <span class="math inline">\(F\)</span> test
statistic used in normal linear models. When using <code>anova</code>
you should use <code>test = "F"</code> rather than
<code>test = "LRT"</code> if you are using quasi-likelihood.</p></li>
<li><p>Function in <strong>emmeans</strong> do not adjust the degrees of
freedom for estimating the dispersion parameter when using
quasi-likelihood. This does not make much difference unless <span
class="math inline">\(n\)</span> is small. But you can specify the it
manually via the <code>df</code> argument (use the degrees of freedom
for the residual deviance from <code>summary</code> or extract it with
<code>modelname$df.residual</code>). But <code>contrast</code> and
<code>lincon</code> do not require manual specification, although you
can via the <code>df</code> argument for those functions.</p>
<pre class="r"><code>library(emmeans)
m.quasi &lt;- glm(cbind(y, total - y) ~ species + density + species:density,
  family = quasibinomial, data = myrotifer)
trtools::contrast(m.quasi,
  a = list(species = c(&quot;kc&quot;,&quot;pm&quot;), density = 0.02),
  b = list(species = c(&quot;kc&quot;,&quot;pm&quot;), density = 0.01),
  cnames = c(&quot;kc&quot;,&quot;pm&quot;), tf = exp)</code></pre>
<pre><code>   estimate lower upper
kc    2.967 2.220 3.965
pm    2.877 1.973 4.195</code></pre>
<pre class="r"><code>pairs(emmeans(m.quasi, ~density|species, at = list(density = c(0.02, 0.01)), 
  type = &quot;response&quot;), infer = TRUE) # wrong df</code></pre>
<pre><code>species = kc:
 contrast                  odds.ratio    SE  df asymp.LCL asymp.UCL null z.ratio p.value
 density0.02 / density0.01       2.97 0.424 Inf      2.24      3.93    1   7.606  &lt;.0001

species = pm:
 contrast                  odds.ratio    SE  df asymp.LCL asymp.UCL null z.ratio p.value
 density0.02 / density0.01       2.88 0.535 Inf      2.00      4.14    1   5.681  &lt;.0001

Confidence level used: 0.95 
Intervals are back-transformed from the log odds ratio scale 
Tests are performed on the log odds ratio scale </code></pre>
<pre class="r"><code>pairs(emmeans(m.quasi, ~density|species, at = list(density = c(0.02, 0.01)), 
  type = &quot;response&quot;), infer = TRUE, df = m.quasi$df.residual) # correct df</code></pre>
<pre><code>species = kc:
 contrast                  odds.ratio    SE df lower.CL upper.CL null t.ratio p.value
 density0.02 / density0.01       2.97 0.424 36     2.22     3.96    1   7.606  &lt;.0001

species = pm:
 contrast                  odds.ratio    SE df lower.CL upper.CL null t.ratio p.value
 density0.02 / density0.01       2.88 0.535 36     1.97     4.20    1   5.681  &lt;.0001

Degrees-of-freedom method: user-specified 
Confidence level used: 0.95 
Intervals are back-transformed from the log odds ratio scale 
Tests are performed on the log odds ratio scale </code></pre>
<p>Admittedly it does not make much difference here.</p></li>
</ol>
</div>
<div id="misspecified-mean-structures-and-over-dispersion"
class="section level2">
<h2>Misspecified Mean Structures and over-dispersion</h2>
<p>A poorly specified <em>mean structure</em> may be mistaken for
over-dispersion.</p>
<pre class="r"><code>library(trtools)
ceriodaphniastrain$strain &lt;- factor(ceriodaphniastrain$strain, labels = c(&quot;a&quot;,&quot;b&quot;))
m &lt;- glm(count ~ strain + sqrt(concentration), family = poisson, data = ceriodaphniastrain)
summary(m)</code></pre>
<pre><code>
Call:
glm(formula = count ~ strain + sqrt(concentration), family = poisson, 
    data = ceriodaphniastrain)

Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           4.5284     0.0400  113.08  &lt; 2e-16 ***
strainb              -0.2750     0.0484   -5.68  1.3e-08 ***
sqrt(concentration)  -1.6576     0.0474  -34.99  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1359.38  on 69  degrees of freedom
Residual deviance:  164.28  on 67  degrees of freedom
AIC: 493.9

Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>plot(predict(m), rstudent(m))</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>m &lt;- glm(count ~ strain + concentration, family = poisson, data = ceriodaphniastrain)
summary(m)</code></pre>
<pre><code>
Call:
glm(formula = count ~ strain + concentration, family = poisson, 
    data = ceriodaphniastrain)

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     4.4546     0.0391  113.82  &lt; 2e-16 ***
strainb        -0.2750     0.0484   -5.68  1.3e-08 ***
concentration  -1.5431     0.0466  -33.11  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1359.381  on 69  degrees of freedom
Residual deviance:   86.376  on 67  degrees of freedom
AIC: 416

Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>plot(predict(m), rstudent(m))</code></pre>
<p><img src="lecture-03-22-2024_files/figure-html/unnamed-chunk-16-2.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="quasi-likelihood-and-nonlinear-regression"
class="section level2">
<h2>Quasi-Likelihood and Nonlinear Regression</h2>
<p>Quasi-likelihood for a GLM is essentially the same as using
(nonlinear) regression with iteratively weighted least squares to
account for heteroscedasticity. The weights are <span
class="math display">\[
    w_i = \frac{1}{V(\hat{y}_i)},
\]</span> where <span class="math inline">\(V\)</span> is the variance
function.</p>
<p><strong>Example</strong>: Consider the model for the trawling data
where the variance is proportional to <span
class="math inline">\(E(Y_i)^2\)</span>. To estimate this model using
iteratively weighted least squares we use weights of <span
class="math inline">\(w_i = 1/E(Y_i)^2\)</span>.</p>
<pre class="r"><code>m.quasi &lt;- glm(totabund ~ period * meandepth + offset(log(sweptarea)),
  family = quasi(link = &quot;log&quot;, variance = &quot;mu^2&quot;), data = fishing)
summary(m.quasi)$coefficients</code></pre>
<pre><code>                            Estimate Std. Error  t value  Pr(&gt;|t|)
(Intercept)               -3.250e+00  1.592e-01 -20.4180 3.187e-44
period2000-2002           -6.041e-01  2.720e-01  -2.2212 2.791e-02
meandepth                 -1.041e-03  5.866e-05 -17.7403 5.988e-38
period2000-2002:meandepth  7.272e-05  9.992e-05   0.7278 4.679e-01</code></pre>
<pre class="r"><code>fishing$w &lt;- 1
for (i in 1:10) {
  m.iwls &lt;- nls(totabund ~ exp(b0 + b1*(period == &quot;2000-2002&quot;) + b2*meandepth + 
    b3*(period == &quot;2000-2002&quot;)*meandepth + log(sweptarea)), data = fishing,
    start = list(b0 = -3, b1 = -0.6, b2 = 0, b3 = 0), weights = w)
  fishing$w &lt;- 1 / predict(m.iwls)^2
}
summary(m.iwls)$coefficients</code></pre>
<pre><code>     Estimate Std. Error  t value  Pr(&gt;|t|)
b0 -3.250e+00  1.592e-01 -20.4179 3.189e-44
b1 -6.041e-01  2.720e-01  -2.2213 2.790e-02
b2 -1.041e-03  5.866e-05 -17.7402 5.991e-38
b3  7.273e-05  9.992e-05   0.7279 4.679e-01</code></pre>
<p><strong>Example</strong>: Consider the model for the rotifer data.
Here the variance is proportional to <span
class="math inline">\(E(Y_i)[1 - E(Y_i)_i]/m_i\)</span> (recall that
<span class="math inline">\(m_i\)</span> is the “total possible” for the
counts). To estimate this model using iteratively weighted least squares
we use weights of <span class="math display">\[
  w_i = \frac{m_i}{E(Y_i)[1-E(Y_i)]}.
\]</span></p>
<pre class="r"><code>m.binomial &lt;- glm(cbind(y, total - y) ~ species * density, 
  family = quasibinomial, data = myrotifer)
summary(m.binomial)$coefficients</code></pre>
<pre><code>                  Estimate Std. Error t value  Pr(&gt;|t|)
(Intercept)       -114.352      14.95 -7.6472 4.736e-09
speciespm            4.629      24.46  0.1893 8.509e-01
density            108.746      14.30  7.6056 5.358e-09
speciespm:density   -3.077      23.46 -0.1312 8.964e-01</code></pre>
<pre class="r"><code>myrotifer$w &lt;- 1
for (i in 1:20) {
    m &lt;- nls(y/total ~ plogis(b0 + b1*(species == &quot;pm&quot;) + b2*density + 
      b3*(species == &quot;pm&quot;)*density), data = myrotifer, weights = w, 
      start = list(b0 = -114, b1 = 4.6, b2 = 109, b3 = -3))
    myrotifer$yhat &lt;- predict(m)
    myrotifer$w &lt;- myrotifer$total / (myrotifer$yhat * (1 - myrotifer$yhat))
}
summary(m)$coefficients</code></pre>
<pre><code>   Estimate Std. Error t value  Pr(&gt;|t|)
b0 -114.338      14.95 -7.6485 4.718e-09
b1    4.614      24.46  0.1887 8.514e-01
b2  108.732      14.29  7.6069 5.338e-09
b3   -3.063      23.46 -0.1306 8.968e-01</code></pre>
<p>Note that <code>plogis</code> is the function <span
class="math inline">\(e^x/(1 + e^x)\)</span>. The model can be written
as <span class="math display">\[
  E(Y_i) = \frac{e^{\eta_i}}{1 + e^{\eta_i}}
\]</span> where <span class="math inline">\(Y_i\)</span> is the observed
<em>proportion</em>, and <span class="math display">\[
  \eta_i = \beta_0 + \beta_1 s_i + \beta_2 d_i + \beta_3 s_i d_i,
\]</span> where <span class="math inline">\(s_i\)</span> is an indicator
variable for the <code>pm</code> species, and <span
class="math inline">\(d_i\)</span> is the density.</p>
<p>Using iteratively weighted least squares is not necessary if we can
use <code>quasi</code> or <code>tweedie</code>, but it is a useful
option for cases where the variance structure is outside what can be
done with <code>quasi</code> or <code>tweedie</code> (although one can
<em>program</em> new variance structures).</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
