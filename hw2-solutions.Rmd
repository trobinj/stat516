---
title: "Nonlinear Regression and Heteroscedasticity"
subtitle: Statistics 516, Homework 2 (Solutions)
output:
  html_document:
    theme: readable
  pdf_document: default
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{array}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", message = FALSE, out.width = "100%", fig.align = "center", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](hw2-solutions.pdf) copy of this homework assignment.", sep = ""), "")`

```{r options, echo = FALSE}
options(digits = 4, width = 100)
```

This homework assignment concerns specifying and making inferences from nonlinear regression models, and methods for accounting for heteroscedasticity.

## Using `nls` for Linear Models

Both the `nls` and the `lm` functions can estimate *linear* regression models, but their interfaces are different. The `lm` function allows us to specify a model *symbolically* whereas `nls` requires us to specify the model *mathematically*. In practice we usually use `lm` for linear models because it takes care of some of the intricacies of specifying the model (e.g., indicator variables and interactions), but in some cases it can be useful to use `nls`, particularly if the linear model you are using has a peculiar parameterization. But I think it can be a useful exercise for the student to have experience using `nls` to specify linear models.[^nonlinear] Here you will use `nls` to estimate some of the models you encountered in the previous homework assignment. Note that since all of these models are linear, you need not worry about specifying good starting values. You can safely specify values of zero for all starting values. 

1. The last homework assignment considered a couple of parameterizations of models for the `Dopamine` data from the **BSDA** package. Below the models are estimated using the `lm` function. See the solutions for the previous homework to see how these models can be written mathematically.
    ```{r}
    library(BSDA)
    m1 <- lm(dbh ~ group, data = Dopamine)
    summary(m1)$coefficients
    m2 <- lm(dbh ~ -1 + group, data = Dopamine)
    summary(m2)$coefficients
    ```
For each of these two models, use the `nls` function to estimate the model parameters once by specifying an indicator variable within the model itself (using the `==` operator), and again by using *either* the `ifelse` function *or* the `case_when` function from the **dplyr** package. Show the output of summary for all of these estimated models to verify that you obtained the same results (there may be minor differences several places after the decimal). Be sure the estimates are in the same order in the output of `summary`. This can be controlled by the order in which the starting values are specified to `nls`.

    **Solution**: The first parameterization can be estimated as follows.
    ```{r}
    library(dplyr) # for case_when
    m <- nls(dbh ~ b0 + b1 * (group == "psychotic"), 
      data = Dopamine, start = list(b0 = 0, b1 = 0))
    summary(m)$coefficients
    m <- nls(dbh ~ ifelse(group == "psychotic", b0 + b1, b0), 
      data = Dopamine, start = list(b0 = 0, b1 = 0))
    summary(m)$coefficients
    m <- nls(dbh ~ case_when(
      group == "psychotic" ~ b0 + b1,
      group == "nonpsychotic" ~ b0), 
      data = Dopamine, start = list(b0 = 0, b1 = 0))
    summary(m)$coefficients
    ```
    The second parameterization can be estimated as follows.
    ```{r}
    m <- nls(dbh ~ b1 * (group == "psychotic") + 
      b2 * (group == "nonpsychotic"), 
      data = Dopamine, start = list(b1 = 0, b2 = 0))
    summary(m)$coefficients
    m <- nls(dbh ~ ifelse(group == "psychotic", b1, b2), 
      data = Dopamine, start = list(b1 = 0, b2 = 0))
    summary(m)$coefficients
    m <- nls(dbh ~ case_when(
      group == "psychotic" ~ b1,
      group == "nonpsychotic" ~ b2), 
      data = Dopamine, start = list(b1 = 0, b2 = 0))
    summary(m)$coefficients
    ```
    
2. Another problem in the last homework concerned the `rat` data from the **ALA** package and featured a couple of different models. Again, see the solutions for the previous homework to see how these models can be written mathematically.[^ratmasked] 
    ```{r}
    m <- lm(weight ~ treatment + week + treatment:week, data = ALA::rat)
    summary(m)$coefficients
    m <- lm(weight ~ treatment:week, data = ALA::rat)
    summary(m)$coefficients
    ```
As in the previous problem, use the `nls` function to estimate the model parameters for each model once by specifying an indicator variable within the model itself (using the `==` operator), and again by using the `case_when` function from the **dplyr** package (the `ifelse` function could be used, but is cumbersome for more than two cases since it requires multiple and nested `ifelse` statements). Show the output of summary for all of these to verify that you obtained the same results. 

    **Solution**: Here is how we can estimate the first model using `nls`.
    ```{r}
    m <- nls(weight ~ b0 + b1 * (treatment == "thiouracil") + 
      b2*(treatment == "thyroxin") + b3 * week + 
      b4*(treatment == "thiouracil") * week + b5 * (treatment == "thyroxin") * week,
      start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0),
      data = ALA::rat)
    summary(m)$coefficients
    m <- nls(weight ~ case_when(
      treatment == "control" ~ b0 + b3 * week,
      treatment == "thiouracil" ~ b0 + b1 + (b3 + b4) * week,
      treatment == "thyroxin" ~ b0 + b2 + (b3 + b5) * week),
      start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0),
      data = ALA::rat)
    summary(m)$coefficients
    ```
    The second model can be estimated as follows.
    ```{r}
    m <- nls(weight ~ b0 + 
      b1 * (treatment == "control") * week +
      b2 * (treatment == "thiouracil") * week + 
      b3 * (treatment == "thyroxin") * week,
      start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0),
      data = ALA::rat)
    summary(m)$coefficients
    m <- nls(weight ~ case_when(
      treatment == "control" ~ b0 + b1 * week,
      treatment == "thiouracil" ~ b0 + b2 * week,
      treatment == "thyroxin" ~ b0 + b3 * week),
      start = list(b0 = 0, b1 = 0, b2 = 0, b3 = 0),
      data = ALA::rat)
    summary(m)$coefficients
    ```

[^ratmasked]: Note that here you may want to use `ALA::rat` to refer to the data frame because there is a data frame of the same name in the **alr3** package which you will be using in a later problem. If you load the **alr3** package after the **ALA** package during the same R session then you run into a name conflict. Using `ALA::rat` avoids that problem. 
[^nonlinear]: I have sometimes toyed with the idea of *starting* the class by teaching students to use `nls` rather than `lm` to help them better appreciate and understand what `lm` is doing for them.

## Another Michaelis-Menten Model

This problem features using a Michaelis-Menten model similar to that featured in class. The data frame `inhibitor` from the **isdals** package is from an experiment conducted by students in a biochemistry course at the University of Copenhagen.[^ekstrom] The first few observations can be seen below.
```{r}
library(isdals)
data(inhibitor) # required for this package to make the data available
head(inhibitor)
```
The variables `Sconc` and `RR` are the substrate concentration and reaction rate, respectively. As in a typical experiment using this model, assays were conducted at several substrate concentrations and the reaction rate was observed. But this experiment also featured introducing an inhibitor at concentrations of 0, 50, and 100 $\mu$moles. The variable `Iconc` is the inhibitor concentration. The plot below shows the raw data.
```{r}
library(ggplot2)
p <- ggplot(inhibitor, aes(x = Sconc, y = RR)) + 
  theme_minimal() + geom_point() + 
  facet_wrap(~Iconc, labeller = label_both) + 
  labs(x = "Substrate Concentration (micro moles)",
    y = "Reaction Rate (units unknown)")
plot(p)
```
Recall that the basic Michaelis-Menten regression model can be written as 
$$
  E(R_i) = \frac{\alpha s_i}{\lambda + s_i},
$$
where $R_i$ and $s_i$ are the $i$-th observation of reaction rate and substrate concentration, respectively, and $\alpha$ and $\lambda$ here denote the "asymptote" and "half-life" parameters, respectively. Here we want to model how the inhibitor concentration "interacts" with the substrate concentration in the sense that one or both parameters of this model depend on the concentration.

1. Estimate a nonlinear regression model using the `nls` function that allows for each of the three levels of concentration to have a different value of the $\alpha$ and $\lambda$ parameters, similar to how the example from lecture allowed these parameters to be different for cells that were treated or untreated by puromycin. So your model should estimate *six* parameters which we can denote using subscripts as $\alpha_0$ and $\lambda_0$ for the control condition, $\alpha_1$ and $\lambda_1$ for a inhibitor concentration of 50 $\mu$moles, and $\alpha_2$ and $\lambda_2$ for an inhibitor concentration of 100 $\mu$moles. Essentially this is a case-wise model like that featured in class but with three cases rather than two. I *strongly* recommend that you use the `case_when` function from the **dplyr** package to manage the case-wise structure of this model.[^casewise] Note that you should be able to "eyeball" (i.e., guess) the starting values from the plot of the raw data. Report the parameter estimates using `summary`. Finally, plot the model by producing a plot similar to that shown above but with curves depicting the estimated model.[^modelcheck]

    **Solution**: We can estimate the model as follows.
    ```{r}
    library(dplyr)
    m <- nls(RR ~ case_when(
      Iconc == 0 ~ alpha0 * Sconc / (lambda0 + Sconc),
      Iconc == 50 ~ alpha1 * Sconc / (lambda1 + Sconc),
      Iconc == 100 ~ alpha2 * Sconc / (lambda2 + Sconc)),
      start = list(alpha0 = 3, lambda0 = 50, alpha1 = 3, lambda1 = 100,
        alpha2 = 3, lambda2 = 150), data = inhibitor)
    summary(m)$coefficients
    d <- expand.grid(Iconc = c(0, 50, 100), Sconc = seq(0, 600, length = 100))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(inhibitor, aes(x = Sconc, y = RR)) + 
      theme_minimal() + geom_point() + geom_line(aes(y = yhat), data = d) + 
      facet_wrap(~Iconc, labeller = label_both) + 
      labs(x = "Substrate Concentration (micro moles)",
        y = "Reaction Rate (units unknown)")
    plot(p)
    ```

0. Using `summary` you can make inferences about the six model parameters, but they do not provide comparisons of the parameters. Comparisons can be made using the `lincon` function from the **trtools** package. Use `lincon` to estimate the *difference* between the corresponding Michaelis-Menten parameters between the control condition and the condition with an inhibition concentration of 50 $\mu$moles (i.e., $\alpha_1 - \alpha_0$ and $\lambda_1 - \lambda_0$), and also between the control condition and the condition with an inhibition concentration of 100 $\mu$moles (i.e., $\alpha_2 - \alpha_0$ and $\lambda_2 - \lambda_0$). 

    **Solution**: The differences between the parameters can be estimated as follows.
    ```{r}
    library(trtools)
    lincon(m, a = c(-1,0,1,0,0,0)) # alpha1 - alpha0
    lincon(m, a = c(-1,0,0,0,1,0)) # alpha2 - alpha0
    lincon(m, a = c(0,-1,0,1,0,0)) # lambda1 - lambda0
    lincon(m, a = c(0,-1,0,0,0,1)) # lambda2 - lambda0
    ```

0. Now consider an alternative parameterization of the model where we write the model case-wise as
$$
  E(R_i) = \frac{\alpha_0 s_i}{\lambda_0 + s_i}
$$
if the $i$-th observation is from the control condition with an inhibitor concentration of zero,
$$
  E(R_i) = \frac{(\alpha_0 + \delta_1)s_i}{\lambda_0 + \tau_1 + s_i}
$$
if the $i$-th observation is from the condition with an inhibitor concentration of 50 $\mu$moles, and
$$
  E(R_i) = \frac{(\alpha_0 + \delta_2)s_i}{\lambda_0 + \tau_2 + s_i}
$$
if the $i$-th observation is from the condition with an inhibitor concentration of 100 $\mu$moles The parameters of this model are related to previous parameterization. We can see that $\alpha_1 = \alpha_0 + \delta_1$, $\alpha_2 = \alpha_1 + \delta_2$, $\lambda_1 = \lambda_0 + \tau_1$, and $\lambda_2 = \lambda_0 + \tau_2$. Estimate this model using the `nls` and give the parameter estimates using `summary`. Again, I would strongly recommend you use the `case_when` function here. Finally plot the model with the raw data like you did with the previous model. Your plot should look the same as that for the previous model.

    **Solution**: We can estimate the model as follows. For the starting values for the new parameters, I used (approximately) the differences between the parameter estimates from the previous model since these new parameters are equal to the differences between the parameters in the previous model.
    ```{r}
    m <- nls(RR ~ case_when(
      Iconc == 0 ~ alpha0 * Sconc / (lambda0 + Sconc),
      Iconc == 50 ~ (alpha0 + delta1) * Sconc / (lambda0 + tau1 + Sconc),
      Iconc == 100 ~ (alpha0 + delta2) * Sconc / (lambda0 + tau2 + Sconc)),
      start = list(alpha0 = 3, lambda0 = 36, delta1 = 0, tau1 = 43, delta2 = 0, tau2 = 82),
      data = inhibitor)
    summary(m)$coefficients
    d <- expand.grid(Iconc = c(0, 50, 100), Sconc = seq(0, 600, length = 100))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(inhibitor, aes(x = Sconc, y = RR)) + 
      theme_minimal() + geom_point() + geom_line(aes(y = yhat), data = d) + 
      facet_wrap(~Iconc, labeller = label_both) + 
      labs(x = "Substrate Concentration (micro moles)",
        y = "Reaction Rate (units unknown)")
    plot(p)
    ```
    Notice how the estimates of $\delta_1$, $\tau_1$, $\delta_2$, and $\tau_2$ are the same as those obtained for the differences between the parameters in the second problem based on the original parameterization. 

0. The model you estimated in the previous problem provides estimates of $\delta_1 = \alpha_1 - \alpha_0$, $\delta_2 = \alpha_2 - \alpha_0$ as well as $\tau_1 = \lambda_1 - \lambda_0$ and $\tau_2 = \lambda_2 - \lambda_0$, so you can use just the output from `summary` to make comparisons with the control condition. And you should find that these estimates agree with what you obtained using `lincon` and the first model you estimated. Now use `lincon` with the model you estimated in the previous problem to estimate $\alpha_1 = \alpha_0 + \delta_1$, $\alpha_2 = \alpha_0 + \delta_2$, $\lambda_1 = \lambda_0 + \tau_1$, and $\lambda_2 = \lambda_0 + \tau_2$. These inferences should agree with what you obtained from `summary` in the first problem.

    **Solution**: Here are the estimates.
    ```{r}
    lincon(m, a = c(1,0,1,0,0,0)) # alpha1
    lincon(m, a = c(1,0,0,0,1,0)) # alpha2
    lincon(m, a = c(0,1,0,1,0,0)) # lambda1
    lincon(m, a = c(0,1,0,0,0,1)) # lambda2
    ```

0. The models considered above treat the inhibitor concentration as a categorical variable (i.e., a factor with three levels). Another approach motivated by the biochemistry of the inhibitor is to let the $\lambda$ parameter depend on the inhibitor concentration so that
$$
  E(R_i) = \frac{\alpha s_i}{\lambda_0(1 + h_i/\kappa) + s_i},
$$
where $h_i$ is the inhibitor concentration for the $i$-th observation. This model has three parameters: $\alpha$, $\lambda_0$, and $\kappa$. Here the inhibitor does not affect the asymptote ($\alpha$), but the half-life parameter is a linear function of the inhibitor concentration.[^lambda] Here $\lambda_0$ is the value of the half-life parameter when the inhibitor concentration is zero, and $\kappa$ is the inhibitor concentration necessary to double the half-life parameter from this value.[^double] Estimate this nonlinear regression model and report the parameter estimates and their confidence intervals using `summary`.[^startingvalues] Finally plot the model with the raw data like you did with the previous models.

    **Solution**: (Note: I made a mistake here in that I asked you to get confidence intervals using `summary`, which it does not provide. You can get confidence intervals using `confint`, but since the instructions were unclear I did not require the confidence intervals.) To get a starting value for $\kappa$ I used the approximate estimates of $\lambda_0$ and $\lambda_1$ from the first model and then solved for $\kappa$ in $79.2 = 35.8(1 + 50/\kappa)$ which gives a starting value of $\kappa \approx 41.24$. For the starting value for $\alpha$ is just guessed from the plot.
    ```{r}
    m <- nls(RR ~ alpha * Sconc / (lambda0 * (1 + Iconc / kappa) + Sconc),
      data = inhibitor, start = list(alpha = 3, lambda0 = 36, kappa = 41))
    summary(m)$coefficients
    d <- expand.grid(Iconc = c(0, 50, 100), Sconc = seq(0, 600, length = 100))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(inhibitor, aes(x = Sconc, y = RR)) + 
      theme_minimal() + geom_point() + geom_line(aes(y = yhat), data = d) + 
      facet_wrap(~Iconc, labeller = label_both) + 
      labs(x = "Substrate Concentration (micro moles)",
        y = "Reaction Rate (units unknown)")
    plot(p)
    ```
    
    You might wonder how we might choose between this model and the previous model (although there are two parameterizations you considered earlier, they are effectively the same model). It can be shown that this model is a special case of the previous model where the $\alpha$ parameters are assumed to be the same regardless of the inhibitor concentration, and where $\lambda$ is assumed to be a linear function of the inhibitor concentration rather than just different for the three levels of inhibitor of concentration. One approach is a goodness-of-fit test where these constraints on the parameters form a null hypothesis.
    ```{r}
    m.full <- nls(RR ~ case_when(
      Iconc == 0 ~ alpha0 * Sconc / (lambda0 + Sconc),
      Iconc == 50 ~ alpha1 * Sconc / (lambda1 + Sconc),
      Iconc == 100 ~ alpha2 * Sconc / (lambda2 + Sconc)),
      start = list(alpha0 = 3, lambda0 = 50, alpha1 = 3, lambda1 = 100,
        alpha2 = 3, lambda2 = 150), data = inhibitor)
    m.null <- nls(RR ~ alpha * Sconc / (lambda0 * (1 + Iconc / kappa) + Sconc),
      data = inhibitor, start = list(alpha = 3, lambda0 = 36, kappa = 41))
    anova(m.full, m.null)
    ```
    The test is not significant, so the latter simpler model is not rejected (although a larger sample size resulting in a more powerful test might reject it). Another approach that we will discuss later is to use the Akaike's Information Criterion (AIC) where a "better" model has a smaller AIC value (we will discuss what we mean by "better" later).
    ```{r}
    AIC(m.full)
    AIC(m.null)
    ```
    Here AIC favors (slightly) the simpler model. Again, it is possible that a larger sample size would reveal that the more complex model is a better fit to the data (which can happen as there are more data to estimate the more complex model). But in practice something like using AIC can be helpful to justify using a given model based on the design and data we have.

[^double]: To see why we can interpret $\kappa$ this way, note that if $h_i = \kappa$ then $\lambda_0(1 + h_i/\kappa) = 2\lambda_0$.

[^startingvalues]: To specify your starting values try the following strategy. The $\alpha$ parameter is the asymptote for the curve, regardless of the inhibitor concentration, so you can guess this from the plot of the raw data. And because $\lambda_0$ is the value of the half-life parameter when the concentration is zero, you can guess this from the plot or use an estimate from one of the previous models. For $\kappa$ try guessing the value of the half-life parameter when the inhibitor concentration is at, say, $h_i = 50$, or use an estimate from one of the previous models. Call this guess/estimate $\hat\lambda_1$. We have that $\lambda_1 = \lambda_0(1 + 50/\kappa)$. Replace $\lambda_0$ and $\lambda_1$ in that expression with guesses/estimates as described above and then solve for $\kappa$ to get a starting value for that parameter.

[^lambda]: We could alternatively write the model as
$$
  E(R_i) = \frac{\alpha s_i}{\lambda_i + s_i},
$$
where $\lambda_i = \lambda_0(1 + h_i/\kappa)$ to show how the half-life parameter $\lambda_i$ is now indexed by the observation since it depends on the inhibitor concentration, $h_i$.

[^ekstrom]: Source: Ekstrom, C. T. & Sorensen, H. (2010). *Introduction to statistical data analysis for the life sciences*. CRC Press.

[^casewise]: It could, in principle, be done using indicator variables or even the `ifelse` function, but the code would be more complex.

[^modelcheck]: A plot like this is a good way to determine if you made significant mistake in estimating your model. If you do not produce curves that look consistent with the raw data you may have made a mistake.

## Aerial Survey of Snow Geese

The data frame `snowgeese` from the **alr3** package is from an unpublished study of aerial survey methods for estimating the number of snow geese (*Anser caerulescens*) in their summer range areas west of Hudson Bay in Canada.[^snowgeese] Counts were made separately by two observers from an aircraft that flew near flocks of geese. For comparison, an exact count of the number of geese in the flock was also obtained from a photograph. The first few observations can be seen below.
```{r}
library(alr3)
head(snowgeese)
```
Note: To install the **alr3** package use `install.packages("alr3", repos = "http://R-Forge.R-project.org")` since it is no longer available on the Comprehensive R Archive Network (CRAN) repository, which is the default for `install.packages`.[^alr3] The variable `photo` is the exact count of the number of geese in a flock while `obs1` and `obs2` are the visual counts from the two observers. To plot and model these data they need to be reshaped into "long form" so that we have one observer count in each row. This can be done as follows.
```{r}
library(dplyr)
library(tidyr)
goosecount <- snowgeese %>% mutate(flock = 1:n()) %>%
  pivot_longer(c(obs1, obs2), names_to = "observer", values_to = "count")
head(goosecount)
```
Here is a plot of these data.
```{r}
library(ggplot2)
p <- ggplot(goosecount, aes(x = photo, y = count, color = observer)) + 
  theme_minimal() + geom_point(alpha = 0.5) + 
  labs(x = "Photo Count", y = "Observer Count", color = "Observer") + 
  theme(legend.position = c(0.2, 0.8))
plot(p)
```
The goal here is to use a regression model to investigate the accuracy (or lack thereof) of using human observers in aerial surveys to estimate the size of snow geese flocks.

1. Here you will use a model like that used for the `biomass` data featured in lecture. The model can be written as
$$
  E(Y_i) = 
  \begin{cases}
    \beta_1 x_i, & \text{if the $i$-th observation is from observer 1}, \\
    \beta_2 x_i, & \text{if the $i$-th observation is from observer 2}, 
  \end{cases}
$$
where $Y_i$ is the observer count and $x_i$ is the photo count. See the lecture notes for how to specify this model. Estimate this model and give the parameter estimates using `summary`. Also plot the model with the raw data. 

    **Solution**: We can estimate this model as follows.
    ```{r}
    m <- lm(count ~ -1 + photo:observer, data = goosecount)
    summary(m)$coefficients
    ```
    Here is a plot of the model.
    ```{r}
    d <- expand.grid(observer = c("obs1","obs2"), 
      photo = seq(0, 410, length = 100))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(goosecount, aes(x = photo, y = count, color = observer)) + 
      theme_minimal() + geom_point(alpha = 0.5) +
      geom_line(aes(y = yhat), data = d) + 
      labs(x = "Photo Count", y = "Observer Count", color = "Observer") + 
      theme(legend.position = c(0.2, 0.8))
    plot(p)
    ```
  
0. Using the model you estimated estimated above, determine if there is a statistically significant difference between $\beta_1$ and $\beta_2$ using the `lincon` function. Also conduct a test of the null hypothesis $\beta_1 = 1$, and then again for the null hypothesis $\beta_2 = 1$. The reason why the null hypotheses $\beta_1 = 1$ and $\beta_2 = 1$ are interesting here is that if we can show that the slope of the line is greater/less than one then we can show that the observers tend to overestimate/underestimate the the size of a flock. You can test these hypotheses one of two ways: use the `lincon` function (hint: use the `b` argument for the `lincon` function and note that, for example, $\beta_1 = 1$ can also be written as $\beta_1 - 1 = 0$), or use a confidence interval to conduct the test. For each test be sure to state your conclusion for each test (i.e., reject or do not reject the null hypothesis). Use a significance level of $\alpha$ = 0.05. 

    **Solution**: We can test the difference between $\beta_1$ and $\beta_2$ as follows.
    ```{r}
    lincon(m, a = c(-1,1)) # beta2 - beta1
    ```
    The slopes are significantly different. You can also do that using the **emmeans** package as follows.
    ```{r}
    library(emmeans)
    pairs(emtrends(m, ~observer, var = "photo"), reverse = TRUE, infer = TRUE)
    ```
    And here is another approach, which is a bit more convoluted, but can be useful in some circumstances. First we can estimate the expected response for each observer at one unit of `photo` apart.
    ```{r}
    emmeans(m, ~photo*observer, 
      at = list(photo = c(2,1)))
    ```
    Now we pass that to `pairs` and specify `by = observer` so that the pairwise comparison is within each observer.
    ```{r}
    pairs(emmeans(m, ~photo*observer, 
      at = list(photo = c(2,1))), by = "observer")
    ```
    The we pass that to `pairs` and specify `by = NULL` to get rid of the grouping to compute the difference of differences to get the comparison of the slopes. 
    ```{r}
    pairs(pairs(emmeans(m, ~photo*observer, 
      at = list(photo = c(2,1))), by = "observer"), by = NULL, reverse = TRUE)
    ```
    I also specified `reverse = TRUE` to get the difference in the same direction as before. Again, it is very convoluted, but it does demonstrate some tricks you can do with the **emmeans** package.
    
    Here is how to use `lincon` to test the null hypothesis that $\beta_1 = 1$ and again for $\beta_2 = 1$.
    ```{r}
    lincon(m, a = c(1,0), b = -1) # H0: beta1 - 1 = 0
    lincon(m, a = c(0,1), b = -1) # H0: beta2 - 1 = 0
    ```
    So at the $\alpha$ = 0.05 significance level, the first slope for the first observer is significantly below one, showing a tendency to underestimate the flock size. The slope for the second observer is not significantly different from zero. Alternatively we can just look at the confidence intervals for $\beta_1$ and $\beta_2$.
    ```{r}
    cbind(summary(m)$coefficients, confint(m))
    ```
    The confidence interval for $\beta_1$ does not contain one, so we reject that null hypothesis, but the confidence interval for $\beta_2$ does contain one, so we do not reject that null hypothesis.
    
0. As with the `biomass` data, these data show considerable heteroscedasticity. As shown in lecture, use an iteratively weighted least squares approach assuming that $\text{Var}(Y_i) \propto E(Y_i)^p$ for some value of $p$ to determine your weights. Try different values of $p$ and decide what you think is a reasonable value of $p$ to address the heteroscedasticity based on a plot of the studentized residuals against the predicted values. Provide your residual plot for that value of $p$, and then repeat what you did in the previous problem using estimates obtained using the iteratively weighted least squares with your choice of $p$.

    **Solution**: Here is the iteratively weighted least squares algorithm for $p = 2$, which is the value I thought looked reasonable.  
    ```{r}
    goosecount$w <- 1
    for (i in 1:10) {
      m <- lm(count ~ -1 + photo:observer, data = goosecount, weights = w)
      goosecount$w <- 1 / predict(m)^2
    }
    ```
    And here is the residual plot.
    ```{r}
    goosecount$yhat <- predict(m)
    goosecount$residual <- rstudent(m)
    p <- ggplot(goosecount, aes(x = yhat, y = residual)) + theme_minimal() + 
      geom_point() + labs(x = "Predicted Value", y = "Studentized Residual")
    plot(p)
    ```
    Here the variability of the residuals seems relatively uniform. Compare this with what we get when we do not use weights.
    ```{r}
    m.ols <- lm(count ~ -1 + photo:observer, data = goosecount)
    goosecount$yhat <- predict(m.ols)
    goosecount$residual <- rstudent(m.ols)
    p <- ggplot(goosecount, aes(x = yhat, y = residual)) + theme_minimal() + 
      geom_point() + labs(x = "Predicted Value", y = "Studentized Residual")
    plot(p)
    ```
    Below I repeat the inferences concerning $\beta_1$ and $\beta_2$.
    ```{r}
    summary(m)$coefficients
    lincon(m, a = c(-1,1)) # beta2 - beta1
    lincon(m, a = c(1,0), b = -1) # H0: beta1 - 1 = 0
    lincon(m, a = c(0,1), b = -1) # H0: beta2 - 1 = 0
    ```
    There are no changes in significance, but the estimates have changed when using weights.
    
[^alr3]: The **alr3** package was replaced with a newer package, **alr4**, but unfortunately the `snowgeese` data are not included with this newer package. 

[^snowgeese]: Source: Cook, R. D. & Jacobsen, J. O. (1978). *Analysis of the 1977 West Hudson Bay snow goose surveys*. Unpublished report, Canadian Wildlife Service.

## Iron Retention in Rats

The data frame `ironretention` from the **trtools** package is from a randomized experiment on the retention of two iron ions, ferrous (Fe2+) and ferric (Fe3+), administered to mice at three different concentrations (0.3, 1.2, and 10.2 millimolars).[^rice] The response variable is the percent of iron retained. The data are shown in the plot below.[^dodge]
```{r}
library(trtools)
library(ggplot2)
p <- ggplot(ironretention, aes(x = factor(concentration), y = retention, color = ion)) +
  geom_point(alpha = 0.5, position = position_dodge(width = 0.25)) + 
  labs(x = "Concentration (millimolars)", 
    y = "Percent Iron Retained", color = "Iron Ion") + 
  theme_minimal() + theme(legend.pos = c(0.9, 0.8))
plot(p)
```
Some descriptive statistics for these data can be obtained as follows.
```{r}
library(dplyr)
ironretention %>% group_by(concentration, ion) %>% 
  summarize(mean = mean(retention), sd = sd(retention), obs = n())
```
Notice that the variability of the observations of percent iron retention tends to be higher when the mean retention is higher. 

In what follows we will treat concentration as a categorical variable. We can see that it is a numeric variable in the data frame using `str` (structure).
```{r}
str(ironretention)
```
We can use the following code to create a variable `concf` which is a factor created from the variable `concentration`.
```{r}
ironretention$concf <- factor(ironretention$concentration)
```
That `concf` is a factor can be confirmed by using `str(ironretention)`.[^chr] 
In what follows you will start with the following linear model estimated by (ordinary/unweighted) least squares.
```{r}
m <- lm(retention ~ concf + ion + concf:ion, data = ironretention)
summary(m)$coefficients
```
Note that this model can be written as
$$
  E(Y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5},
$$
where $Y_i$ is the $i$-th observation of iron retention,
$$
  x_{i1} = 
  \begin{cases}
    1, & \text{if the $i$-th observation of concentration is 1.2}, \\
    0, & \text{otherwise},
  \end{cases}
$$
$$
  x_{i2} = 
  \begin{cases}
    1, & \text{if the $i$-th observation of concentration is 10.2}, \\
    0, & \text{otherwise},
  \end{cases}
$$
$$
  x_{i3} = 
  \begin{cases}
    1, & \text{if the $i$-th observation is iron ion is ferrous}, \\
    0, & \text{otherwise},
  \end{cases}
$$
$x_{i4} = x_{i1}x_{i3}$ and $x_{i5} = x_{i2}x_{i3}$. So the model can be written case-wise as
$$
  E(Y_i) = 
  \begin{cases}
  \beta_0, & \text{if the concentration is 0.3 and the ion is ferric}, \\
  \beta_0 + \beta_1, & \text{if the concentration is 1.2 and the ion is ferric}, \\
  \beta_0 + \beta_2, & \text{if the concentration is 10.2 and the ion is ferric}, \\
  \beta_0 + \beta_3, & \text{if the concentration is 0.3 and the ion is ferrous}, \\
  \beta_0 + \beta_1 + \beta_3 + \beta_4, & \text{if the concentration is 1.2 and the ion is ferrous}, \\
  \beta_0 + \beta_2 + \beta_3 + \beta_5, & \text{if the concentration is 10.2 and the ion is ferrous}. 
  \end{cases}
$$
This model specifies an interaction between concentration and ion so that the difference in the expected iron retention between the two ions can be different at each of the three concentrations.[^interaction]

[^dodge]: Note the use of the `position` argument in `geom_point`. Without that argument the points denoting the ion would be in the same position over the abscissa. But by specifying `position = position_dodge(width = 0.25)` we "dodge" the points by moving them slightly sideways (with the amount determined by the `width` argument to `position_dodge`).

1. Based on the model defined above, use either the `contrast` function from the **trtools** package or functions from the **emmeans** package to estimate (a) the expected iron retention at each of the six combinations of concentration and ion and (b) the *difference* in the expected iron retention between the two ions at each of the three concentrations (i.e., the difference between ferrous and ferric at concentrations of 0.3, 1.2, and 10.2 millimolars).

    **Solution**: First I will estimate the expected iron retention, both with `contrast` and also `emmeans`.
    ```{r}
    trtools::contrast(m, a = list(concf = c("0.3","0.3","1.2","1.2","10.2","10.2"),
      ion = c("ferric","ferrous","ferric","ferrous","ferric","ferrous")),
      cnames = c("ferric,0.3","ferrous,0.3","ferric,1.2","ferrous,1.2",
        "ferric,10.2","ferrous,10.2"))
    emmeans(m, ~ion*concf)
    ```
    Now I will estimate the difference in expected iron retention between the two ions.
    ```{r}
    trtools::contrast(m, 
      a = list(concf = c("0.3","1.2","10.2"), ion = "ferrous"),
      b = list(concf = c("0.3","1.2","10.2"), ion = "ferric"),
      cnames = c(0.3, 1.2, 10.2))
    pairs(emmeans(m, ~ion*concf), by = "concf", infer = TRUE, reverse = TRUE)
    ```
  
0. Use a residual plot to check for heteroscedasticity. Comment briefly on if you believe there is evidence of heteroscedasticity and why. Be sure to include your plot.

    **Solution**: Here is the residual plot.
    ```{r}
    ironretention$yhat <- predict(m)
    ironretention$residual <- rstudent(m)
    p <- ggplot(ironretention, aes(x = yhat, y = residual)) + theme_minimal() + 
      geom_jitter(height = 0, width = 0.1, alpha = 0.5) + 
      labs(x = "Predicted Value", y = "Studentized Residual")
    plot(p)
    ```
    Note that I used `geom_jitter` to avoid so many overlapping points. Heteroscedasticity is evident from the non-constant variability of the residuals. The variability tends to increase with the predicted value. 

0. In models where the response variable is a percent so that $0 \le Y_i \le 100$ as it is here, it has been suggested that an appropriate variance structure might be
$$
  \text{Var}(Y_i) \propto [E(Y_i)/100]^p[1-E(Y_i)/100]^p,
$$
where $p \ge 1$.[^proportion] Use an *iteratively weighted least squares* algorithm similar to that we used in class, but specify your weights so that they are the *reciprocal* of $[E(Y_i)/100]^p[1-E(Y_i)/100]^p$ where each $E(Y_i)$ is estimated using $\hat{y}_i$ so that
$$
  w_i = \frac{1}{(\hat{y}_i/100)^p(1-\hat{y}_i/100)^p}.
$$
Be careful that you compute the weights correctly.[^with] Try this for values of $p$ of 1, 2, 3, 4, and 5.[^p] For each of $p$ produce a plot of the studentized residuals against the predicted values. Comment briefly on what you might think might be an appropriate value (or values) of $p$ to produce appropriate weights to address any heteroscedasticity. Then using what you believe is the best value of $p$ to address any heteroscedasticity, estimate the model using weighted least squares based on that value of $p$ and repeat what you did in the first problem. Compare the estimates and standard errors of these results with those you did before, and comment briefly on if or how they changed. 

    **Solution**: For fun I will use a loop to create all five plots.
    ```{r}
    for (p in 1:5) {
      ironretention$w <- 1
      for (i in 1:10) {
        m <- lm(retention ~ concf + ion + concf:ion,
          data = ironretention, weights = w)
        ironretention$yhat <- predict(m)
        ironretention$w <- with(ironretention, 1 / ((yhat/100)^p * (1-yhat/100)^p))
      }
      ironretention$residual <- rstudent(m)
      p <- ggplot(ironretention, aes(x = yhat, y = residual)) + theme_minimal() + 
        geom_jitter(height = 0, width = 0.1, alpha = 0.5) + 
        labs(x = "Predicted Value", y = "Studentized Residual") + 
        ggtitle(paste("p =", p))
      plot(p)
    }
    ```
    I would say $p$ = 2 might be the best choice here. Here I will estimate the model using weighted least squares and repeat the inferences.
    ```{r}
    ironretention$w <- 1
    for (i in 1:10) {
      m <- lm(retention ~ concf + ion + concf:ion,
        data = ironretention, weights = w)
      ironretention$yhat <- predict(m)
      ironretention$w <- with(ironretention, 1 / ((yhat/100)^2 * (1 - yhat/100)^2))
    }
    summary(m)$coefficients
    trtools::contrast(m, a = list(concf = c("0.3","0.3","1.2","1.2","10.2","10.2"),
      ion = c("ferric","ferrous","ferric","ferrous","ferric","ferrous")),
      cnames = c("ferric,0.3","ferrous,0.3","ferric,1.2","ferrous,1.2",
        "ferric,10.2","ferrous,10.2"))
    trtools::contrast(m, 
      a = list(concf = c("0.3","1.2","10.2"), ion = "ferrous"),
      b = list(concf = c("0.3","1.2","10.2"), ion = "ferric"),
      cnames = c(0.3, 1.2, 10.2))
    ```
    For this particular model the point estimates are the same, but the standard errors have changed (some smaller and some larger).[^weightspointestimates] Changes in the standard errors also impacts confidence intervals and test statistics.
    
0. Since the explanatory variables are categorical and there are multiple observations per treatment condition, another approach to dealing with the heteroscedasticity is to let the variance vary over the treatment conditions so that 
$$
\text{Var}(Y_i) = 
  \begin{cases}
  \sigma_1^2, & \text{if the concentration is 0.3 and the ion is ferric}, \\
  \sigma_2^2, & \text{if the concentration is 1.2 and the ion is ferric}, \\
  \sigma_3^2, & \text{if the concentration is 10.2 and the ion is ferric}, \\
  \sigma_4^2, & \text{if the concentration is 0.3 and the ion is ferrous}, \\
  \sigma_5^2, & \text{if the concentration is 1.2 and the ion is ferrous}, \\
  \sigma_6^2, & \text{if the concentration is 10.2 and the ion is ferrous}. 
\end{cases}
$$
If these variances were known then the weights for the observations in each treatment condition would be the reciprocals of these variances (e.g., the weight for the observations in the first treatment condition would be $w_i = 1/\sigma_1^2$). These variances are not known but can be estimated from the data. Use the sample variances from the six treatment conditions (i.e., $s_1^2, s_2^2, \dots, s_6^2$) to create weights and estimate the model using weighted least squares with these weights. Repeat what you did in the first problem and comment briefly on if and how your inferences changed. 

    **Solution**: Here we compute the weights based on the sample variances.
    ```{r}
    library(dplyr)
    ironretention <- ironretention %>% group_by(concf,ion) %>% 
      mutate(w = 1 / var(retention))
    m <- lm(retention ~ concf + ion + concf:ion,
      data = ironretention, weights = w)
    summary(m)$coefficients
    trtools::contrast(m, a = list(concf = c("0.3","0.3","1.2","1.2","10.2","10.2"),
      ion = c("ferric","ferrous","ferric","ferrous","ferric","ferrous")),
      cnames = c("ferric,0.3","ferrous,0.3","ferric,1.2","ferrous,1.2",
        "ferric,10.2","ferrous,10.2"))
    trtools::contrast(m, 
      a = list(concf = c("0.3","1.2","10.2"), ion = "ferrous"),
      b = list(concf = c("0.3","1.2","10.2"), ion = "ferric"),
      cnames = c(0.3, 1.2, 10.2))
    ```
    Similarly to when using iteratively weighted least squares, using these weights does not affect the point estimates but it does impact the standard errors and thus confidence intervals and test statistics.

[^weightspointestimates]: Whether weights affect point estimates depends on the model. Usually they do unless all explanatory variables are categorical and all interactions are included in the model. 

[^with]: Because these weights are a more complicated function of the predicted values you might find it useful to compute the weights in a couple of steps where you first save the predicted values to the data frame and then you compute the weights using the `with` function so that you do not need to write `ironretention$yhat` to reference the predicted values. Here is how that would look like for the example in class on 2/24. 
    ```{r, eval = FALSE}
    daphniastrat$yhat <- predict(m)              # save predicted values
    daphniastrat$w <- with(daphniastrat, 1/yhat) # compute weights
    ```
Note that you will compute the weights differently here and not use `1/yhat`.

[^p]: Although we are only considering integer values of $p$, it could be any real number (e.g., $p$ = 1.5).

[^proportion]: If $Y_i$ is a *proportion* so that $0 \le Y_i \le 1$, the division by 100 is omitted so that $\text{Var}(Y_i) \propto E(Y_i)^p[1-E(Y_i)]^p$.

[^interaction]: Models with interactions can often be confusing and intimidating. The parameterization that is used by default is selected largely for computational convenience, and to some degree by tradition. Perhaps a simpler way to look at this model is with an alternative parameterization.
    ```{r}
    m <- lm(retention ~ -1 + concf:ion, data = ironretention)
    summary(m)$coefficients
    ```
Here there is effectively an indicator variable (formed as the product of two indicator variables) for each combination of concentration and ion, so the model can be written case-wise as
$$
  E(Y_i) = 
  \begin{cases}
  \beta_1, & \text{if the concentration is 0.3 and the ion is ferric}, \\
  \beta_2, & \text{if the concentration is 1.2 and the ion is ferric}, \\
  \beta_3, & \text{if the concentration is 10.2 and the ion is ferric}, \\
  \beta_4, & \text{if the concentration is 0.3 and the ion is ferrous}, \\
  \beta_5, & \text{if the concentration is 1.2 and the ion is ferrous}, \\
  \beta_6, & \text{if the concentration is 10.2 and the ion is ferrous}. 
  \end{cases}
$$
Note that this is the *same* model, just written (i.e., parameterized) differently. But this parameterization shows that a model with two categorical variables and an interaction between them effectively treats each distinct combination of categories as a category, so we can view this model as one with *one* factor formed by the six combinations of the levels of two factors. Fortunately when we are using functions like `contrast` or `emmeans` for inferences we do not need to be too concerned about the specific parameterization.  

[^rice]: Rice, J. A. (1998). *Mathematical statistics and data analysis*. Pacific Grove, CA: Wadsworth & Brook/Cole.

[^chr]: Notice that the variable `ion` is not a factor but is instead a *character* (`chr`) variable (i.e., a word or phrase, what is sometimes called a "string"). A character variable is not treated quite the same as a factor in R, and sometimes this is an issue, but most regression modeling functions will automatically convert a character variable to a factor when they are used in the model formula so the distinction is not generally a concern unless we try to use functions to manipulate the variable that are specifically designed for factors (e.g., `level`, `reorder`, and the functions in the **forcats** package). 

## Modeling the Biopotency of Methionine in Turkeys

**Note**: This problem is extra credit for students in Stat 436, but required for students in Stat 516.

This problem concerns data from a study of the effects of varying doses and sources of methionine (an essential amino acid) on the growth of young turkeys.[^noll] The data are not, to my knowledge, included in an existing R package, but they can be entered into a data frame called `turkeys` as follows.
```{r}
turkeys <- data.frame(
  weight = c(674, 764, 795, 796, 826, 782, 834, 836, 830),
  pens = c(10, 5, 2, 2, 5, 5, 2, 2, 5),
  dosea = c(c(0, 0.12, 0.22, 0.32, 0.44), rep(0, 4)),
  doseb = c(rep(0, 5), c(0.12, 0.22, 0.32, 0.44))
)
turkeys
```
Each observation of `weight` is the average weight (in grams) of a given number of `pens`, where each pen contained 15 turkeys. The turkeys in a given pen were given a dose of one of two sources of methionine, with the doses recorded as the percent of the total diet. Note that 10 pens were not given any additional methionine. The researchers varied the number of pens for a given dose as part of an optimal experimental design. To plot these data it is useful to create a couple of new variables.
```{r}
library(dplyr)
turkeys <- turkeys %>% 
  mutate(dose = dosea + doseb) %>%
  mutate(source = case_when(
    dose == 0 ~ "none",
    dosea > 0 ~ "a",
    doseb > 0 ~ "b")
  )
turkeys
library(ggplot2)
p <- ggplot(turkeys, aes(x = dose, y = weight, color = source)) + 
  theme_minimal() + geom_point(aes(size = pens)) + 
  scale_size(breaks = c(2, 5, 10)) + 
  labs(x = "Dose of Methionine (% of diet)", y = "Mean Weight (g)", 
    color = "Source", size = "Number of Pens")
plot(p)
```
Here you will consider modeling expected mean weight as a function of dose and source of methionine, while also accounting the number of pens used for mean weight. The basic model you will be using has the form
$$
  E(W) = \alpha + (\gamma - \alpha)2^{-d/\lambda},
$$
where $W$ is the (mean) weight and $d$ is dose of methionine. Here $\alpha$ is the asymptote representing the expected weight approached as $d$ is increases, $\gamma$ is the expected weight when no methionine is given, and $\lambda$ is a "half-life" parameter interpreted as the dose of methionine that results in an expected weight half way between $\gamma$ and $\alpha$ so that if $d = \lambda$ then $E(W) = (\alpha + \gamma)/2$. But here you will be extending this model to account for modeling different sources of methionine. The assumption will be that if the biopotency of methionine varies depending on the source, then this will be reflected in differences in the value of $\lambda$. 

Because the number of pens used to compute the average weight varied by dose, the variance of the average weight will be inversely proportional to the number of pens --- i.e., $\text{Var}(Y_i) \propto 1/n_i$ where $n_i$ is the number of pens averaged for the $i$-th observation (similar to the example from lecture on 2/17). So in what follows use the number of pens to specify weights to estimate the model parameters using *weighted* least squares. 

1. Consider the regression model
$$
  E(W_i) = 
  \begin{cases}
    \gamma, & \text{if no methionine was given}, \\
    \alpha + (\gamma - \alpha)2^{-d_i/\lambda_a}, & 
      \text{if methionine was given from source $a$}, \\
    \alpha + (\gamma - \alpha)2^{-d_i/\lambda_b}, & 
      \text{if methionine was given from source $b$}.
  \end{cases}
$$
Note that if the dose is zero then the expected weight is $\gamma$, since $\alpha + (\gamma - \alpha)2^{-d/\lambda} = \gamma$ if $d = 0$. Estimate this model using the `nls` function and report the parameter estimates by giving the output from `summary`.[^lambdastart] I would recommend using the `case_when` function from the **dplyr** package to specify this model in `nls`. Plot the model with the raw data. Finally, use the `lincon` function to determine if there is a statistically significant difference between $\lambda_a$ and $\lambda_b$, meaning that the biopotency of methionine does depend on the source (assume a significance level of $\alpha$ = 0.05). 

    **Solution**: Here is how we can estimate the model. I guessed the starting values from the plot.
    ```{r}
    library(dplyr)
    m <- nls(weight ~ case_when(
      source == "none" ~ gamma,
      source == "a" ~ alpha + (gamma - alpha) * 2^(-dose/lambdaa),
      source == "b" ~ alpha + (gamma - alpha) * 2^(-dose/lambdab)),
      start = list(alpha = 825, gamma = 675, lambdaa = 0.1, lambdab = 0.1),
      data = turkeys, weights = pens)
    summary(m)$coefficients
    ```
    Here is a plot of the model.
    ```{r}
    d <- expand.grid(source = c("none","a","b"), dose = seq(0, 0.44, length = 100))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(turkeys, aes(x = dose, y = weight, color = source)) +
      geom_line(aes(y = yhat), data = d) + 
      theme_minimal() + geom_point(aes(size = pens)) + 
      scale_size(breaks = c(2, 5, 10)) + 
      labs(x = "Dose of Methionine (% of diet)", y = "Mean Weight (g)", 
        color = "Source", size = "Number of Pens")
    plot(p)
    ```
    The plot looks a little goofy since if the source is "none" then dose must be zero. We could fix this by editing the data used for plotting the curves by setting the dose to zero if source is "none".
    ```{r}
    library(dplyr) 
    d <- d %>% mutate(dose = ifelse(source == "none", 0, dose))
    d$yhat <- predict(m, newdata = d)
    p <- ggplot(turkeys, aes(x = dose, y = weight, color = source)) +
      geom_line(aes(y = yhat), data = d) + 
      theme_minimal() + geom_point(aes(size = pens)) + 
      scale_size(breaks = c(2, 5, 10)) + 
      labs(x = "Dose of Methionine (% of diet)", y = "Mean Weight (g)", 
        color = "Source", size = "Number of Pens")
    plot(p)
    ```
    Note that you could use this without using the **dplyr** package with the following.
    ```{r, eval = FALSE}
    d$dose[d$source == "none"] <- 0
    ```
    Here is the test of the difference between $\lambda_a$ and $\lambda_b$.
    ```{r}
    lincon(m, a = c(0,0,1,-1))
    ```
    The half-life parameter is significantly higher for source *b*. 

0. Another way to write the model that avoids having to explicitly write the model case-wise is
$$
  E(W_i) = \alpha + (\gamma - \alpha)2^{-(a_i/\lambda_a + b_i/\lambda_b)},
$$
where $a_i$ and $b_i$ are the doses of methionine from sources $a$ and $b$, respectively (i.e., these are the variables `dosea` and `doseb` in the data frame). Estimate this model using the `nls` function and report the parameter estimates using the output from `summary`. You should get the same results as you obtained in the previous problem. 

    **Source**: We can estimate this parameterization of the model as follows.
    ```{r}
    m <- nls(weight ~ alpha + (gamma - alpha) * 2^(-(dosea/lambdaa + doseb/lambdab)),
      start = list(alpha = 825, gamma = 675, lambdaa = 0.1, lambdab = 0.1),
      data = turkeys, weights = pens)
    summary(m)$coefficients
    ```
    Note that the inferences are the same.

[^noll]: Source: Noll, S. L., Waibel, P. E., Cook, R. D., & Witmer, J. A. (1984). Biopotency of methionine sources for young turkeys. *Poultry Science*, *63*, 2458--2470.

[^lambdastart]: The models you are estimated here may be somewhat sensitive to specifying good starting values. If you have difficulties I would recommend that you start by *not* estimating $\lambda_a$ and $\lambda_b$ but instead insert values in the model based on guessing their values from the plot (similar to how I guessed the value of the $h$ parameter for the `ToothGrowth` data from lecture). Then use the estimates of $\alpha$ and $\gamma$ you get as starting values for those parameters, and your guesses of $\lambda_a$ and $\lambda_b$ as starting values for those parameters, and proceed to estimate the model with all four unknown parameters.
